{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Import libaries**"
      ],
      "metadata": {
        "id": "AJNSlcJUTn6z"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 133,
      "metadata": {
        "id": "DB3Zk3HAT12x"
      },
      "outputs": [],
      "source": [
        "#plan\n",
        "\n",
        "#oop nn, ie has classes \n",
        "\n",
        "# get the predicted values stored somewhere\n",
        "\n",
        "#import libraries \n",
        "\n",
        "import numpy as np\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "import time"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Implement the base layer class"
      ],
      "metadata": {
        "id": "6AAb2cfLTv1b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#the abstract base layer class\n",
        "\n",
        "# rename? \n",
        "\n",
        "class Layer:\n",
        "  def __init__(base):           #constructor\n",
        "    base.input = None           # change to array? \n",
        "    base.output = None \n",
        "\n",
        "  def forward(base, input):\n",
        "    raise NotImplementedError\n",
        "\n",
        "  def backward(base, outputGrad, learnRate):\n",
        "    raise NotImplementedError"
      ],
      "metadata": {
        "id": "Fs1W8fesUs-D"
      },
      "execution_count": 134,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Implement dense layer\n",
        "\n",
        "\n",
        "explain  the maths (derivatives)"
      ],
      "metadata": {
        "id": "np1k3EpDT-6B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# rename\n",
        "\n",
        "class Dense(Layer):\n",
        "  def __init__(base, inputSize, outputSize):          # constructor with number of input neurons and number of output neurons \n",
        "    #base.inputSize = inputSize\n",
        "    #base.outputSize = outputSize\n",
        "    base.weights = np.random.rand(outputSize, inputSize) - 0.5        # create a 2D array with random values between -0.5 and 0.5 for the initial weights for the layer \n",
        "    base.bias = np.random.rand(outputSize, 1) - 0.5                   # create a vector with random values between -0.5 and 0.5 for the initial biases for the layer\n",
        "    #Layer.__init__(base)\n",
        "\n",
        "  def forward(base, input_data):                                      # forward propagation \n",
        "    base.input = input_data                                           # store input data\n",
        "    base.output = np.dot(base.weights, base.input) + base.bias        # calculate z \n",
        "    #print(\"Weights size is \", base.weights.shape)\n",
        "    #print(\"Bias size is \", base.bias.shape)\n",
        "    #print(\"Input size is \", base.input.shape)\n",
        "    #print(\"Output size is \", base.output.shape)\n",
        "    #print(\"forward pass - dense\")\n",
        "    #print(base.output)\n",
        "    return base.output                                                # return z as the ouput for this layer \n",
        "\n",
        "#  def backward(base, outputGrad, learnRate):\n",
        "#    weightGrad = np.dot(outputGrad, base.input.T)\n",
        "#    base.weights -= learnRate*weightGrad\n",
        "#    base.bias -= learnRate*outputGrad\n",
        "#    return np.dot(base.weights.T, outputGrad)\n",
        "\n",
        "  def backward(self, outputGrad, learning_rate):                      # backward propagation \n",
        "    input_error = np.dot(self.weights.T, outputGrad)                  # calculate the derivative of the error wrt the inputs \n",
        "    weights_error = np.dot(outputGrad, self.input.T)                  # calculate the derivative of the error wrt the weights\n",
        "    bias_error = outputGrad                                           # the derivative of the error wrt the bias is the same as the derivative of the error wrt to the output \n",
        "\n",
        "    # update parameters\n",
        "    self.weights = self.weights - learning_rate * weights_error       # updates the weights \n",
        "    self.bias = self.bias - learning_rate * bias_error                # updates the bias\n",
        "    ##print(\"backwards pass - dense\")\n",
        "    return input_error                                                # returns the derivative of the error wrt the inputs for the derivative of the activation function \n",
        "  "
      ],
      "metadata": {
        "id": "K2miSzFKWEN2"
      },
      "execution_count": 135,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Implement activation layer"
      ],
      "metadata": {
        "id": "cZrcJEoHUMGF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Activation(Layer):\n",
        "  def __init__(base, activationFn, activationFnDerv):\n",
        "    base.activationFn = activationFn                            # the activation function provided for forward propagation \n",
        "    base.activationFnDerv = activationFnDerv                    # the derivative of the activation function for backpropagation \n",
        "\n",
        "  def forward(base, input):\n",
        "    base.input = input                                          # gets z from the previous layer \n",
        "    #print(\"Output in forward pass - act\")\n",
        "    #print(\"z for act is \", base.input)\n",
        "    base.output = base.activationFn(base.input)                 # passes z to the activation function\n",
        "    ##print(\"forward pass - act\")\n",
        "    #print(\"act returns is \", base.output)\n",
        "    return base.output                                          # returns the outputs of the forward propagation \n",
        "  \n",
        "  def backward(base, outputGrad, learnRate):\n",
        "    #TODO\n",
        "    ##print(\"backwards pass - act\")\n",
        "    return base.activationFnDerv(base.input) * outputGrad       # returns the result of the derivative of the activation function \n"
      ],
      "metadata": {
        "id": "qBZlye0kWWg2"
      },
      "execution_count": 136,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Activation Functions"
      ],
      "metadata": {
        "id": "slJz2g0HF3Gf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#class Tanh(Activation):\n",
        "#  def __init__(base):\n",
        "#    tanh = lambda x: np.tanh(x)\n",
        "#    tanhDerv = lambda x: 1 - np.tanh(x)**2\n",
        "#    super().__init__(tanh, tanhDerv)\n",
        "\n",
        "\n",
        "# the hyperbolic tangent activation function for forward propagation \n",
        "def tanh(z):\n",
        "  ##print(\"act func\")\n",
        "  #print(\"input - act \", z)\n",
        "  z = z.astype(float)\n",
        "  #r = np.zeros([1,len(z[0])])\n",
        "  #for i in range(len(z[0])):\n",
        "  #  r[0][i] = np.tanh(z[0][i])\n",
        "  #print(\"r is \", r)\n",
        "  #print(\"returned from act \", np.tanh(z))\n",
        "  return np.tanh(z)\n",
        "\n",
        "# the hyperbolic tangent activation function for backward propagation \n",
        "def tanhDerv(x):\n",
        "  ##print(\"act func derv\")\n",
        "  x = x.astype(float)\n",
        "  return 1 - np.tanh(x)**2\n",
        "\n",
        "def relu(z):\n",
        "  for i in range(len(z[0])):\n",
        "    z[0][i] = np.maximum(0,z[0][i])\n",
        "  return z\n",
        "\n",
        "def reluDerv(z):\n",
        "  #print(\"Shape of input - act \", z.shape)\n",
        "  r = np.zeros([1,len(z[0])])\n",
        "  for i in range(len(z[0])):\n",
        "    if(z[0][i]<0):\n",
        "      r[0][i] = 0\n",
        "      #return 0\n",
        "    else: \n",
        "      r[0][i] = 1\n",
        "      #return 1\n",
        "  #print(\"Shape of output - act \", r.shape)\n",
        "  return r\n",
        "\n",
        "def Logistic(x):\n",
        "  x=x.astype(float)\n",
        "  #print(\"Act returns \", 1/(1+np.exp(-x)))\n",
        "  return 1/(1+np.exp(-x))\n",
        "\n",
        "def LogisticDerv(x):\n",
        "  x=x.astype(float)\n",
        "  #print(\"act derv gets \", x)\n",
        "  #print(\"Act derv returns \",(-np.exp(-x)/((1+np.exp(-x))**2)))\n",
        "  #print(\"or it could return \", Logistic(x)*(1-Logistic(x)))\n",
        "  #Logistic(x)*(1-Logistic(x))\n",
        "\n",
        "  return Logistic(x)*(1-Logistic(x)) #(-np.exp(-x)/((1+np.exp(-x))**2))\n",
        "\n",
        "a = np.array([[2,0],[0.1,6]])\n",
        "print(tanh(a))"
      ],
      "metadata": {
        "id": "abIt1wd7YYdT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5a463703-3d89-4c81-e67c-256742c6337f"
      },
      "execution_count": 137,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.96402758 0.        ]\n",
            " [0.09966799 0.99998771]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loss Functions"
      ],
      "metadata": {
        "id": "S7oz9jFPFtBq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def mse(yExpect, yPred):\n",
        "  ##print(\"loss func\")\n",
        "  #print(\"mse - input expcted\", yExpect, \" and pred \", yPred)\n",
        "  #print(\"mse - yExpect \", yExpect)\n",
        "  #print(\"mse - yPred \", yPred)\n",
        "  #print(\"mse - returns \", np.mean(np.power(yExpect - yPred, 2)))\n",
        "  #print(\"mse - power \", np.power(yExpect - yPred, 2))\n",
        "  return np.mean(np.power(yExpect - yPred, 2))\n",
        "\n",
        "def mseDerv(yExpect, yPred):\n",
        "  ##print(\"loss func derv\")\n",
        "  #print(\"mseDerv - input expcted\", yExpect, \" and pred \", yPred)\n",
        "  #print(\"mseDerv - yExpect \", yExpect)\n",
        "  #print(\"mseDerv - yPred \", yPred)\n",
        "  #print(\"mseDerv - returns \", 2*(yPred - yExpect)/yExpect.size)\n",
        "  return 2*(yPred - yExpect)/yExpect.size\n",
        "\n",
        "def crossEn(yExpect, yPred):\n",
        "  if yPred < 0:\n",
        "    yPred = 0\n",
        "  #print(\"cross - input expcted\", yExpect, \" and pred \", yPred)\n",
        "  errCE = -(yExpect*np.log(yPred)+(1-yExpect)*np.log(1-yPred))\n",
        "  #print(\"cross - returns \", errCE)\n",
        "  return errCE\n",
        "\n",
        "def crossEnDerv(yExpect, yPred):\n",
        "  #print(\"crossDerv - input expcted\", yExpect, \" and pred \", yPred)\n",
        "  #print(\"cross - returns \", -(yExpect/yPred)+((1-yExpect)/(1-yPred)))\n",
        "  return -(yExpect/yPred)+((1-yExpect)/(1-yPred))\n",
        "\n",
        "def absLoss(yExpect, yPred):\n",
        "  return abs(yExpect-yPred)\n",
        "\n",
        "def absLossDerv(yExpect, yPred):\n",
        "  #print(\"yExpect \", yExpect)\n",
        "  #print(\"yPred \", yPred)\n",
        "  loss = np.zeros([len(yPred), 1])\n",
        "  for i in range(len(yExpect)):\n",
        "    if yPred[i] > yExpect[i]:\n",
        "      loss[i] = 1\n",
        "    elif yPred[i] < yExpect[i]:\n",
        "      loss[i] = -1\n",
        "    else :\n",
        "      loss[i] = 0\n",
        "  #print(\"loss \", loss)\n",
        "  return loss\n",
        "\n",
        "#crossEn(0, -0.24704899)\n",
        "#np.log(0)"
      ],
      "metadata": {
        "id": "x4NwpK64ZeQX"
      },
      "execution_count": 138,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Nextwork class"
      ],
      "metadata": {
        "id": "RIebDOSWUPqi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from numpy.lib import index_tricks\n",
        "# want to return an array of predicted outputs - NOT a list!\n",
        "\n",
        "class Network:\n",
        "    def __init__(self):\n",
        "        self.layers = []\n",
        "        self.loss = None\n",
        "        self.loss_prime = None\n",
        "\n",
        "    # add layer to network\n",
        "    def add(self, layer):\n",
        "        self.layers.append(layer)\n",
        "\n",
        "    # set loss to use\n",
        "    def use(self, loss, loss_prime):\n",
        "        self.loss = loss\n",
        "        self.loss_prime = loss_prime\n",
        "\n",
        "    # predict output for given input\n",
        "    def predict(self, input_data):\n",
        "        # sample dimension first\n",
        "                              ##print(\"predict\")\n",
        "        samples = len(input_data)                             # predicts an output for every row in the data \n",
        "                              #print(\"size of input datd is \", len(input_data), len(input_data[0]), len(input_data[0][0]))\n",
        "        result = []\n",
        "                              #result2 = np.zeros(len(input_data))                                           # initialise array to store the predicted results \n",
        "                              #print(result2)\n",
        "                              # run network over all samples\n",
        "        for i in range(samples):                              # for each row in the data \n",
        "                            # forward propagation\n",
        "                            #output = np.zeros([len(input_data[0]), samples], dtype = int)\n",
        "                            #input = input_data[i]                            # get \n",
        "                            #print(\"All input for predict \", input_data)\n",
        "            #print(\"Input for predict \", input_data[i])\n",
        "                            #print(\"Input length \", len(input_data[i]))\n",
        "                            #print(\"Input length x2 \", len(input_data[i][0]))\n",
        "            output = input_data[i]\n",
        "                            #print(\"len output as input - initial\", len(output))\n",
        "                            #output = np.zeros(len(input_data[i][0]))\n",
        "                            #output = np.zeros([len(input_data[i]), len(input_data[i][0])])\n",
        "                            #print(\"TEST 1 \", input)\n",
        "                            #print(\"TEST 2 \", input[0])\n",
        "                            #print(\"TEST 3 \", input[0][0])\n",
        "                            \n",
        "            for layer in self.layers:\n",
        "             # print(\"Input for forward \", output)         \n",
        "              output = layer.forward(output)\n",
        "              #print(\"Output for forward \", output)    \n",
        "                ##\n",
        "                            #print(\"len output as output\", len(output))\n",
        "                            #print(\"output \", output)\n",
        "                            #bob = output.reshape(1)\n",
        "                            #print(\"output reshape \", bob)\n",
        "                            #result2[i] = bob\n",
        "            #print(\"appended\")\n",
        "            result.append(output)\n",
        "                            #print(\"output[0][0] is \", output[0][0])\n",
        "                            #result2[0][i] = output[0][0]\n",
        "                            #print(\"i\", i)\n",
        "                            ##\n",
        "                            #print(\"result - predict \", result)\n",
        "                            #print(\"result2 - predict \", result2)\n",
        "                            ##print(\"result 2 - predict \", result)\n",
        "                            #print(\"result at pos 0 \", result[0])\n",
        "                            #print(\"result 2 at pos 0 \", result2[0][0])\n",
        "        return result #, result2\n",
        "\n",
        "    # train the network\n",
        "    def fitSGD(self, x_train, y_train, epochs, learning_rate):\n",
        "        # sample dimension first\n",
        "        samples = len(x_train)\n",
        "        errorStore = np.zeros(epochs)\n",
        "                              ##\n",
        "                              #print(\"fit\")\n",
        "        # training loop\n",
        "        for i in range(epochs):\n",
        "            err = 0\n",
        "            for j in range(samples):\n",
        "                # forward propagation\n",
        "                output = x_train[j]\n",
        "                for layer in self.layers:\n",
        "                                    #print(\"print output - train\", output)\n",
        "                  output = layer.forward(output)\n",
        "                #print(output)\n",
        "                # compute loss (for display purpose only)\n",
        "                err += self.loss(y_train[j], output)\n",
        "                                    #print(\"new err \", err)\n",
        "                                    #print(\"this is err\", err)\n",
        "                # backward propagation\n",
        "                error = self.loss_prime(y_train[j], output)\n",
        "                for layer in reversed(self.layers):\n",
        "                    error = layer.backward(error, learning_rate)\n",
        "\n",
        "            # calculate average error on all samples\n",
        "                                    #print(\"this is total err\", err)\n",
        "            err /= samples\n",
        "                                    #print(\"this is avg err\", err)\n",
        "                                    #print(\"final err is \", err)\n",
        "            errorStore[i] = err\n",
        "            print('epoch %d/%d   error=%f' % (i+1, epochs, err))\n",
        "        return errorStore\n",
        "    \n",
        "    \n",
        "    # train the network\n",
        "    def fitBatch(self, x_train, y_train, epochs, learning_rate):\n",
        "        # sample dimension first\n",
        "        samples = len(x_train)\n",
        "        errorStore = np.zeros(epochs)\n",
        "                              ##\n",
        "        #print(\"fit\")\n",
        "        # training loop\n",
        "        for i in range(epochs):\n",
        "            err = 0\n",
        "            for j in range(samples):\n",
        "                # forward propagation\n",
        "                output = x_train[j]\n",
        "                for layer in self.layers:\n",
        "                  #print(\"print input - forward\", output)\n",
        "                  output = layer.forward(output)\n",
        "                 # print(\"print output - forward\", output)\n",
        "                # compute loss (for display purpose only)\n",
        "                err += self.loss(y_train[j], output)\n",
        "                                    #print(\"new err \", err)\n",
        "                #print(\"this is loss \", err)\n",
        "                # backward propagation\n",
        "                error = self.loss_prime(y_train[j], output)\n",
        "                #print(\"this is error \", err)\n",
        "            for layer in reversed(self.layers):\n",
        "                error = layer.backward(error, learning_rate)\n",
        "\n",
        "            # calculate average error on all samples\n",
        "                                    #print(\"this is total err\", err)\n",
        "            err /= samples\n",
        "                                    #print(\"this is avg err\", err)\n",
        "                                    #print(\"final err is \", err)\n",
        "            errorStore[i] = err\n",
        "            print('epoch %d/%d   error=%f' % (i+1, epochs, err))\n",
        "        return errorStore\n",
        "\n",
        "    def fitMinBatch(self, x_train, y_train, epochs, learning_rate, batchSize):\n",
        "        # sample dimension first\n",
        "        samples = len(x_train)\n",
        "        errorStore = np.zeros(epochs)\n",
        "                              ##\n",
        "        #print(\"fit\")\n",
        "        # training loop\n",
        "        for i in range(epochs):\n",
        "            err = 0\n",
        "            for j in range(batchSize):\n",
        "                # forward propagation\n",
        "                index = random.randint(0, len(x_train))-1\n",
        "                output = x_train[index]\n",
        "                for layer in self.layers:\n",
        "                  #print(\"print input - forward\", output)\n",
        "                  output = layer.forward(output)\n",
        "                 # print(\"print output - forward\", output)\n",
        "                # compute loss (for display purpose only)\n",
        "                err += self.loss(y_train[index], output)\n",
        "                                    #print(\"new err \", err)\n",
        "                #print(\"this is loss \", err)\n",
        "                # backward propagation\n",
        "                error = self.loss_prime(y_train[index], output)\n",
        "                #print(\"this is error \", err)\n",
        "            for layer in reversed(self.layers):\n",
        "                error = layer.backward(error, learning_rate)\n",
        "\n",
        "            # calculate average error on all samples\n",
        "                                    #print(\"this is total err\", err)\n",
        "            err /= samples\n",
        "                                    #print(\"this is avg err\", err)\n",
        "                                    #print(\"final err is \", err)\n",
        "            errorStore[i] = err\n",
        "            print('epoch %d/%d   error=%f' % (i+1, epochs, err))\n",
        "        return errorStore\n",
        "\n",
        "\n",
        "\n",
        "    # train the network - stop condition \n",
        "    def fit3(self, x_train, y_train, epochs, learning_rate):\n",
        "        # sample dimension first\n",
        "        samples = len(x_train)\n",
        "        errorStore = np.zeros(epochs)\n",
        "                              ##\n",
        "                              #print(\"fit\")\n",
        "        # training loop\n",
        "        i = 0\n",
        "        err = 0\n",
        "        while i < epochs and err > 0.001 :\n",
        "        #for i in range(epochs):\n",
        "          print(\"here\")\n",
        "          err = 0\n",
        "          for j in range(samples):\n",
        "            #forward propagation\n",
        "            output = x_train[j]\n",
        "            for layer in self.layers:\n",
        "                                    #print(\"print output - train\", output)\n",
        "              output = layer.forward(output)\n",
        "\n",
        "                # compute loss (for display purpose only)\n",
        "              err += self.loss(y_train[j], output)\n",
        "                                    #print(\"new err \", err)\n",
        "                                    #print(\"this is err\", err)\n",
        "                # backward propagation\n",
        "              error = self.loss_prime(y_train[j], output)\n",
        "              for layer in reversed(self.layers):\n",
        "                  error = layer.backward(error, learning_rate)\n",
        "\n",
        "            # calculate average error on all samples\n",
        "                                    #print(\"this is total err\", err)\n",
        "            err /= samples\n",
        "                                    #print(\"this is avg err\", err)\n",
        "                                    #print(\"final err is \", err)\n",
        "            errorStore[i] = err\n",
        "            print('epoch %d/%d   error=%f' % (i+1, epochs, err))\n",
        "        return errorStore"
      ],
      "metadata": {
        "id": "hZymovfzmJIR"
      },
      "execution_count": 184,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Data stuff**"
      ],
      "metadata": {
        "id": "yql0a0j3b__y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data manipulation "
      ],
      "metadata": {
        "id": "_wBkHnc-cMV7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "assert sys.version_info >= (3, 5)\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import time\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "import io\n",
        "from google.colab import files\n",
        "uploaded = files.upload()     #upload the wdbc dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "id": "5w3vkmj9b7O9",
        "outputId": "1b3d9863-b829-4da0-dff7-8edd89c2b5d5"
      },
      "execution_count": 140,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-5ffca6b1-377c-4739-87ca-d382251d5429\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-5ffca6b1-377c-4739-87ca-d382251d5429\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving wdbc.data to wdbc (4).data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#from google.colab import files\n",
        "#uploaded = files.upload()     #upload the wdbc dataset"
      ],
      "metadata": {
        "id": "uJlrhxxl3EtP"
      },
      "execution_count": 141,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#data2 = pd.read_csv(io.BytesIO(uploaded['test.data']))\n",
        "#print(data2)\n",
        "#data2.head()"
      ],
      "metadata": {
        "id": "-QY8TgJ63FQv"
      },
      "execution_count": 142,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#data2[\"ans\"] = np.where(data[\"Diagnosis\"] == \"M\", 1, 0)\n",
        "\n",
        "#Y = np.array(data2[[\"ans\"]]) # X is the input data (does not include the outcome) \n",
        "#YBinary = np.array(data[[\"DiagnosisBinary\"]])\n",
        "#print(temp) \n",
        "\n",
        "#for i in range(0, len(Y)):\n",
        "#  if Y[i][0] == \"M\" :\n",
        "#    Y[i][0] = 1\n",
        "#  elif Y[i][0] == \"B\":\n",
        "#    Y[i][0] = 0\n",
        "\n",
        "#print(Y)\n",
        "#print(YBinary)\n",
        "\n",
        "#X = np.array(data2[[\"pos1\",\"pos2\"]]) # X is the input data (does not include the outcome)\n",
        "#print(X)"
      ],
      "metadata": {
        "id": "Y5oPdOKM3mO6"
      },
      "execution_count": 143,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv(io.BytesIO(uploaded['wdbc.data']))\n",
        "print(data)\n",
        "data.head()"
      ],
      "metadata": {
        "id": "Ipm-BbOzb8sd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 929
        },
        "outputId": "21263660-b0ee-4c53-a932-5746042468d9"
      },
      "execution_count": 144,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "       842302  M  17.99  10.38   122.8    1001   0.1184   0.2776   0.3001  \\\n",
            "0      842517  M  20.57  17.77  132.90  1326.0  0.08474  0.07864  0.08690   \n",
            "1    84300903  M  19.69  21.25  130.00  1203.0  0.10960  0.15990  0.19740   \n",
            "2    84348301  M  11.42  20.38   77.58   386.1  0.14250  0.28390  0.24140   \n",
            "3    84358402  M  20.29  14.34  135.10  1297.0  0.10030  0.13280  0.19800   \n",
            "4      843786  M  12.45  15.70   82.57   477.1  0.12780  0.17000  0.15780   \n",
            "..        ... ..    ...    ...     ...     ...      ...      ...      ...   \n",
            "563    926424  M  21.56  22.39  142.00  1479.0  0.11100  0.11590  0.24390   \n",
            "564    926682  M  20.13  28.25  131.20  1261.0  0.09780  0.10340  0.14400   \n",
            "565    926954  M  16.60  28.08  108.30   858.1  0.08455  0.10230  0.09251   \n",
            "566    927241  M  20.60  29.33  140.10  1265.0  0.11780  0.27700  0.35140   \n",
            "567     92751  B   7.76  24.54   47.92   181.0  0.05263  0.04362  0.00000   \n",
            "\n",
            "      0.1471  ...   25.38  17.33   184.6    2019   0.1622   0.6656  0.7119  \\\n",
            "0    0.07017  ...  24.990  23.41  158.80  1956.0  0.12380  0.18660  0.2416   \n",
            "1    0.12790  ...  23.570  25.53  152.50  1709.0  0.14440  0.42450  0.4504   \n",
            "2    0.10520  ...  14.910  26.50   98.87   567.7  0.20980  0.86630  0.6869   \n",
            "3    0.10430  ...  22.540  16.67  152.20  1575.0  0.13740  0.20500  0.4000   \n",
            "4    0.08089  ...  15.470  23.75  103.40   741.6  0.17910  0.52490  0.5355   \n",
            "..       ...  ...     ...    ...     ...     ...      ...      ...     ...   \n",
            "563  0.13890  ...  25.450  26.40  166.10  2027.0  0.14100  0.21130  0.4107   \n",
            "564  0.09791  ...  23.690  38.25  155.00  1731.0  0.11660  0.19220  0.3215   \n",
            "565  0.05302  ...  18.980  34.12  126.70  1124.0  0.11390  0.30940  0.3403   \n",
            "566  0.15200  ...  25.740  39.42  184.60  1821.0  0.16500  0.86810  0.9387   \n",
            "567  0.00000  ...   9.456  30.37   59.16   268.6  0.08996  0.06444  0.0000   \n",
            "\n",
            "     0.2654  0.4601   0.1189  \n",
            "0    0.1860  0.2750  0.08902  \n",
            "1    0.2430  0.3613  0.08758  \n",
            "2    0.2575  0.6638  0.17300  \n",
            "3    0.1625  0.2364  0.07678  \n",
            "4    0.1741  0.3985  0.12440  \n",
            "..      ...     ...      ...  \n",
            "563  0.2216  0.2060  0.07115  \n",
            "564  0.1628  0.2572  0.06637  \n",
            "565  0.1418  0.2218  0.07820  \n",
            "566  0.2650  0.4087  0.12400  \n",
            "567  0.0000  0.2871  0.07039  \n",
            "\n",
            "[568 rows x 32 columns]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "     842302  M  17.99  10.38   122.8    1001   0.1184   0.2776  0.3001  \\\n",
              "0    842517  M  20.57  17.77  132.90  1326.0  0.08474  0.07864  0.0869   \n",
              "1  84300903  M  19.69  21.25  130.00  1203.0  0.10960  0.15990  0.1974   \n",
              "2  84348301  M  11.42  20.38   77.58   386.1  0.14250  0.28390  0.2414   \n",
              "3  84358402  M  20.29  14.34  135.10  1297.0  0.10030  0.13280  0.1980   \n",
              "4    843786  M  12.45  15.70   82.57   477.1  0.12780  0.17000  0.1578   \n",
              "\n",
              "    0.1471  ...  25.38  17.33   184.6    2019  0.1622  0.6656  0.7119  0.2654  \\\n",
              "0  0.07017  ...  24.99  23.41  158.80  1956.0  0.1238  0.1866  0.2416  0.1860   \n",
              "1  0.12790  ...  23.57  25.53  152.50  1709.0  0.1444  0.4245  0.4504  0.2430   \n",
              "2  0.10520  ...  14.91  26.50   98.87   567.7  0.2098  0.8663  0.6869  0.2575   \n",
              "3  0.10430  ...  22.54  16.67  152.20  1575.0  0.1374  0.2050  0.4000  0.1625   \n",
              "4  0.08089  ...  15.47  23.75  103.40   741.6  0.1791  0.5249  0.5355  0.1741   \n",
              "\n",
              "   0.4601   0.1189  \n",
              "0  0.2750  0.08902  \n",
              "1  0.3613  0.08758  \n",
              "2  0.6638  0.17300  \n",
              "3  0.2364  0.07678  \n",
              "4  0.3985  0.12440  \n",
              "\n",
              "[5 rows x 32 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-71d52069-f098-421a-933f-d4705caa49bd\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>842302</th>\n",
              "      <th>M</th>\n",
              "      <th>17.99</th>\n",
              "      <th>10.38</th>\n",
              "      <th>122.8</th>\n",
              "      <th>1001</th>\n",
              "      <th>0.1184</th>\n",
              "      <th>0.2776</th>\n",
              "      <th>0.3001</th>\n",
              "      <th>0.1471</th>\n",
              "      <th>...</th>\n",
              "      <th>25.38</th>\n",
              "      <th>17.33</th>\n",
              "      <th>184.6</th>\n",
              "      <th>2019</th>\n",
              "      <th>0.1622</th>\n",
              "      <th>0.6656</th>\n",
              "      <th>0.7119</th>\n",
              "      <th>0.2654</th>\n",
              "      <th>0.4601</th>\n",
              "      <th>0.1189</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>842517</td>\n",
              "      <td>M</td>\n",
              "      <td>20.57</td>\n",
              "      <td>17.77</td>\n",
              "      <td>132.90</td>\n",
              "      <td>1326.0</td>\n",
              "      <td>0.08474</td>\n",
              "      <td>0.07864</td>\n",
              "      <td>0.0869</td>\n",
              "      <td>0.07017</td>\n",
              "      <td>...</td>\n",
              "      <td>24.99</td>\n",
              "      <td>23.41</td>\n",
              "      <td>158.80</td>\n",
              "      <td>1956.0</td>\n",
              "      <td>0.1238</td>\n",
              "      <td>0.1866</td>\n",
              "      <td>0.2416</td>\n",
              "      <td>0.1860</td>\n",
              "      <td>0.2750</td>\n",
              "      <td>0.08902</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>84300903</td>\n",
              "      <td>M</td>\n",
              "      <td>19.69</td>\n",
              "      <td>21.25</td>\n",
              "      <td>130.00</td>\n",
              "      <td>1203.0</td>\n",
              "      <td>0.10960</td>\n",
              "      <td>0.15990</td>\n",
              "      <td>0.1974</td>\n",
              "      <td>0.12790</td>\n",
              "      <td>...</td>\n",
              "      <td>23.57</td>\n",
              "      <td>25.53</td>\n",
              "      <td>152.50</td>\n",
              "      <td>1709.0</td>\n",
              "      <td>0.1444</td>\n",
              "      <td>0.4245</td>\n",
              "      <td>0.4504</td>\n",
              "      <td>0.2430</td>\n",
              "      <td>0.3613</td>\n",
              "      <td>0.08758</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>84348301</td>\n",
              "      <td>M</td>\n",
              "      <td>11.42</td>\n",
              "      <td>20.38</td>\n",
              "      <td>77.58</td>\n",
              "      <td>386.1</td>\n",
              "      <td>0.14250</td>\n",
              "      <td>0.28390</td>\n",
              "      <td>0.2414</td>\n",
              "      <td>0.10520</td>\n",
              "      <td>...</td>\n",
              "      <td>14.91</td>\n",
              "      <td>26.50</td>\n",
              "      <td>98.87</td>\n",
              "      <td>567.7</td>\n",
              "      <td>0.2098</td>\n",
              "      <td>0.8663</td>\n",
              "      <td>0.6869</td>\n",
              "      <td>0.2575</td>\n",
              "      <td>0.6638</td>\n",
              "      <td>0.17300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>84358402</td>\n",
              "      <td>M</td>\n",
              "      <td>20.29</td>\n",
              "      <td>14.34</td>\n",
              "      <td>135.10</td>\n",
              "      <td>1297.0</td>\n",
              "      <td>0.10030</td>\n",
              "      <td>0.13280</td>\n",
              "      <td>0.1980</td>\n",
              "      <td>0.10430</td>\n",
              "      <td>...</td>\n",
              "      <td>22.54</td>\n",
              "      <td>16.67</td>\n",
              "      <td>152.20</td>\n",
              "      <td>1575.0</td>\n",
              "      <td>0.1374</td>\n",
              "      <td>0.2050</td>\n",
              "      <td>0.4000</td>\n",
              "      <td>0.1625</td>\n",
              "      <td>0.2364</td>\n",
              "      <td>0.07678</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>843786</td>\n",
              "      <td>M</td>\n",
              "      <td>12.45</td>\n",
              "      <td>15.70</td>\n",
              "      <td>82.57</td>\n",
              "      <td>477.1</td>\n",
              "      <td>0.12780</td>\n",
              "      <td>0.17000</td>\n",
              "      <td>0.1578</td>\n",
              "      <td>0.08089</td>\n",
              "      <td>...</td>\n",
              "      <td>15.47</td>\n",
              "      <td>23.75</td>\n",
              "      <td>103.40</td>\n",
              "      <td>741.6</td>\n",
              "      <td>0.1791</td>\n",
              "      <td>0.5249</td>\n",
              "      <td>0.5355</td>\n",
              "      <td>0.1741</td>\n",
              "      <td>0.3985</td>\n",
              "      <td>0.12440</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows Ã— 32 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-71d52069-f098-421a-933f-d4705caa49bd')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-71d52069-f098-421a-933f-d4705caa49bd button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-71d52069-f098-421a-933f-d4705caa49bd');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 144
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#ID_Number,Diagnosis,Radius1,Texture1,Perimeter1,Area1,Smoothness1,Compactness1,Concavity1,Concave Points1,Symmetry1,Fractal Dimension1,Radius2,Texture2,Perimeter2,Area2,Smoothness2,Compactness2,Concavity2,Concave Points2,Symmetry2,Fractal Dimension2,Radius3,Texture3,Perimeter3,Area3,Smoothness3,Compactness3,Concavity3,Concave Points3,Symmetry3,Fractal Dimension3\n",
        "data.columns = [\"ID_Number\",\"Diagnosis\",\"Radius1\",\"Texture1\",\"Perimeter1\",\"Area1\",\"Smoothness1\",\"Compactness1\",\"Concavity1\",\"Concave Points1\",\"Symmetry1\",\"Fractal Dimension1\",\"Radius2\",\"Texture2\",\"Perimeter2\",\"Area2\",\"Smoothness2\",\"Compactness2\",\"Concavity2\",\"Concave Points2\",\"Symmetry2\",\"Fractal Dimension2\",\"Radius3\",\"Texture3\",\"Perimeter3\",\"Area3\",\"Smoothness3\",\"Compactness3\",\"Concavity3\",\"Concave Points3\",\"Symmetry3\",\"Fractal Dimension3\"]\n",
        "data.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 317
        },
        "id": "TUIjCXjvpRs8",
        "outputId": "98439930-13c2-483f-cda4-ef076c278221"
      },
      "execution_count": 145,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   ID_Number Diagnosis  Radius1  Texture1  Perimeter1   Area1  Smoothness1  \\\n",
              "0     842517         M    20.57     17.77      132.90  1326.0      0.08474   \n",
              "1   84300903         M    19.69     21.25      130.00  1203.0      0.10960   \n",
              "2   84348301         M    11.42     20.38       77.58   386.1      0.14250   \n",
              "3   84358402         M    20.29     14.34      135.10  1297.0      0.10030   \n",
              "4     843786         M    12.45     15.70       82.57   477.1      0.12780   \n",
              "\n",
              "   Compactness1  Concavity1  Concave Points1  ...  Radius3  Texture3  \\\n",
              "0       0.07864      0.0869          0.07017  ...    24.99     23.41   \n",
              "1       0.15990      0.1974          0.12790  ...    23.57     25.53   \n",
              "2       0.28390      0.2414          0.10520  ...    14.91     26.50   \n",
              "3       0.13280      0.1980          0.10430  ...    22.54     16.67   \n",
              "4       0.17000      0.1578          0.08089  ...    15.47     23.75   \n",
              "\n",
              "   Perimeter3   Area3  Smoothness3  Compactness3  Concavity3  Concave Points3  \\\n",
              "0      158.80  1956.0       0.1238        0.1866      0.2416           0.1860   \n",
              "1      152.50  1709.0       0.1444        0.4245      0.4504           0.2430   \n",
              "2       98.87   567.7       0.2098        0.8663      0.6869           0.2575   \n",
              "3      152.20  1575.0       0.1374        0.2050      0.4000           0.1625   \n",
              "4      103.40   741.6       0.1791        0.5249      0.5355           0.1741   \n",
              "\n",
              "   Symmetry3  Fractal Dimension3  \n",
              "0     0.2750             0.08902  \n",
              "1     0.3613             0.08758  \n",
              "2     0.6638             0.17300  \n",
              "3     0.2364             0.07678  \n",
              "4     0.3985             0.12440  \n",
              "\n",
              "[5 rows x 32 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-1f848a0d-d93a-430b-b4aa-6f6e794cd193\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ID_Number</th>\n",
              "      <th>Diagnosis</th>\n",
              "      <th>Radius1</th>\n",
              "      <th>Texture1</th>\n",
              "      <th>Perimeter1</th>\n",
              "      <th>Area1</th>\n",
              "      <th>Smoothness1</th>\n",
              "      <th>Compactness1</th>\n",
              "      <th>Concavity1</th>\n",
              "      <th>Concave Points1</th>\n",
              "      <th>...</th>\n",
              "      <th>Radius3</th>\n",
              "      <th>Texture3</th>\n",
              "      <th>Perimeter3</th>\n",
              "      <th>Area3</th>\n",
              "      <th>Smoothness3</th>\n",
              "      <th>Compactness3</th>\n",
              "      <th>Concavity3</th>\n",
              "      <th>Concave Points3</th>\n",
              "      <th>Symmetry3</th>\n",
              "      <th>Fractal Dimension3</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>842517</td>\n",
              "      <td>M</td>\n",
              "      <td>20.57</td>\n",
              "      <td>17.77</td>\n",
              "      <td>132.90</td>\n",
              "      <td>1326.0</td>\n",
              "      <td>0.08474</td>\n",
              "      <td>0.07864</td>\n",
              "      <td>0.0869</td>\n",
              "      <td>0.07017</td>\n",
              "      <td>...</td>\n",
              "      <td>24.99</td>\n",
              "      <td>23.41</td>\n",
              "      <td>158.80</td>\n",
              "      <td>1956.0</td>\n",
              "      <td>0.1238</td>\n",
              "      <td>0.1866</td>\n",
              "      <td>0.2416</td>\n",
              "      <td>0.1860</td>\n",
              "      <td>0.2750</td>\n",
              "      <td>0.08902</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>84300903</td>\n",
              "      <td>M</td>\n",
              "      <td>19.69</td>\n",
              "      <td>21.25</td>\n",
              "      <td>130.00</td>\n",
              "      <td>1203.0</td>\n",
              "      <td>0.10960</td>\n",
              "      <td>0.15990</td>\n",
              "      <td>0.1974</td>\n",
              "      <td>0.12790</td>\n",
              "      <td>...</td>\n",
              "      <td>23.57</td>\n",
              "      <td>25.53</td>\n",
              "      <td>152.50</td>\n",
              "      <td>1709.0</td>\n",
              "      <td>0.1444</td>\n",
              "      <td>0.4245</td>\n",
              "      <td>0.4504</td>\n",
              "      <td>0.2430</td>\n",
              "      <td>0.3613</td>\n",
              "      <td>0.08758</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>84348301</td>\n",
              "      <td>M</td>\n",
              "      <td>11.42</td>\n",
              "      <td>20.38</td>\n",
              "      <td>77.58</td>\n",
              "      <td>386.1</td>\n",
              "      <td>0.14250</td>\n",
              "      <td>0.28390</td>\n",
              "      <td>0.2414</td>\n",
              "      <td>0.10520</td>\n",
              "      <td>...</td>\n",
              "      <td>14.91</td>\n",
              "      <td>26.50</td>\n",
              "      <td>98.87</td>\n",
              "      <td>567.7</td>\n",
              "      <td>0.2098</td>\n",
              "      <td>0.8663</td>\n",
              "      <td>0.6869</td>\n",
              "      <td>0.2575</td>\n",
              "      <td>0.6638</td>\n",
              "      <td>0.17300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>84358402</td>\n",
              "      <td>M</td>\n",
              "      <td>20.29</td>\n",
              "      <td>14.34</td>\n",
              "      <td>135.10</td>\n",
              "      <td>1297.0</td>\n",
              "      <td>0.10030</td>\n",
              "      <td>0.13280</td>\n",
              "      <td>0.1980</td>\n",
              "      <td>0.10430</td>\n",
              "      <td>...</td>\n",
              "      <td>22.54</td>\n",
              "      <td>16.67</td>\n",
              "      <td>152.20</td>\n",
              "      <td>1575.0</td>\n",
              "      <td>0.1374</td>\n",
              "      <td>0.2050</td>\n",
              "      <td>0.4000</td>\n",
              "      <td>0.1625</td>\n",
              "      <td>0.2364</td>\n",
              "      <td>0.07678</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>843786</td>\n",
              "      <td>M</td>\n",
              "      <td>12.45</td>\n",
              "      <td>15.70</td>\n",
              "      <td>82.57</td>\n",
              "      <td>477.1</td>\n",
              "      <td>0.12780</td>\n",
              "      <td>0.17000</td>\n",
              "      <td>0.1578</td>\n",
              "      <td>0.08089</td>\n",
              "      <td>...</td>\n",
              "      <td>15.47</td>\n",
              "      <td>23.75</td>\n",
              "      <td>103.40</td>\n",
              "      <td>741.6</td>\n",
              "      <td>0.1791</td>\n",
              "      <td>0.5249</td>\n",
              "      <td>0.5355</td>\n",
              "      <td>0.1741</td>\n",
              "      <td>0.3985</td>\n",
              "      <td>0.12440</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows Ã— 32 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-1f848a0d-d93a-430b-b4aa-6f6e794cd193')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-1f848a0d-d93a-430b-b4aa-6f6e794cd193 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-1f848a0d-d93a-430b-b4aa-6f6e794cd193');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 145
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data.describe()"
      ],
      "metadata": {
        "id": "3P0TQ6VzcAcg",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 411
        },
        "outputId": "cd6227af-9ba8-4e14-f64a-59377561db3b"
      },
      "execution_count": 146,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "          ID_Number     Radius1    Texture1  Perimeter1        Area1  \\\n",
              "count  5.680000e+02  568.000000  568.000000  568.000000   568.000000   \n",
              "mean   3.042382e+07   14.120491   19.305335   91.914754   654.279754   \n",
              "std    1.251246e+08    3.523416    4.288506   24.285848   351.923751   \n",
              "min    8.670000e+03    6.981000    9.710000   43.790000   143.500000   \n",
              "25%    8.692225e+05   11.697500   16.177500   75.135000   420.175000   \n",
              "50%    9.061570e+05   13.355000   18.855000   86.210000   548.750000   \n",
              "75%    8.825022e+06   15.780000   21.802500  103.875000   782.625000   \n",
              "max    9.113205e+08   28.110000   39.280000  188.500000  2501.000000   \n",
              "\n",
              "       Smoothness1  Compactness1  Concavity1  Concave Points1   Symmetry1  \\\n",
              "count   568.000000    568.000000  568.000000       568.000000  568.000000   \n",
              "mean      0.096321      0.104036    0.088427         0.048746    0.181055   \n",
              "std       0.014046      0.052355    0.079294         0.038617    0.027319   \n",
              "min       0.052630      0.019380    0.000000         0.000000    0.106000   \n",
              "25%       0.086290      0.064815    0.029540         0.020310    0.161900   \n",
              "50%       0.095865      0.092525    0.061400         0.033455    0.179200   \n",
              "75%       0.105300      0.130400    0.129650         0.073730    0.195625   \n",
              "max       0.163400      0.345400    0.426800         0.201200    0.304000   \n",
              "\n",
              "       ...    Radius3    Texture3  Perimeter3        Area3  Smoothness3  \\\n",
              "count  ...  568.00000  568.000000  568.000000   568.000000   568.000000   \n",
              "mean   ...   16.25315   25.691919  107.125053   878.578873     0.132316   \n",
              "std    ...    4.82232    6.141662   33.474687   567.846267     0.022818   \n",
              "min    ...    7.93000   12.020000   50.410000   185.200000     0.071170   \n",
              "25%    ...   13.01000   21.095000   84.102500   514.975000     0.116600   \n",
              "50%    ...   14.96500   25.425000   97.655000   685.550000     0.131300   \n",
              "75%    ...   18.76750   29.757500  125.175000  1073.500000     0.146000   \n",
              "max    ...   36.04000   49.540000  251.200000  4254.000000     0.222600   \n",
              "\n",
              "       Compactness3  Concavity3  Concave Points3   Symmetry3  \\\n",
              "count    568.000000  568.000000       568.000000  568.000000   \n",
              "mean       0.253541    0.271414         0.114341    0.289776   \n",
              "std        0.156523    0.207989         0.065484    0.061508   \n",
              "min        0.027290    0.000000         0.000000    0.156500   \n",
              "25%        0.146900    0.114475         0.064730    0.250350   \n",
              "50%        0.211850    0.226550         0.099840    0.282050   \n",
              "75%        0.337600    0.381400         0.161325    0.317675   \n",
              "max        1.058000    1.252000         0.291000    0.663800   \n",
              "\n",
              "       Fractal Dimension3  \n",
              "count          568.000000  \n",
              "mean             0.083884  \n",
              "std              0.018017  \n",
              "min              0.055040  \n",
              "25%              0.071412  \n",
              "50%              0.080015  \n",
              "75%              0.092065  \n",
              "max              0.207500  \n",
              "\n",
              "[8 rows x 31 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-875dad09-3ca3-467d-884c-df1221972e88\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ID_Number</th>\n",
              "      <th>Radius1</th>\n",
              "      <th>Texture1</th>\n",
              "      <th>Perimeter1</th>\n",
              "      <th>Area1</th>\n",
              "      <th>Smoothness1</th>\n",
              "      <th>Compactness1</th>\n",
              "      <th>Concavity1</th>\n",
              "      <th>Concave Points1</th>\n",
              "      <th>Symmetry1</th>\n",
              "      <th>...</th>\n",
              "      <th>Radius3</th>\n",
              "      <th>Texture3</th>\n",
              "      <th>Perimeter3</th>\n",
              "      <th>Area3</th>\n",
              "      <th>Smoothness3</th>\n",
              "      <th>Compactness3</th>\n",
              "      <th>Concavity3</th>\n",
              "      <th>Concave Points3</th>\n",
              "      <th>Symmetry3</th>\n",
              "      <th>Fractal Dimension3</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>5.680000e+02</td>\n",
              "      <td>568.000000</td>\n",
              "      <td>568.000000</td>\n",
              "      <td>568.000000</td>\n",
              "      <td>568.000000</td>\n",
              "      <td>568.000000</td>\n",
              "      <td>568.000000</td>\n",
              "      <td>568.000000</td>\n",
              "      <td>568.000000</td>\n",
              "      <td>568.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>568.00000</td>\n",
              "      <td>568.000000</td>\n",
              "      <td>568.000000</td>\n",
              "      <td>568.000000</td>\n",
              "      <td>568.000000</td>\n",
              "      <td>568.000000</td>\n",
              "      <td>568.000000</td>\n",
              "      <td>568.000000</td>\n",
              "      <td>568.000000</td>\n",
              "      <td>568.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>3.042382e+07</td>\n",
              "      <td>14.120491</td>\n",
              "      <td>19.305335</td>\n",
              "      <td>91.914754</td>\n",
              "      <td>654.279754</td>\n",
              "      <td>0.096321</td>\n",
              "      <td>0.104036</td>\n",
              "      <td>0.088427</td>\n",
              "      <td>0.048746</td>\n",
              "      <td>0.181055</td>\n",
              "      <td>...</td>\n",
              "      <td>16.25315</td>\n",
              "      <td>25.691919</td>\n",
              "      <td>107.125053</td>\n",
              "      <td>878.578873</td>\n",
              "      <td>0.132316</td>\n",
              "      <td>0.253541</td>\n",
              "      <td>0.271414</td>\n",
              "      <td>0.114341</td>\n",
              "      <td>0.289776</td>\n",
              "      <td>0.083884</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>1.251246e+08</td>\n",
              "      <td>3.523416</td>\n",
              "      <td>4.288506</td>\n",
              "      <td>24.285848</td>\n",
              "      <td>351.923751</td>\n",
              "      <td>0.014046</td>\n",
              "      <td>0.052355</td>\n",
              "      <td>0.079294</td>\n",
              "      <td>0.038617</td>\n",
              "      <td>0.027319</td>\n",
              "      <td>...</td>\n",
              "      <td>4.82232</td>\n",
              "      <td>6.141662</td>\n",
              "      <td>33.474687</td>\n",
              "      <td>567.846267</td>\n",
              "      <td>0.022818</td>\n",
              "      <td>0.156523</td>\n",
              "      <td>0.207989</td>\n",
              "      <td>0.065484</td>\n",
              "      <td>0.061508</td>\n",
              "      <td>0.018017</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>8.670000e+03</td>\n",
              "      <td>6.981000</td>\n",
              "      <td>9.710000</td>\n",
              "      <td>43.790000</td>\n",
              "      <td>143.500000</td>\n",
              "      <td>0.052630</td>\n",
              "      <td>0.019380</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.106000</td>\n",
              "      <td>...</td>\n",
              "      <td>7.93000</td>\n",
              "      <td>12.020000</td>\n",
              "      <td>50.410000</td>\n",
              "      <td>185.200000</td>\n",
              "      <td>0.071170</td>\n",
              "      <td>0.027290</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.156500</td>\n",
              "      <td>0.055040</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>8.692225e+05</td>\n",
              "      <td>11.697500</td>\n",
              "      <td>16.177500</td>\n",
              "      <td>75.135000</td>\n",
              "      <td>420.175000</td>\n",
              "      <td>0.086290</td>\n",
              "      <td>0.064815</td>\n",
              "      <td>0.029540</td>\n",
              "      <td>0.020310</td>\n",
              "      <td>0.161900</td>\n",
              "      <td>...</td>\n",
              "      <td>13.01000</td>\n",
              "      <td>21.095000</td>\n",
              "      <td>84.102500</td>\n",
              "      <td>514.975000</td>\n",
              "      <td>0.116600</td>\n",
              "      <td>0.146900</td>\n",
              "      <td>0.114475</td>\n",
              "      <td>0.064730</td>\n",
              "      <td>0.250350</td>\n",
              "      <td>0.071412</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>9.061570e+05</td>\n",
              "      <td>13.355000</td>\n",
              "      <td>18.855000</td>\n",
              "      <td>86.210000</td>\n",
              "      <td>548.750000</td>\n",
              "      <td>0.095865</td>\n",
              "      <td>0.092525</td>\n",
              "      <td>0.061400</td>\n",
              "      <td>0.033455</td>\n",
              "      <td>0.179200</td>\n",
              "      <td>...</td>\n",
              "      <td>14.96500</td>\n",
              "      <td>25.425000</td>\n",
              "      <td>97.655000</td>\n",
              "      <td>685.550000</td>\n",
              "      <td>0.131300</td>\n",
              "      <td>0.211850</td>\n",
              "      <td>0.226550</td>\n",
              "      <td>0.099840</td>\n",
              "      <td>0.282050</td>\n",
              "      <td>0.080015</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>8.825022e+06</td>\n",
              "      <td>15.780000</td>\n",
              "      <td>21.802500</td>\n",
              "      <td>103.875000</td>\n",
              "      <td>782.625000</td>\n",
              "      <td>0.105300</td>\n",
              "      <td>0.130400</td>\n",
              "      <td>0.129650</td>\n",
              "      <td>0.073730</td>\n",
              "      <td>0.195625</td>\n",
              "      <td>...</td>\n",
              "      <td>18.76750</td>\n",
              "      <td>29.757500</td>\n",
              "      <td>125.175000</td>\n",
              "      <td>1073.500000</td>\n",
              "      <td>0.146000</td>\n",
              "      <td>0.337600</td>\n",
              "      <td>0.381400</td>\n",
              "      <td>0.161325</td>\n",
              "      <td>0.317675</td>\n",
              "      <td>0.092065</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>9.113205e+08</td>\n",
              "      <td>28.110000</td>\n",
              "      <td>39.280000</td>\n",
              "      <td>188.500000</td>\n",
              "      <td>2501.000000</td>\n",
              "      <td>0.163400</td>\n",
              "      <td>0.345400</td>\n",
              "      <td>0.426800</td>\n",
              "      <td>0.201200</td>\n",
              "      <td>0.304000</td>\n",
              "      <td>...</td>\n",
              "      <td>36.04000</td>\n",
              "      <td>49.540000</td>\n",
              "      <td>251.200000</td>\n",
              "      <td>4254.000000</td>\n",
              "      <td>0.222600</td>\n",
              "      <td>1.058000</td>\n",
              "      <td>1.252000</td>\n",
              "      <td>0.291000</td>\n",
              "      <td>0.663800</td>\n",
              "      <td>0.207500</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>8 rows Ã— 31 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-875dad09-3ca3-467d-884c-df1221972e88')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-875dad09-3ca3-467d-884c-df1221972e88 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-875dad09-3ca3-467d-884c-df1221972e88');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 146
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data.info()"
      ],
      "metadata": {
        "id": "0cSiiGTtcCGM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2b702c71-199a-41c8-ee4a-aa27f9a0c802"
      },
      "execution_count": 147,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 568 entries, 0 to 567\n",
            "Data columns (total 32 columns):\n",
            " #   Column              Non-Null Count  Dtype  \n",
            "---  ------              --------------  -----  \n",
            " 0   ID_Number           568 non-null    int64  \n",
            " 1   Diagnosis           568 non-null    object \n",
            " 2   Radius1             568 non-null    float64\n",
            " 3   Texture1            568 non-null    float64\n",
            " 4   Perimeter1          568 non-null    float64\n",
            " 5   Area1               568 non-null    float64\n",
            " 6   Smoothness1         568 non-null    float64\n",
            " 7   Compactness1        568 non-null    float64\n",
            " 8   Concavity1          568 non-null    float64\n",
            " 9   Concave Points1     568 non-null    float64\n",
            " 10  Symmetry1           568 non-null    float64\n",
            " 11  Fractal Dimension1  568 non-null    float64\n",
            " 12  Radius2             568 non-null    float64\n",
            " 13  Texture2            568 non-null    float64\n",
            " 14  Perimeter2          568 non-null    float64\n",
            " 15  Area2               568 non-null    float64\n",
            " 16  Smoothness2         568 non-null    float64\n",
            " 17  Compactness2        568 non-null    float64\n",
            " 18  Concavity2          568 non-null    float64\n",
            " 19  Concave Points2     568 non-null    float64\n",
            " 20  Symmetry2           568 non-null    float64\n",
            " 21  Fractal Dimension2  568 non-null    float64\n",
            " 22  Radius3             568 non-null    float64\n",
            " 23  Texture3            568 non-null    float64\n",
            " 24  Perimeter3          568 non-null    float64\n",
            " 25  Area3               568 non-null    float64\n",
            " 26  Smoothness3         568 non-null    float64\n",
            " 27  Compactness3        568 non-null    float64\n",
            " 28  Concavity3          568 non-null    float64\n",
            " 29  Concave Points3     568 non-null    float64\n",
            " 30  Symmetry3           568 non-null    float64\n",
            " 31  Fractal Dimension3  568 non-null    float64\n",
            "dtypes: float64(30), int64(1), object(1)\n",
            "memory usage: 142.1+ KB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#data = data.drop('ID_Number', axis=1)\n",
        "#data = data.drop('Radius1', axis=1)\n",
        "#data = data.drop('Texture1', axis=1)\n",
        "#data = data.drop('Perimeter1', axis=1)\n",
        "#data = data.drop('Area1', axis=1)\n",
        "#data = data.drop('Smoothness1', axis=1)\n",
        "#data = data.drop('Compactness1', axis=1)\n",
        "#data = data.drop('Concavity1', axis=1)\n",
        "#data = data.drop('Concave Points1', axis=1)\n",
        "#data = data.drop('Symmetry1', axis=1)\n",
        "##data = data.drop('Fractal Dimension1', axis=1)\n",
        "#data = data.drop('Radius2', axis=1)\n",
        "#data = data.drop('Texture2', axis=1)\n",
        "#data = data.drop('Perimeter2', axis=1)\n",
        "#data = data.drop('Area2', axis=1)\n",
        "#data = data.drop('Smoothness2', axis=1)\n",
        "#data = data.drop('Compactness2', axis=1)\n",
        "#data = data.drop('Concavity2', axis=1)\n",
        "##data = data.drop('Concave Points2', axis=1)\n",
        "#data = data.drop('Symmetry2', axis=1)\n",
        "#data = data.drop('Fractal Dimension2', axis=1)\n",
        "\n"
      ],
      "metadata": {
        "id": "godx7qJfcF2U"
      },
      "execution_count": 148,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#data = data.drop('DiagnosisBinary', axis=1)\n",
        "#data.head()"
      ],
      "metadata": {
        "id": "UKE4Uhk3xyJF"
      },
      "execution_count": 150,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Ytest #= data[\"Diagnosis\"] \n",
        "data[\"Diagnosis\"] = np.where(data[\"Diagnosis\"] == \"M\", 1, 0)\n",
        "#data[\"DiagnosisBinary\"] = np.where(data[\"Diagnosis\"] == 1, 0, 1)\n",
        "#print(Ytest)\n",
        "Y = np.array(data[[\"Diagnosis\"]]) # X is the input data (does not include the outcome) \n",
        "#YBinary = np.array(data[[\"DiagnosisBinary\"]])\n",
        "#print(temp) \n",
        "\n",
        "#for i in range(0, len(Y)):\n",
        "#  if Y[i][0] == \"M\" :\n",
        "#    Y[i][0] = 1\n",
        "#  elif Y[i][0] == \"B\":\n",
        "#    Y[i][0] = 0\n",
        "\n",
        "print(Y)\n",
        "#print(YBinary)\n",
        "\n",
        "X = np.array(data[[\"Radius1\",\"Texture1\",\"Perimeter1\",\"Area1\",\"Smoothness1\",\"Compactness1\",\"Concavity1\",\"Concave Points1\",\"Symmetry1\",\"Fractal Dimension1\",\"Radius2\",\"Texture2\",\"Perimeter2\",\"Area2\",\"Smoothness2\",\"Compactness2\",\"Concavity2\",\"Concave Points2\",\"Symmetry2\",\"Fractal Dimension2\",\"Radius3\",\"Texture3\",\"Perimeter3\",\"Area3\",\"Smoothness3\",\"Compactness3\",\"Concavity3\",\"Concave Points3\",\"Symmetry3\",\"Fractal Dimension3\"]]) # X is the input data (does not include the outcome)\n",
        "print(X)"
      ],
      "metadata": {
        "id": "zQhTAPnth9FI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "79d13069-457c-4c29-a71f-ce539f727e1d"
      },
      "execution_count": 151,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [0]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [0]\n",
            " [1]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [1]\n",
            " [1]\n",
            " [0]\n",
            " [1]\n",
            " [1]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [1]\n",
            " [0]\n",
            " [1]\n",
            " [1]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [1]\n",
            " [0]\n",
            " [1]\n",
            " [1]\n",
            " [0]\n",
            " [1]\n",
            " [0]\n",
            " [1]\n",
            " [1]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [1]\n",
            " [1]\n",
            " [0]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [1]\n",
            " [0]\n",
            " [0]\n",
            " [1]\n",
            " [1]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [1]\n",
            " [1]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [1]\n",
            " [0]\n",
            " [0]\n",
            " [1]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [0]\n",
            " [1]\n",
            " [1]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [1]\n",
            " [1]\n",
            " [0]\n",
            " [1]\n",
            " [0]\n",
            " [1]\n",
            " [1]\n",
            " [0]\n",
            " [1]\n",
            " [1]\n",
            " [0]\n",
            " [0]\n",
            " [1]\n",
            " [0]\n",
            " [0]\n",
            " [1]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [1]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [1]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [1]\n",
            " [1]\n",
            " [0]\n",
            " [1]\n",
            " [0]\n",
            " [0]\n",
            " [1]\n",
            " [1]\n",
            " [0]\n",
            " [0]\n",
            " [1]\n",
            " [1]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [1]\n",
            " [0]\n",
            " [0]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [0]\n",
            " [1]\n",
            " [0]\n",
            " [1]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [1]\n",
            " [0]\n",
            " [0]\n",
            " [1]\n",
            " [1]\n",
            " [0]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [0]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [0]\n",
            " [1]\n",
            " [0]\n",
            " [1]\n",
            " [0]\n",
            " [0]\n",
            " [1]\n",
            " [0]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [0]\n",
            " [0]\n",
            " [1]\n",
            " [1]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [1]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [1]\n",
            " [1]\n",
            " [0]\n",
            " [0]\n",
            " [1]\n",
            " [0]\n",
            " [0]\n",
            " [1]\n",
            " [1]\n",
            " [0]\n",
            " [1]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [1]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [1]\n",
            " [0]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [1]\n",
            " [0]\n",
            " [1]\n",
            " [0]\n",
            " [0]\n",
            " [1]\n",
            " [0]\n",
            " [0]\n",
            " [1]\n",
            " [0]\n",
            " [1]\n",
            " [1]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [1]\n",
            " [0]\n",
            " [0]\n",
            " [1]\n",
            " [0]\n",
            " [1]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [1]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [1]\n",
            " [0]\n",
            " [1]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [1]\n",
            " [0]\n",
            " [1]\n",
            " [0]\n",
            " [1]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [1]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [1]\n",
            " [1]\n",
            " [0]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [0]\n",
            " [1]\n",
            " [1]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [1]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [1]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [1]\n",
            " [0]\n",
            " [0]\n",
            " [1]\n",
            " [1]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [1]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [1]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [1]\n",
            " [0]\n",
            " [0]\n",
            " [1]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [1]\n",
            " [0]\n",
            " [1]\n",
            " [1]\n",
            " [0]\n",
            " [1]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [1]\n",
            " [0]\n",
            " [0]\n",
            " [1]\n",
            " [0]\n",
            " [1]\n",
            " [0]\n",
            " [0]\n",
            " [1]\n",
            " [0]\n",
            " [1]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [1]\n",
            " [1]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [1]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [1]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [1]\n",
            " [0]\n",
            " [1]\n",
            " [0]\n",
            " [0]\n",
            " [1]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [1]\n",
            " [1]\n",
            " [0]\n",
            " [1]\n",
            " [0]\n",
            " [1]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [1]\n",
            " [0]\n",
            " [0]\n",
            " [1]\n",
            " [0]\n",
            " [1]\n",
            " [0]\n",
            " [1]\n",
            " [1]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [1]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [1]\n",
            " [0]\n",
            " [1]\n",
            " [1]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [0]]\n",
            "[[2.057e+01 1.777e+01 1.329e+02 ... 1.860e-01 2.750e-01 8.902e-02]\n",
            " [1.969e+01 2.125e+01 1.300e+02 ... 2.430e-01 3.613e-01 8.758e-02]\n",
            " [1.142e+01 2.038e+01 7.758e+01 ... 2.575e-01 6.638e-01 1.730e-01]\n",
            " ...\n",
            " [1.660e+01 2.808e+01 1.083e+02 ... 1.418e-01 2.218e-01 7.820e-02]\n",
            " [2.060e+01 2.933e+01 1.401e+02 ... 2.650e-01 4.087e-01 1.240e-01]\n",
            " [7.760e+00 2.454e+01 4.792e+01 ... 0.000e+00 2.871e-01 7.039e-02]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalise the inputs \n",
        "XNorm = (X - np.amin(X)) / (np.amax(X) - np.amin(X))\n",
        "print(XNorm)  # Sanity check\n",
        "X = XNorm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X-MpPEW8OV1Q",
        "outputId": "159a7449-b3ff-448d-f124-3a6ebcfa7118"
      },
      "execution_count": 152,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[4.83544899e-03 4.17724495e-03 3.12411848e-02 ... 4.37235543e-05\n",
            "  6.46450400e-05 2.09261871e-05]\n",
            " [4.62858486e-03 4.99529854e-03 3.05594734e-02 ... 5.71227080e-05\n",
            "  8.49318289e-05 2.05876822e-05]\n",
            " [2.68453220e-03 4.79078514e-03 1.82369535e-02 ... 6.05312647e-05\n",
            "  1.56041373e-04 4.06676070e-05]\n",
            " ...\n",
            " [3.90220969e-03 6.60084626e-03 2.54583921e-02 ... 3.33333333e-05\n",
            "  5.21391631e-05 1.83826986e-05]\n",
            " [4.84250118e-03 6.89468735e-03 3.29337094e-02 ... 6.22943112e-05\n",
            "  9.60742830e-05 2.91490362e-05]\n",
            " [1.82416549e-03 5.76868829e-03 1.12646921e-02 ... 0.00000000e+00\n",
            "  6.74894217e-05 1.65467795e-05]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Main"
      ],
      "metadata": {
        "id": "qbL2MLHuUSDn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Main"
      ],
      "metadata": {
        "id": "II5G6ueqcDuS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "start = time.time()\n",
        "print(\"hello\")\n",
        "end = time.time()\n",
        "print(end - start)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VczvPmj-zYzY",
        "outputId": "4b822340-0c13-426b-ef43-4c73bdafdd55"
      },
      "execution_count": 153,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hello\n",
            "0.00014472007751464844\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# DO NOT RUN - Testing python stuff \n",
        "a = np.array([[[0]], [[1]], [[1]], [[4]]])\n",
        "b = a.reshape(len(a), len(a[0]))\n",
        "print(a)\n",
        "print(b)\n",
        "np.tanh(-0.1)"
      ],
      "metadata": {
        "id": "YYOaQxDSn3fQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c4a1a2a0-41da-4c8f-81a0-324c2766445c"
      },
      "execution_count": 154,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[[0]]\n",
            "\n",
            " [[1]]\n",
            "\n",
            " [[1]]\n",
            "\n",
            " [[4]]]\n",
            "[[0]\n",
            " [1]\n",
            " [1]\n",
            " [4]]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "-0.09966799462495582"
            ]
          },
          "metadata": {},
          "execution_count": 154
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# DO NOT RUN - Binary output stuff, not ready yet\n",
        "\n",
        "x_train = np.array([[[0,0]], [[0,1]], [[1,0]], [[1,1]], [[1,1]]])\n",
        "\n",
        "y_trainA = np.array([[[0]], [[1]], [[1]], [[0]], [[0]]])\n",
        "y_trainB = np.array([[[1]], [[0]], [[0]], [[1]], [[1]],])\n",
        "\n",
        "y_train = np.concatenate((y_trainA, y_trainB), axis=2)\n",
        "\n",
        "print(y_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jgNTGQ-fNPEg",
        "outputId": "27d76114-47b5-4f61-b240-d7d7482c5f19"
      },
      "execution_count": 155,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[[0 1]]\n",
            "\n",
            " [[1 0]]\n",
            "\n",
            " [[1 0]]\n",
            "\n",
            " [[0 1]]\n",
            "\n",
            " [[0 1]]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# DO NOT RUN - NO LONGER WORKS \n",
        "\n",
        "# USES SIMPLE XOR DATA - Runs quicker if you want to see impact of different hyperparameters \n",
        "\n",
        "# training data\n",
        "x_train = np.array([[[0,0]], [[0,1]], [[1,0]], [[1,1]], [[1,1]]])\n",
        "y_train = np.array([[[0]], [[1]], [[1]], [[0]], [[0]]])\n",
        "#y_trainA = np.array([[[0]], [[1]], [[1]], [[0]], [[0]]])\n",
        "#y_trainB = np.array([[[1]], [[0]], [[0]], [[1]], [[1]],])\n",
        "\n",
        "#y_train = zip(y_trainA, y_trainB)\n",
        "\n",
        "#y_train = np.array([[[1, 0]], [[0, 1]], [[0, 1]], [[1, 0]], [[1, 0]]])\n",
        "\n",
        "# network\n",
        "\n",
        "startTime = time.time()\n",
        "\n",
        "net = Network()\n",
        "\n",
        "\n",
        "net.add(Dense(2, 5))\n",
        "net.add(Activation(tanh, tanhDerv))\n",
        "net.add(Dense(5, 15))\n",
        "net.add(Activation(tanh, tanhDerv))\n",
        "net.add(Dense(15, 5))\n",
        "net.add(Activation(tanh, tanhDerv))\n",
        "net.add(Dense(5, 1))\n",
        "net.add(Activation(tanh, tanhDerv))\n",
        "\n",
        "net.use(mse, mseDerv)\n",
        "errorStore = net.fit(x_train, y_train, epochs=1000, learning_rate=0.01)\n",
        "endTime = time.time()\n",
        "print(\"last error is \", errorStore[-1])\n",
        "\n",
        "#plot graph of error vs epochs \n",
        "xPlot = np.array(range(0, 1000))\n",
        "yPlot = errorStore\n",
        "plt.title(\"Plotting 1-D array\")\n",
        "plt.xlabel(\"X axis\")\n",
        "plt.ylabel(\"Y axis\")\n",
        "plt.plot(xPlot, yPlot, color = \"red\", label = \"Array elements\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "print(\"Time taken is \", endTime - startTime)\n",
        "\n",
        "\n",
        "#print(out)\n",
        "#print(y_train.T)\n",
        "out = net.predict(x_train)\n",
        "print(out)\n",
        "#print(out[0])\n",
        "\n",
        "def answerFn(out):\n",
        "  p = np.zeros([len(x_train)])\n",
        "  for i in range(len(x_train)):\n",
        "    if out[i] < 0.5:\n",
        "      p[i] = 0\n",
        "    else:\n",
        "      p[i] = 1\n",
        "  return p\n",
        "\n",
        "pred = answerFn(out)\n",
        "expt = y_train.reshape(len(y_train)) * 1.0\n",
        "print(\"pred \", pred)\n",
        "print(\"expect \", expt)\n",
        "#res = pd.DataFrame()\n",
        "#expd = y_train.reshape(len(y_train)).T\n",
        "#pred = out2\n",
        "##pred = np.concatenate( out, axis=0 )\n",
        "##pred = np.array(pred)\n",
        "##pred = np.array(out).T\n",
        "#print(\"expected \", expd)\n",
        "#print(\"predicted \", pred)\n",
        "#res = pd.DataFrame()\n",
        "#res['Predictions'] = pred\n",
        "#res['Expectation'] = expd\n",
        "#print(res)\n",
        "#print(\"Accuracy: \", res.loc[res['Predictions']==res['Expectation']].shape[0] / res.shape[0] * 100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "id": "Mel5BWH0aYZn",
        "outputId": "64a5595e-42a9-4631-eb6a-6d080a7bb522"
      },
      "execution_count": 185,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-185-132fb629e0e5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmseDerv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m \u001b[0merrorStore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0mendTime\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"last error is \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrorStore\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'Network' object has no attribute 'fit'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# USES wdbc.data\n",
        "\n",
        "# THE ONLY MAIN THAT MATTERS\n",
        "\n",
        "# training data\n",
        "#x_train = np.array([[[0,0]], [[0,1]], [[1,0]], [[1,1]], [[1,1]]])\n",
        "#print(x_train)\n",
        "\n",
        "#y_train = np.array([[[0]], [[1]], [[1]], [[0]], [[0]]])\n",
        "#y_train = np.array([[[1, 0]], [[0, 1]], [[0, 1]], [[1, 0]], [[1, 0]]])\n",
        "#print(len(x_train[0][0]))\n",
        "x_train = X.reshape(len(X), len(X[0]), 1)\n",
        "y_train = Y.reshape(len(Y), 1)\n",
        "\n",
        "#print(x_train)\n",
        "#print(\"length of input 0\", len(x_train))\n",
        "#print(\"length of input 1\", len(x_train[0]))\n",
        "#print(\"length of input 2\", len(x_train[0][0]))\n",
        "\n",
        "\n",
        "#if len(Y) == len(X): \n",
        "#  print(\"lol\")\n",
        "\n",
        "startTimeTotal = time.time()\n",
        "\n",
        "# network\n",
        "net = Network()\n",
        "\n",
        "\n",
        "#net.add(Dense(2, 5))\n",
        "#net.add(Activation(tanh, tanhDerv))\n",
        "#net.add(Dense(5, 15))\n",
        "#net.add(Activation(tanh, tanhDerv))\n",
        "#net.add(Dense(15, 5))\n",
        "#net.add(Activation(tanh, tanhDerv))\n",
        "#net.add(Dense(5, 2))\n",
        "#net.add(Activation(tanh, tanhDerv))\n",
        "\n",
        "net.add(Dense(len(x_train[0]), 120))\n",
        "net.add(Activation(tanh, tanhDerv))\n",
        "#net.add(Dense(120, 120))\n",
        "#net.add(Activation(tanh, tanhDerv))\n",
        "net.add(Dense(120, 5))\n",
        "net.add(Activation(tanh, tanhDerv))\n",
        "net.add(Dense(5, 1))\n",
        "net.add(Activation(tanh, tanhDerv))\n",
        "#net.add(Dense(10, 1))\n",
        "#net.add(Activation(tanh, tanhDerv))\n",
        "\n",
        "# train\n",
        "net.use(mse, mseDerv)\n",
        "\n",
        "startTimeTrain = time.time()\n",
        "\n",
        "#different gradient descent methods, min batch has an adjustable batch size \n",
        "\n",
        "#errorStore = net.fitSGD(x_train, y_train, epochs = 1000, learning_rate = 0.00001)\n",
        "#errorStore = net.fitBatch(x_train, y_train, epochs = 1000, learning_rate = 0.00001)\n",
        "errorStore = net.fitMinBatch(x_train, y_train, epochs = 1000, learning_rate = 0.001, batchSize = 50)\n",
        "#endTimeHere = time.time()\n",
        "\n",
        "endTimeTrain = time.time()\n",
        "print(\"last error is \", errorStore[-1])\n",
        "xPlot = np.array(range(0, 1000))\n",
        "yPlot = errorStore\n",
        "plt.title(\"Plotting 1-D array\")\n",
        "plt.xlabel(\"X axis\")\n",
        "plt.ylabel(\"Y axis\")\n",
        "plt.plot(xPlot, yPlot, color = \"red\", label = \"Array elements\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "# test\n",
        "#out = net.predict(x_train)\n",
        "\n",
        "#print(\"Time taken is \", endTimeHere - startTime)\n",
        "\n",
        "#print(y_train.T)\n",
        "\n",
        "startTimePredict = time.time()\n",
        "\n",
        "out = net.predict(x_train)\n",
        "\n",
        "endTimePredict = time.time()\n",
        "print(out)\n",
        "#print(out[0])\n",
        "\n",
        "def answerFn(out):\n",
        "  p = np.zeros([len(x_train)])\n",
        "  for i in range(len(x_train)):\n",
        "    if out[i] < 0.5:\n",
        "      p[i] = 0\n",
        "    else:\n",
        "      p[i] = 1\n",
        "  return p\n",
        "\n",
        "pred = answerFn(out)\n",
        "expt = y_train.reshape(len(y_train)) * 1.0\n",
        "\n",
        "# Accuracy calculation \n",
        "\n",
        "TP = 0\n",
        "FP = 0\n",
        "FN = 0\n",
        "TN = 0\n",
        "\n",
        "for i in range(len(pred)):\n",
        "  if pred[i] == 1:\n",
        "    if expt[i] == 1:\n",
        "      TP += 1\n",
        "    else:\n",
        "      FP += 1\n",
        "  elif pred[i] == 0:\n",
        "    if expt[i] == 0:\n",
        "      TN += 1\n",
        "    else:\n",
        "      FN += 1\n",
        "\n",
        "accuracy = (TP + TN)/(TP+TN+FP+FN)\n",
        "\n",
        "print(\"The classification accuracy is \", accuracy*100, \"%\")\n",
        "endTimeTotal = time.time()\n",
        "\n",
        "\n",
        "print(\"Total time = \", endTimeTotal - startTimeTotal)\n",
        "print(\"Train time = \", endTimeTrain - startTimeTrain)\n",
        "print(\"Predict time = \", endTimePredict - startTimePredict)\n",
        "\n",
        "#print(\"pred \", pred)\n",
        "#print(\"expect \", expt)\n",
        "\n",
        "#useless comments below\n",
        "#print(out[0])\n",
        "#res = pd.DataFrame()\n",
        "#expd = y_train.reshape(len(y_train)).T\n",
        "#pred = out2\n",
        "##pred = np.concatenate( out, axis=0 )\n",
        "##pred = np.array(pred)\n",
        "##pred = np.array(out).T\n",
        "#print(\"expected \", expd)\n",
        "#print(\"predicted \", pred)\n",
        "#res = pd.DataFrame()\n",
        "#res['Predictions'] = pred\n",
        "#res['Expectation'] = expd\n",
        "#print(res)\n",
        "#print(\"Accuracy: \", res.loc[res['Predictions']==res['Expectation']].shape[0] / res.shape[0] * 100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "PG3Fd1zdaUVD",
        "outputId": "7a328442-95ab-4c8f-fd65-050d5d112f23"
      },
      "execution_count": 189,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 1/1000   error=0.026314\n",
            "epoch 2/1000   error=0.021624\n",
            "epoch 3/1000   error=0.021369\n",
            "epoch 4/1000   error=0.020681\n",
            "epoch 5/1000   error=0.021783\n",
            "epoch 6/1000   error=0.022823\n",
            "epoch 7/1000   error=0.018168\n",
            "epoch 8/1000   error=0.016022\n",
            "epoch 9/1000   error=0.025684\n",
            "epoch 10/1000   error=0.020668\n",
            "epoch 11/1000   error=0.025244\n",
            "epoch 12/1000   error=0.025560\n",
            "epoch 13/1000   error=0.025510\n",
            "epoch 14/1000   error=0.018794\n",
            "epoch 15/1000   error=0.023588\n",
            "epoch 16/1000   error=0.022669\n",
            "epoch 17/1000   error=0.021993\n",
            "epoch 18/1000   error=0.024286\n",
            "epoch 19/1000   error=0.020379\n",
            "epoch 20/1000   error=0.022328\n",
            "epoch 21/1000   error=0.020903\n",
            "epoch 22/1000   error=0.023010\n",
            "epoch 23/1000   error=0.023062\n",
            "epoch 24/1000   error=0.024980\n",
            "epoch 25/1000   error=0.026699\n",
            "epoch 26/1000   error=0.019428\n",
            "epoch 27/1000   error=0.020366\n",
            "epoch 28/1000   error=0.017838\n",
            "epoch 29/1000   error=0.022880\n",
            "epoch 30/1000   error=0.022708\n",
            "epoch 31/1000   error=0.026083\n",
            "epoch 32/1000   error=0.021714\n",
            "epoch 33/1000   error=0.021773\n",
            "epoch 34/1000   error=0.021556\n",
            "epoch 35/1000   error=0.021753\n",
            "epoch 36/1000   error=0.021805\n",
            "epoch 37/1000   error=0.025878\n",
            "epoch 38/1000   error=0.021635\n",
            "epoch 39/1000   error=0.020909\n",
            "epoch 40/1000   error=0.024202\n",
            "epoch 41/1000   error=0.020817\n",
            "epoch 42/1000   error=0.023086\n",
            "epoch 43/1000   error=0.019314\n",
            "epoch 44/1000   error=0.020674\n",
            "epoch 45/1000   error=0.020618\n",
            "epoch 46/1000   error=0.024235\n",
            "epoch 47/1000   error=0.021303\n",
            "epoch 48/1000   error=0.021664\n",
            "epoch 49/1000   error=0.021173\n",
            "epoch 50/1000   error=0.019112\n",
            "epoch 51/1000   error=0.019761\n",
            "epoch 52/1000   error=0.019103\n",
            "epoch 53/1000   error=0.018364\n",
            "epoch 54/1000   error=0.017664\n",
            "epoch 55/1000   error=0.022327\n",
            "epoch 56/1000   error=0.019581\n",
            "epoch 57/1000   error=0.024197\n",
            "epoch 58/1000   error=0.023603\n",
            "epoch 59/1000   error=0.019555\n",
            "epoch 60/1000   error=0.021599\n",
            "epoch 61/1000   error=0.022577\n",
            "epoch 62/1000   error=0.021523\n",
            "epoch 63/1000   error=0.022551\n",
            "epoch 64/1000   error=0.021514\n",
            "epoch 65/1000   error=0.026242\n",
            "epoch 66/1000   error=0.022907\n",
            "epoch 67/1000   error=0.020740\n",
            "epoch 68/1000   error=0.022308\n",
            "epoch 69/1000   error=0.022160\n",
            "epoch 70/1000   error=0.021003\n",
            "epoch 71/1000   error=0.020598\n",
            "epoch 72/1000   error=0.020572\n",
            "epoch 73/1000   error=0.018493\n",
            "epoch 74/1000   error=0.024235\n",
            "epoch 75/1000   error=0.018011\n",
            "epoch 76/1000   error=0.021640\n",
            "epoch 77/1000   error=0.018444\n",
            "epoch 78/1000   error=0.020567\n",
            "epoch 79/1000   error=0.016215\n",
            "epoch 80/1000   error=0.020142\n",
            "epoch 81/1000   error=0.019520\n",
            "epoch 82/1000   error=0.018920\n",
            "epoch 83/1000   error=0.022541\n",
            "epoch 84/1000   error=0.021200\n",
            "epoch 85/1000   error=0.018440\n",
            "epoch 86/1000   error=0.020453\n",
            "epoch 87/1000   error=0.018659\n",
            "epoch 88/1000   error=0.020967\n",
            "epoch 89/1000   error=0.017822\n",
            "epoch 90/1000   error=0.020413\n",
            "epoch 91/1000   error=0.019493\n",
            "epoch 92/1000   error=0.017324\n",
            "epoch 93/1000   error=0.019929\n",
            "epoch 94/1000   error=0.021385\n",
            "epoch 95/1000   error=0.019984\n",
            "epoch 96/1000   error=0.018646\n",
            "epoch 97/1000   error=0.021813\n",
            "epoch 98/1000   error=0.020863\n",
            "epoch 99/1000   error=0.017713\n",
            "epoch 100/1000   error=0.020872\n",
            "epoch 101/1000   error=0.021265\n",
            "epoch 102/1000   error=0.020185\n",
            "epoch 103/1000   error=0.019480\n",
            "epoch 104/1000   error=0.017902\n",
            "epoch 105/1000   error=0.022972\n",
            "epoch 106/1000   error=0.018433\n",
            "epoch 107/1000   error=0.020723\n",
            "epoch 108/1000   error=0.021750\n",
            "epoch 109/1000   error=0.018509\n",
            "epoch 110/1000   error=0.019973\n",
            "epoch 111/1000   error=0.020382\n",
            "epoch 112/1000   error=0.020368\n",
            "epoch 113/1000   error=0.020834\n",
            "epoch 114/1000   error=0.018879\n",
            "epoch 115/1000   error=0.018672\n",
            "epoch 116/1000   error=0.019609\n",
            "epoch 117/1000   error=0.019450\n",
            "epoch 118/1000   error=0.018093\n",
            "epoch 119/1000   error=0.019998\n",
            "epoch 120/1000   error=0.019870\n",
            "epoch 121/1000   error=0.020687\n",
            "epoch 122/1000   error=0.022269\n",
            "epoch 123/1000   error=0.019929\n",
            "epoch 124/1000   error=0.021333\n",
            "epoch 125/1000   error=0.020368\n",
            "epoch 126/1000   error=0.021005\n",
            "epoch 127/1000   error=0.021411\n",
            "epoch 128/1000   error=0.023581\n",
            "epoch 129/1000   error=0.018377\n",
            "epoch 130/1000   error=0.017895\n",
            "epoch 131/1000   error=0.020030\n",
            "epoch 132/1000   error=0.017508\n",
            "epoch 133/1000   error=0.022178\n",
            "epoch 134/1000   error=0.020315\n",
            "epoch 135/1000   error=0.022604\n",
            "epoch 136/1000   error=0.022048\n",
            "epoch 137/1000   error=0.017182\n",
            "epoch 138/1000   error=0.018222\n",
            "epoch 139/1000   error=0.018633\n",
            "epoch 140/1000   error=0.020687\n",
            "epoch 141/1000   error=0.018032\n",
            "epoch 142/1000   error=0.018967\n",
            "epoch 143/1000   error=0.017020\n",
            "epoch 144/1000   error=0.019159\n",
            "epoch 145/1000   error=0.018122\n",
            "epoch 146/1000   error=0.020785\n",
            "epoch 147/1000   error=0.020670\n",
            "epoch 148/1000   error=0.020782\n",
            "epoch 149/1000   error=0.021497\n",
            "epoch 150/1000   error=0.019850\n",
            "epoch 151/1000   error=0.021384\n",
            "epoch 152/1000   error=0.020548\n",
            "epoch 153/1000   error=0.020539\n",
            "epoch 154/1000   error=0.019917\n",
            "epoch 155/1000   error=0.020940\n",
            "epoch 156/1000   error=0.021617\n",
            "epoch 157/1000   error=0.022719\n",
            "epoch 158/1000   error=0.019948\n",
            "epoch 159/1000   error=0.021138\n",
            "epoch 160/1000   error=0.020308\n",
            "epoch 161/1000   error=0.019738\n",
            "epoch 162/1000   error=0.021515\n",
            "epoch 163/1000   error=0.019044\n",
            "epoch 164/1000   error=0.020980\n",
            "epoch 165/1000   error=0.020087\n",
            "epoch 166/1000   error=0.020368\n",
            "epoch 167/1000   error=0.018467\n",
            "epoch 168/1000   error=0.020552\n",
            "epoch 169/1000   error=0.019845\n",
            "epoch 170/1000   error=0.019451\n",
            "epoch 171/1000   error=0.019444\n",
            "epoch 172/1000   error=0.019275\n",
            "epoch 173/1000   error=0.019970\n",
            "epoch 174/1000   error=0.019619\n",
            "epoch 175/1000   error=0.018394\n",
            "epoch 176/1000   error=0.019517\n",
            "epoch 177/1000   error=0.019950\n",
            "epoch 178/1000   error=0.021283\n",
            "epoch 179/1000   error=0.021844\n",
            "epoch 180/1000   error=0.018826\n",
            "epoch 181/1000   error=0.020659\n",
            "epoch 182/1000   error=0.021358\n",
            "epoch 183/1000   error=0.019725\n",
            "epoch 184/1000   error=0.020549\n",
            "epoch 185/1000   error=0.021685\n",
            "epoch 186/1000   error=0.018311\n",
            "epoch 187/1000   error=0.024730\n",
            "epoch 188/1000   error=0.020773\n",
            "epoch 189/1000   error=0.020018\n",
            "epoch 190/1000   error=0.018990\n",
            "epoch 191/1000   error=0.021491\n",
            "epoch 192/1000   error=0.022947\n",
            "epoch 193/1000   error=0.019255\n",
            "epoch 194/1000   error=0.019165\n",
            "epoch 195/1000   error=0.020249\n",
            "epoch 196/1000   error=0.021044\n",
            "epoch 197/1000   error=0.021677\n",
            "epoch 198/1000   error=0.019788\n",
            "epoch 199/1000   error=0.018815\n",
            "epoch 200/1000   error=0.019566\n",
            "epoch 201/1000   error=0.019848\n",
            "epoch 202/1000   error=0.019858\n",
            "epoch 203/1000   error=0.020832\n",
            "epoch 204/1000   error=0.019396\n",
            "epoch 205/1000   error=0.021591\n",
            "epoch 206/1000   error=0.017968\n",
            "epoch 207/1000   error=0.020790\n",
            "epoch 208/1000   error=0.019113\n",
            "epoch 209/1000   error=0.021259\n",
            "epoch 210/1000   error=0.020503\n",
            "epoch 211/1000   error=0.018364\n",
            "epoch 212/1000   error=0.020803\n",
            "epoch 213/1000   error=0.020842\n",
            "epoch 214/1000   error=0.020270\n",
            "epoch 215/1000   error=0.020183\n",
            "epoch 216/1000   error=0.018494\n",
            "epoch 217/1000   error=0.020537\n",
            "epoch 218/1000   error=0.020376\n",
            "epoch 219/1000   error=0.021301\n",
            "epoch 220/1000   error=0.020191\n",
            "epoch 221/1000   error=0.019130\n",
            "epoch 222/1000   error=0.020239\n",
            "epoch 223/1000   error=0.020203\n",
            "epoch 224/1000   error=0.020405\n",
            "epoch 225/1000   error=0.020006\n",
            "epoch 226/1000   error=0.022789\n",
            "epoch 227/1000   error=0.019388\n",
            "epoch 228/1000   error=0.022595\n",
            "epoch 229/1000   error=0.019825\n",
            "epoch 230/1000   error=0.019957\n",
            "epoch 231/1000   error=0.020421\n",
            "epoch 232/1000   error=0.018212\n",
            "epoch 233/1000   error=0.020220\n",
            "epoch 234/1000   error=0.018568\n",
            "epoch 235/1000   error=0.020407\n",
            "epoch 236/1000   error=0.017073\n",
            "epoch 237/1000   error=0.020252\n",
            "epoch 238/1000   error=0.019961\n",
            "epoch 239/1000   error=0.021416\n",
            "epoch 240/1000   error=0.019928\n",
            "epoch 241/1000   error=0.022096\n",
            "epoch 242/1000   error=0.019666\n",
            "epoch 243/1000   error=0.017831\n",
            "epoch 244/1000   error=0.017578\n",
            "epoch 245/1000   error=0.023217\n",
            "epoch 246/1000   error=0.020976\n",
            "epoch 247/1000   error=0.016617\n",
            "epoch 248/1000   error=0.020043\n",
            "epoch 249/1000   error=0.019475\n",
            "epoch 250/1000   error=0.018696\n",
            "epoch 251/1000   error=0.019923\n",
            "epoch 252/1000   error=0.019930\n",
            "epoch 253/1000   error=0.019156\n",
            "epoch 254/1000   error=0.019950\n",
            "epoch 255/1000   error=0.020681\n",
            "epoch 256/1000   error=0.022368\n",
            "epoch 257/1000   error=0.019622\n",
            "epoch 258/1000   error=0.019564\n",
            "epoch 259/1000   error=0.019046\n",
            "epoch 260/1000   error=0.019986\n",
            "epoch 261/1000   error=0.020757\n",
            "epoch 262/1000   error=0.018831\n",
            "epoch 263/1000   error=0.019553\n",
            "epoch 264/1000   error=0.019412\n",
            "epoch 265/1000   error=0.021179\n",
            "epoch 266/1000   error=0.018974\n",
            "epoch 267/1000   error=0.020543\n",
            "epoch 268/1000   error=0.018845\n",
            "epoch 269/1000   error=0.020331\n",
            "epoch 270/1000   error=0.019849\n",
            "epoch 271/1000   error=0.019896\n",
            "epoch 272/1000   error=0.019206\n",
            "epoch 273/1000   error=0.019811\n",
            "epoch 274/1000   error=0.021772\n",
            "epoch 275/1000   error=0.018023\n",
            "epoch 276/1000   error=0.018112\n",
            "epoch 277/1000   error=0.019053\n",
            "epoch 278/1000   error=0.020936\n",
            "epoch 279/1000   error=0.019018\n",
            "epoch 280/1000   error=0.019015\n",
            "epoch 281/1000   error=0.020336\n",
            "epoch 282/1000   error=0.017391\n",
            "epoch 283/1000   error=0.021450\n",
            "epoch 284/1000   error=0.019778\n",
            "epoch 285/1000   error=0.021275\n",
            "epoch 286/1000   error=0.021123\n",
            "epoch 287/1000   error=0.019983\n",
            "epoch 288/1000   error=0.021548\n",
            "epoch 289/1000   error=0.020009\n",
            "epoch 290/1000   error=0.017826\n",
            "epoch 291/1000   error=0.017040\n",
            "epoch 292/1000   error=0.021325\n",
            "epoch 293/1000   error=0.019416\n",
            "epoch 294/1000   error=0.018571\n",
            "epoch 295/1000   error=0.018806\n",
            "epoch 296/1000   error=0.021326\n",
            "epoch 297/1000   error=0.019139\n",
            "epoch 298/1000   error=0.021521\n",
            "epoch 299/1000   error=0.020076\n",
            "epoch 300/1000   error=0.021945\n",
            "epoch 301/1000   error=0.022390\n",
            "epoch 302/1000   error=0.018705\n",
            "epoch 303/1000   error=0.019254\n",
            "epoch 304/1000   error=0.020609\n",
            "epoch 305/1000   error=0.018563\n",
            "epoch 306/1000   error=0.018657\n",
            "epoch 307/1000   error=0.020426\n",
            "epoch 308/1000   error=0.017566\n",
            "epoch 309/1000   error=0.021367\n",
            "epoch 310/1000   error=0.018505\n",
            "epoch 311/1000   error=0.016534\n",
            "epoch 312/1000   error=0.018892\n",
            "epoch 313/1000   error=0.020795\n",
            "epoch 314/1000   error=0.018339\n",
            "epoch 315/1000   error=0.020203\n",
            "epoch 316/1000   error=0.016733\n",
            "epoch 317/1000   error=0.018248\n",
            "epoch 318/1000   error=0.017521\n",
            "epoch 319/1000   error=0.020222\n",
            "epoch 320/1000   error=0.020513\n",
            "epoch 321/1000   error=0.019654\n",
            "epoch 322/1000   error=0.016777\n",
            "epoch 323/1000   error=0.017660\n",
            "epoch 324/1000   error=0.020885\n",
            "epoch 325/1000   error=0.019524\n",
            "epoch 326/1000   error=0.020356\n",
            "epoch 327/1000   error=0.018526\n",
            "epoch 328/1000   error=0.017796\n",
            "epoch 329/1000   error=0.019238\n",
            "epoch 330/1000   error=0.016408\n",
            "epoch 331/1000   error=0.020701\n",
            "epoch 332/1000   error=0.020018\n",
            "epoch 333/1000   error=0.020653\n",
            "epoch 334/1000   error=0.018659\n",
            "epoch 335/1000   error=0.022159\n",
            "epoch 336/1000   error=0.019178\n",
            "epoch 337/1000   error=0.020815\n",
            "epoch 338/1000   error=0.021235\n",
            "epoch 339/1000   error=0.020532\n",
            "epoch 340/1000   error=0.018497\n",
            "epoch 341/1000   error=0.018216\n",
            "epoch 342/1000   error=0.020262\n",
            "epoch 343/1000   error=0.021614\n",
            "epoch 344/1000   error=0.020642\n",
            "epoch 345/1000   error=0.021135\n",
            "epoch 346/1000   error=0.018355\n",
            "epoch 347/1000   error=0.019465\n",
            "epoch 348/1000   error=0.017666\n",
            "epoch 349/1000   error=0.019682\n",
            "epoch 350/1000   error=0.019852\n",
            "epoch 351/1000   error=0.018635\n",
            "epoch 352/1000   error=0.020350\n",
            "epoch 353/1000   error=0.020308\n",
            "epoch 354/1000   error=0.019163\n",
            "epoch 355/1000   error=0.019407\n",
            "epoch 356/1000   error=0.020861\n",
            "epoch 357/1000   error=0.019234\n",
            "epoch 358/1000   error=0.020039\n",
            "epoch 359/1000   error=0.019116\n",
            "epoch 360/1000   error=0.019147\n",
            "epoch 361/1000   error=0.019192\n",
            "epoch 362/1000   error=0.020436\n",
            "epoch 363/1000   error=0.020685\n",
            "epoch 364/1000   error=0.019564\n",
            "epoch 365/1000   error=0.019524\n",
            "epoch 366/1000   error=0.019108\n",
            "epoch 367/1000   error=0.020227\n",
            "epoch 368/1000   error=0.019845\n",
            "epoch 369/1000   error=0.020013\n",
            "epoch 370/1000   error=0.019488\n",
            "epoch 371/1000   error=0.018879\n",
            "epoch 372/1000   error=0.018985\n",
            "epoch 373/1000   error=0.019382\n",
            "epoch 374/1000   error=0.020906\n",
            "epoch 375/1000   error=0.018786\n",
            "epoch 376/1000   error=0.019103\n",
            "epoch 377/1000   error=0.019287\n",
            "epoch 378/1000   error=0.020815\n",
            "epoch 379/1000   error=0.020805\n",
            "epoch 380/1000   error=0.020004\n",
            "epoch 381/1000   error=0.020468\n",
            "epoch 382/1000   error=0.019635\n",
            "epoch 383/1000   error=0.020704\n",
            "epoch 384/1000   error=0.020386\n",
            "epoch 385/1000   error=0.018910\n",
            "epoch 386/1000   error=0.020940\n",
            "epoch 387/1000   error=0.020166\n",
            "epoch 388/1000   error=0.018185\n",
            "epoch 389/1000   error=0.019421\n",
            "epoch 390/1000   error=0.020063\n",
            "epoch 391/1000   error=0.019783\n",
            "epoch 392/1000   error=0.021059\n",
            "epoch 393/1000   error=0.017448\n",
            "epoch 394/1000   error=0.020101\n",
            "epoch 395/1000   error=0.018946\n",
            "epoch 396/1000   error=0.019155\n",
            "epoch 397/1000   error=0.020487\n",
            "epoch 398/1000   error=0.020474\n",
            "epoch 399/1000   error=0.019981\n",
            "epoch 400/1000   error=0.019860\n",
            "epoch 401/1000   error=0.019042\n",
            "epoch 402/1000   error=0.018719\n",
            "epoch 403/1000   error=0.018489\n",
            "epoch 404/1000   error=0.020695\n",
            "epoch 405/1000   error=0.020134\n",
            "epoch 406/1000   error=0.019288\n",
            "epoch 407/1000   error=0.018493\n",
            "epoch 408/1000   error=0.020292\n",
            "epoch 409/1000   error=0.017495\n",
            "epoch 410/1000   error=0.021369\n",
            "epoch 411/1000   error=0.021001\n",
            "epoch 412/1000   error=0.018966\n",
            "epoch 413/1000   error=0.018150\n",
            "epoch 414/1000   error=0.018668\n",
            "epoch 415/1000   error=0.020719\n",
            "epoch 416/1000   error=0.019762\n",
            "epoch 417/1000   error=0.019306\n",
            "epoch 418/1000   error=0.021153\n",
            "epoch 419/1000   error=0.019741\n",
            "epoch 420/1000   error=0.019712\n",
            "epoch 421/1000   error=0.019427\n",
            "epoch 422/1000   error=0.018922\n",
            "epoch 423/1000   error=0.020576\n",
            "epoch 424/1000   error=0.019307\n",
            "epoch 425/1000   error=0.019560\n",
            "epoch 426/1000   error=0.018279\n",
            "epoch 427/1000   error=0.020421\n",
            "epoch 428/1000   error=0.017811\n",
            "epoch 429/1000   error=0.019603\n",
            "epoch 430/1000   error=0.017767\n",
            "epoch 431/1000   error=0.019465\n",
            "epoch 432/1000   error=0.017237\n",
            "epoch 433/1000   error=0.020328\n",
            "epoch 434/1000   error=0.021193\n",
            "epoch 435/1000   error=0.019892\n",
            "epoch 436/1000   error=0.018106\n",
            "epoch 437/1000   error=0.020126\n",
            "epoch 438/1000   error=0.019056\n",
            "epoch 439/1000   error=0.019160\n",
            "epoch 440/1000   error=0.020421\n",
            "epoch 441/1000   error=0.019353\n",
            "epoch 442/1000   error=0.020982\n",
            "epoch 443/1000   error=0.018047\n",
            "epoch 444/1000   error=0.018728\n",
            "epoch 445/1000   error=0.018680\n",
            "epoch 446/1000   error=0.019545\n",
            "epoch 447/1000   error=0.018311\n",
            "epoch 448/1000   error=0.020556\n",
            "epoch 449/1000   error=0.016151\n",
            "epoch 450/1000   error=0.019508\n",
            "epoch 451/1000   error=0.017922\n",
            "epoch 452/1000   error=0.020220\n",
            "epoch 453/1000   error=0.019318\n",
            "epoch 454/1000   error=0.019016\n",
            "epoch 455/1000   error=0.022226\n",
            "epoch 456/1000   error=0.017463\n",
            "epoch 457/1000   error=0.018901\n",
            "epoch 458/1000   error=0.018603\n",
            "epoch 459/1000   error=0.021071\n",
            "epoch 460/1000   error=0.018076\n",
            "epoch 461/1000   error=0.018446\n",
            "epoch 462/1000   error=0.019809\n",
            "epoch 463/1000   error=0.018925\n",
            "epoch 464/1000   error=0.019603\n",
            "epoch 465/1000   error=0.015959\n",
            "epoch 466/1000   error=0.018710\n",
            "epoch 467/1000   error=0.018841\n",
            "epoch 468/1000   error=0.017963\n",
            "epoch 469/1000   error=0.019373\n",
            "epoch 470/1000   error=0.020124\n",
            "epoch 471/1000   error=0.015913\n",
            "epoch 472/1000   error=0.018201\n",
            "epoch 473/1000   error=0.018252\n",
            "epoch 474/1000   error=0.019068\n",
            "epoch 475/1000   error=0.020351\n",
            "epoch 476/1000   error=0.020289\n",
            "epoch 477/1000   error=0.020251\n",
            "epoch 478/1000   error=0.017433\n",
            "epoch 479/1000   error=0.021326\n",
            "epoch 480/1000   error=0.016808\n",
            "epoch 481/1000   error=0.019035\n",
            "epoch 482/1000   error=0.021901\n",
            "epoch 483/1000   error=0.020032\n",
            "epoch 484/1000   error=0.019364\n",
            "epoch 485/1000   error=0.019911\n",
            "epoch 486/1000   error=0.018890\n",
            "epoch 487/1000   error=0.017930\n",
            "epoch 488/1000   error=0.017999\n",
            "epoch 489/1000   error=0.019358\n",
            "epoch 490/1000   error=0.019015\n",
            "epoch 491/1000   error=0.019264\n",
            "epoch 492/1000   error=0.018673\n",
            "epoch 493/1000   error=0.018453\n",
            "epoch 494/1000   error=0.018325\n",
            "epoch 495/1000   error=0.018095\n",
            "epoch 496/1000   error=0.019267\n",
            "epoch 497/1000   error=0.017083\n",
            "epoch 498/1000   error=0.019478\n",
            "epoch 499/1000   error=0.020005\n",
            "epoch 500/1000   error=0.019239\n",
            "epoch 501/1000   error=0.019090\n",
            "epoch 502/1000   error=0.019836\n",
            "epoch 503/1000   error=0.019174\n",
            "epoch 504/1000   error=0.019533\n",
            "epoch 505/1000   error=0.017849\n",
            "epoch 506/1000   error=0.019980\n",
            "epoch 507/1000   error=0.018136\n",
            "epoch 508/1000   error=0.017055\n",
            "epoch 509/1000   error=0.019996\n",
            "epoch 510/1000   error=0.019980\n",
            "epoch 511/1000   error=0.017139\n",
            "epoch 512/1000   error=0.020201\n",
            "epoch 513/1000   error=0.017024\n",
            "epoch 514/1000   error=0.017993\n",
            "epoch 515/1000   error=0.019214\n",
            "epoch 516/1000   error=0.019193\n",
            "epoch 517/1000   error=0.021830\n",
            "epoch 518/1000   error=0.018524\n",
            "epoch 519/1000   error=0.017796\n",
            "epoch 520/1000   error=0.018508\n",
            "epoch 521/1000   error=0.015878\n",
            "epoch 522/1000   error=0.020654\n",
            "epoch 523/1000   error=0.017787\n",
            "epoch 524/1000   error=0.018500\n",
            "epoch 525/1000   error=0.017084\n",
            "epoch 526/1000   error=0.020375\n",
            "epoch 527/1000   error=0.018806\n",
            "epoch 528/1000   error=0.019508\n",
            "epoch 529/1000   error=0.019419\n",
            "epoch 530/1000   error=0.019594\n",
            "epoch 531/1000   error=0.017698\n",
            "epoch 532/1000   error=0.019907\n",
            "epoch 533/1000   error=0.018322\n",
            "epoch 534/1000   error=0.018867\n",
            "epoch 535/1000   error=0.019844\n",
            "epoch 536/1000   error=0.019365\n",
            "epoch 537/1000   error=0.019664\n",
            "epoch 538/1000   error=0.018414\n",
            "epoch 539/1000   error=0.021015\n",
            "epoch 540/1000   error=0.019412\n",
            "epoch 541/1000   error=0.018525\n",
            "epoch 542/1000   error=0.018441\n",
            "epoch 543/1000   error=0.021220\n",
            "epoch 544/1000   error=0.019964\n",
            "epoch 545/1000   error=0.018804\n",
            "epoch 546/1000   error=0.019254\n",
            "epoch 547/1000   error=0.020047\n",
            "epoch 548/1000   error=0.019498\n",
            "epoch 549/1000   error=0.016216\n",
            "epoch 550/1000   error=0.018275\n",
            "epoch 551/1000   error=0.020260\n",
            "epoch 552/1000   error=0.017651\n",
            "epoch 553/1000   error=0.019226\n",
            "epoch 554/1000   error=0.017845\n",
            "epoch 555/1000   error=0.017573\n",
            "epoch 556/1000   error=0.020586\n",
            "epoch 557/1000   error=0.018859\n",
            "epoch 558/1000   error=0.018813\n",
            "epoch 559/1000   error=0.019353\n",
            "epoch 560/1000   error=0.019683\n",
            "epoch 561/1000   error=0.019088\n",
            "epoch 562/1000   error=0.018798\n",
            "epoch 563/1000   error=0.019997\n",
            "epoch 564/1000   error=0.019189\n",
            "epoch 565/1000   error=0.017757\n",
            "epoch 566/1000   error=0.018492\n",
            "epoch 567/1000   error=0.018942\n",
            "epoch 568/1000   error=0.019519\n",
            "epoch 569/1000   error=0.017916\n",
            "epoch 570/1000   error=0.019301\n",
            "epoch 571/1000   error=0.018335\n",
            "epoch 572/1000   error=0.019473\n",
            "epoch 573/1000   error=0.019782\n",
            "epoch 574/1000   error=0.021067\n",
            "epoch 575/1000   error=0.016563\n",
            "epoch 576/1000   error=0.018514\n",
            "epoch 577/1000   error=0.020290\n",
            "epoch 578/1000   error=0.019416\n",
            "epoch 579/1000   error=0.017620\n",
            "epoch 580/1000   error=0.020738\n",
            "epoch 581/1000   error=0.018374\n",
            "epoch 582/1000   error=0.017497\n",
            "epoch 583/1000   error=0.020979\n",
            "epoch 584/1000   error=0.022345\n",
            "epoch 585/1000   error=0.016858\n",
            "epoch 586/1000   error=0.019221\n",
            "epoch 587/1000   error=0.020044\n",
            "epoch 588/1000   error=0.018622\n",
            "epoch 589/1000   error=0.017815\n",
            "epoch 590/1000   error=0.019318\n",
            "epoch 591/1000   error=0.019578\n",
            "epoch 592/1000   error=0.019112\n",
            "epoch 593/1000   error=0.018810\n",
            "epoch 594/1000   error=0.021019\n",
            "epoch 595/1000   error=0.017702\n",
            "epoch 596/1000   error=0.015949\n",
            "epoch 597/1000   error=0.021508\n",
            "epoch 598/1000   error=0.019015\n",
            "epoch 599/1000   error=0.017639\n",
            "epoch 600/1000   error=0.018673\n",
            "epoch 601/1000   error=0.018339\n",
            "epoch 602/1000   error=0.021017\n",
            "epoch 603/1000   error=0.016730\n",
            "epoch 604/1000   error=0.017688\n",
            "epoch 605/1000   error=0.020537\n",
            "epoch 606/1000   error=0.019381\n",
            "epoch 607/1000   error=0.017035\n",
            "epoch 608/1000   error=0.019777\n",
            "epoch 609/1000   error=0.020719\n",
            "epoch 610/1000   error=0.018962\n",
            "epoch 611/1000   error=0.018078\n",
            "epoch 612/1000   error=0.018740\n",
            "epoch 613/1000   error=0.019376\n",
            "epoch 614/1000   error=0.018322\n",
            "epoch 615/1000   error=0.019283\n",
            "epoch 616/1000   error=0.018884\n",
            "epoch 617/1000   error=0.017162\n",
            "epoch 618/1000   error=0.016824\n",
            "epoch 619/1000   error=0.018422\n",
            "epoch 620/1000   error=0.017746\n",
            "epoch 621/1000   error=0.019014\n",
            "epoch 622/1000   error=0.018549\n",
            "epoch 623/1000   error=0.019245\n",
            "epoch 624/1000   error=0.017802\n",
            "epoch 625/1000   error=0.019349\n",
            "epoch 626/1000   error=0.018418\n",
            "epoch 627/1000   error=0.017155\n",
            "epoch 628/1000   error=0.019218\n",
            "epoch 629/1000   error=0.019012\n",
            "epoch 630/1000   error=0.015569\n",
            "epoch 631/1000   error=0.019220\n",
            "epoch 632/1000   error=0.021372\n",
            "epoch 633/1000   error=0.016625\n",
            "epoch 634/1000   error=0.020685\n",
            "epoch 635/1000   error=0.016858\n",
            "epoch 636/1000   error=0.021173\n",
            "epoch 637/1000   error=0.018243\n",
            "epoch 638/1000   error=0.015522\n",
            "epoch 639/1000   error=0.021176\n",
            "epoch 640/1000   error=0.019137\n",
            "epoch 641/1000   error=0.021911\n",
            "epoch 642/1000   error=0.022794\n",
            "epoch 643/1000   error=0.020984\n",
            "epoch 644/1000   error=0.018432\n",
            "epoch 645/1000   error=0.018005\n",
            "epoch 646/1000   error=0.015201\n",
            "epoch 647/1000   error=0.015755\n",
            "epoch 648/1000   error=0.017173\n",
            "epoch 649/1000   error=0.018521\n",
            "epoch 650/1000   error=0.019210\n",
            "epoch 651/1000   error=0.019159\n",
            "epoch 652/1000   error=0.018389\n",
            "epoch 653/1000   error=0.020069\n",
            "epoch 654/1000   error=0.019172\n",
            "epoch 655/1000   error=0.017369\n",
            "epoch 656/1000   error=0.017706\n",
            "epoch 657/1000   error=0.016799\n",
            "epoch 658/1000   error=0.017530\n",
            "epoch 659/1000   error=0.020126\n",
            "epoch 660/1000   error=0.018063\n",
            "epoch 661/1000   error=0.018997\n",
            "epoch 662/1000   error=0.021009\n",
            "epoch 663/1000   error=0.020531\n",
            "epoch 664/1000   error=0.020765\n",
            "epoch 665/1000   error=0.016528\n",
            "epoch 666/1000   error=0.016785\n",
            "epoch 667/1000   error=0.021120\n",
            "epoch 668/1000   error=0.020124\n",
            "epoch 669/1000   error=0.019810\n",
            "epoch 670/1000   error=0.019050\n",
            "epoch 671/1000   error=0.017906\n",
            "epoch 672/1000   error=0.018246\n",
            "epoch 673/1000   error=0.019485\n",
            "epoch 674/1000   error=0.018142\n",
            "epoch 675/1000   error=0.017751\n",
            "epoch 676/1000   error=0.018136\n",
            "epoch 677/1000   error=0.020289\n",
            "epoch 678/1000   error=0.021000\n",
            "epoch 679/1000   error=0.019507\n",
            "epoch 680/1000   error=0.019373\n",
            "epoch 681/1000   error=0.018245\n",
            "epoch 682/1000   error=0.018078\n",
            "epoch 683/1000   error=0.019045\n",
            "epoch 684/1000   error=0.018527\n",
            "epoch 685/1000   error=0.016624\n",
            "epoch 686/1000   error=0.021398\n",
            "epoch 687/1000   error=0.019377\n",
            "epoch 688/1000   error=0.018631\n",
            "epoch 689/1000   error=0.017476\n",
            "epoch 690/1000   error=0.017376\n",
            "epoch 691/1000   error=0.020340\n",
            "epoch 692/1000   error=0.017647\n",
            "epoch 693/1000   error=0.017230\n",
            "epoch 694/1000   error=0.017281\n",
            "epoch 695/1000   error=0.020102\n",
            "epoch 696/1000   error=0.018042\n",
            "epoch 697/1000   error=0.018398\n",
            "epoch 698/1000   error=0.018391\n",
            "epoch 699/1000   error=0.019239\n",
            "epoch 700/1000   error=0.019253\n",
            "epoch 701/1000   error=0.017683\n",
            "epoch 702/1000   error=0.018144\n",
            "epoch 703/1000   error=0.018653\n",
            "epoch 704/1000   error=0.019678\n",
            "epoch 705/1000   error=0.016852\n",
            "epoch 706/1000   error=0.015152\n",
            "epoch 707/1000   error=0.017524\n",
            "epoch 708/1000   error=0.020303\n",
            "epoch 709/1000   error=0.018441\n",
            "epoch 710/1000   error=0.019894\n",
            "epoch 711/1000   error=0.017136\n",
            "epoch 712/1000   error=0.017070\n",
            "epoch 713/1000   error=0.018798\n",
            "epoch 714/1000   error=0.018075\n",
            "epoch 715/1000   error=0.016437\n",
            "epoch 716/1000   error=0.019350\n",
            "epoch 717/1000   error=0.017024\n",
            "epoch 718/1000   error=0.019901\n",
            "epoch 719/1000   error=0.016517\n",
            "epoch 720/1000   error=0.017692\n",
            "epoch 721/1000   error=0.015074\n",
            "epoch 722/1000   error=0.017695\n",
            "epoch 723/1000   error=0.017449\n",
            "epoch 724/1000   error=0.019085\n",
            "epoch 725/1000   error=0.017775\n",
            "epoch 726/1000   error=0.017444\n",
            "epoch 727/1000   error=0.017848\n",
            "epoch 728/1000   error=0.020027\n",
            "epoch 729/1000   error=0.019712\n",
            "epoch 730/1000   error=0.017544\n",
            "epoch 731/1000   error=0.018329\n",
            "epoch 732/1000   error=0.017370\n",
            "epoch 733/1000   error=0.018191\n",
            "epoch 734/1000   error=0.021425\n",
            "epoch 735/1000   error=0.017273\n",
            "epoch 736/1000   error=0.019002\n",
            "epoch 737/1000   error=0.016907\n",
            "epoch 738/1000   error=0.017315\n",
            "epoch 739/1000   error=0.020405\n",
            "epoch 740/1000   error=0.016881\n",
            "epoch 741/1000   error=0.017493\n",
            "epoch 742/1000   error=0.016651\n",
            "epoch 743/1000   error=0.018192\n",
            "epoch 744/1000   error=0.019173\n",
            "epoch 745/1000   error=0.017184\n",
            "epoch 746/1000   error=0.018117\n",
            "epoch 747/1000   error=0.019686\n",
            "epoch 748/1000   error=0.016888\n",
            "epoch 749/1000   error=0.018243\n",
            "epoch 750/1000   error=0.019144\n",
            "epoch 751/1000   error=0.018147\n",
            "epoch 752/1000   error=0.016944\n",
            "epoch 753/1000   error=0.017376\n",
            "epoch 754/1000   error=0.021412\n",
            "epoch 755/1000   error=0.018978\n",
            "epoch 756/1000   error=0.017569\n",
            "epoch 757/1000   error=0.016869\n",
            "epoch 758/1000   error=0.017597\n",
            "epoch 759/1000   error=0.018567\n",
            "epoch 760/1000   error=0.017969\n",
            "epoch 761/1000   error=0.016928\n",
            "epoch 762/1000   error=0.014342\n",
            "epoch 763/1000   error=0.018085\n",
            "epoch 764/1000   error=0.017346\n",
            "epoch 765/1000   error=0.017758\n",
            "epoch 766/1000   error=0.017283\n",
            "epoch 767/1000   error=0.020048\n",
            "epoch 768/1000   error=0.020240\n",
            "epoch 769/1000   error=0.018484\n",
            "epoch 770/1000   error=0.017359\n",
            "epoch 771/1000   error=0.019452\n",
            "epoch 772/1000   error=0.018610\n",
            "epoch 773/1000   error=0.017986\n",
            "epoch 774/1000   error=0.017350\n",
            "epoch 775/1000   error=0.019808\n",
            "epoch 776/1000   error=0.017630\n",
            "epoch 777/1000   error=0.018566\n",
            "epoch 778/1000   error=0.016038\n",
            "epoch 779/1000   error=0.018010\n",
            "epoch 780/1000   error=0.017436\n",
            "epoch 781/1000   error=0.017647\n",
            "epoch 782/1000   error=0.018202\n",
            "epoch 783/1000   error=0.018922\n",
            "epoch 784/1000   error=0.018399\n",
            "epoch 785/1000   error=0.019655\n",
            "epoch 786/1000   error=0.017290\n",
            "epoch 787/1000   error=0.017002\n",
            "epoch 788/1000   error=0.018658\n",
            "epoch 789/1000   error=0.017990\n",
            "epoch 790/1000   error=0.018527\n",
            "epoch 791/1000   error=0.017879\n",
            "epoch 792/1000   error=0.018543\n",
            "epoch 793/1000   error=0.020019\n",
            "epoch 794/1000   error=0.017333\n",
            "epoch 795/1000   error=0.018093\n",
            "epoch 796/1000   error=0.018802\n",
            "epoch 797/1000   error=0.018836\n",
            "epoch 798/1000   error=0.018057\n",
            "epoch 799/1000   error=0.018064\n",
            "epoch 800/1000   error=0.017078\n",
            "epoch 801/1000   error=0.020845\n",
            "epoch 802/1000   error=0.018518\n",
            "epoch 803/1000   error=0.018585\n",
            "epoch 804/1000   error=0.019268\n",
            "epoch 805/1000   error=0.019412\n",
            "epoch 806/1000   error=0.018974\n",
            "epoch 807/1000   error=0.020374\n",
            "epoch 808/1000   error=0.018845\n",
            "epoch 809/1000   error=0.020411\n",
            "epoch 810/1000   error=0.020541\n",
            "epoch 811/1000   error=0.017723\n",
            "epoch 812/1000   error=0.017478\n",
            "epoch 813/1000   error=0.017633\n",
            "epoch 814/1000   error=0.018083\n",
            "epoch 815/1000   error=0.017956\n",
            "epoch 816/1000   error=0.019004\n",
            "epoch 817/1000   error=0.018965\n",
            "epoch 818/1000   error=0.019987\n",
            "epoch 819/1000   error=0.020078\n",
            "epoch 820/1000   error=0.017752\n",
            "epoch 821/1000   error=0.017081\n",
            "epoch 822/1000   error=0.017643\n",
            "epoch 823/1000   error=0.018634\n",
            "epoch 824/1000   error=0.018344\n",
            "epoch 825/1000   error=0.020023\n",
            "epoch 826/1000   error=0.018988\n",
            "epoch 827/1000   error=0.016702\n",
            "epoch 828/1000   error=0.017786\n",
            "epoch 829/1000   error=0.018210\n",
            "epoch 830/1000   error=0.019092\n",
            "epoch 831/1000   error=0.018670\n",
            "epoch 832/1000   error=0.017162\n",
            "epoch 833/1000   error=0.019670\n",
            "epoch 834/1000   error=0.019692\n",
            "epoch 835/1000   error=0.018459\n",
            "epoch 836/1000   error=0.020650\n",
            "epoch 837/1000   error=0.016730\n",
            "epoch 838/1000   error=0.019972\n",
            "epoch 839/1000   error=0.018351\n",
            "epoch 840/1000   error=0.018007\n",
            "epoch 841/1000   error=0.017454\n",
            "epoch 842/1000   error=0.017744\n",
            "epoch 843/1000   error=0.018191\n",
            "epoch 844/1000   error=0.018613\n",
            "epoch 845/1000   error=0.017641\n",
            "epoch 846/1000   error=0.017429\n",
            "epoch 847/1000   error=0.019750\n",
            "epoch 848/1000   error=0.019043\n",
            "epoch 849/1000   error=0.019102\n",
            "epoch 850/1000   error=0.017673\n",
            "epoch 851/1000   error=0.018108\n",
            "epoch 852/1000   error=0.016363\n",
            "epoch 853/1000   error=0.018312\n",
            "epoch 854/1000   error=0.020328\n",
            "epoch 855/1000   error=0.019731\n",
            "epoch 856/1000   error=0.018722\n",
            "epoch 857/1000   error=0.018458\n",
            "epoch 858/1000   error=0.018097\n",
            "epoch 859/1000   error=0.015597\n",
            "epoch 860/1000   error=0.017535\n",
            "epoch 861/1000   error=0.018121\n",
            "epoch 862/1000   error=0.019605\n",
            "epoch 863/1000   error=0.018490\n",
            "epoch 864/1000   error=0.016363\n",
            "epoch 865/1000   error=0.016285\n",
            "epoch 866/1000   error=0.016123\n",
            "epoch 867/1000   error=0.019026\n",
            "epoch 868/1000   error=0.016532\n",
            "epoch 869/1000   error=0.019517\n",
            "epoch 870/1000   error=0.017184\n",
            "epoch 871/1000   error=0.016788\n",
            "epoch 872/1000   error=0.018858\n",
            "epoch 873/1000   error=0.019145\n",
            "epoch 874/1000   error=0.016781\n",
            "epoch 875/1000   error=0.018965\n",
            "epoch 876/1000   error=0.017694\n",
            "epoch 877/1000   error=0.018098\n",
            "epoch 878/1000   error=0.018657\n",
            "epoch 879/1000   error=0.017203\n",
            "epoch 880/1000   error=0.017279\n",
            "epoch 881/1000   error=0.017907\n",
            "epoch 882/1000   error=0.017950\n",
            "epoch 883/1000   error=0.018403\n",
            "epoch 884/1000   error=0.020209\n",
            "epoch 885/1000   error=0.020387\n",
            "epoch 886/1000   error=0.017596\n",
            "epoch 887/1000   error=0.019702\n",
            "epoch 888/1000   error=0.017657\n",
            "epoch 889/1000   error=0.017253\n",
            "epoch 890/1000   error=0.017107\n",
            "epoch 891/1000   error=0.017672\n",
            "epoch 892/1000   error=0.018500\n",
            "epoch 893/1000   error=0.017956\n",
            "epoch 894/1000   error=0.018473\n",
            "epoch 895/1000   error=0.018397\n",
            "epoch 896/1000   error=0.015985\n",
            "epoch 897/1000   error=0.015259\n",
            "epoch 898/1000   error=0.016463\n",
            "epoch 899/1000   error=0.017618\n",
            "epoch 900/1000   error=0.018197\n",
            "epoch 901/1000   error=0.017787\n",
            "epoch 902/1000   error=0.019974\n",
            "epoch 903/1000   error=0.020244\n",
            "epoch 904/1000   error=0.015747\n",
            "epoch 905/1000   error=0.017487\n",
            "epoch 906/1000   error=0.017870\n",
            "epoch 907/1000   error=0.017753\n",
            "epoch 908/1000   error=0.017133\n",
            "epoch 909/1000   error=0.020396\n",
            "epoch 910/1000   error=0.019012\n",
            "epoch 911/1000   error=0.019503\n",
            "epoch 912/1000   error=0.017142\n",
            "epoch 913/1000   error=0.016849\n",
            "epoch 914/1000   error=0.021139\n",
            "epoch 915/1000   error=0.016724\n",
            "epoch 916/1000   error=0.017603\n",
            "epoch 917/1000   error=0.017879\n",
            "epoch 918/1000   error=0.020075\n",
            "epoch 919/1000   error=0.017738\n",
            "epoch 920/1000   error=0.019419\n",
            "epoch 921/1000   error=0.017407\n",
            "epoch 922/1000   error=0.016935\n",
            "epoch 923/1000   error=0.017479\n",
            "epoch 924/1000   error=0.019085\n",
            "epoch 925/1000   error=0.019256\n",
            "epoch 926/1000   error=0.019369\n",
            "epoch 927/1000   error=0.016574\n",
            "epoch 928/1000   error=0.017167\n",
            "epoch 929/1000   error=0.017321\n",
            "epoch 930/1000   error=0.016875\n",
            "epoch 931/1000   error=0.017925\n",
            "epoch 932/1000   error=0.019685\n",
            "epoch 933/1000   error=0.016247\n",
            "epoch 934/1000   error=0.018309\n",
            "epoch 935/1000   error=0.017129\n",
            "epoch 936/1000   error=0.019444\n",
            "epoch 937/1000   error=0.018129\n",
            "epoch 938/1000   error=0.017047\n",
            "epoch 939/1000   error=0.018947\n",
            "epoch 940/1000   error=0.017710\n",
            "epoch 941/1000   error=0.019565\n",
            "epoch 942/1000   error=0.013766\n",
            "epoch 943/1000   error=0.020529\n",
            "epoch 944/1000   error=0.018132\n",
            "epoch 945/1000   error=0.020037\n",
            "epoch 946/1000   error=0.017085\n",
            "epoch 947/1000   error=0.017088\n",
            "epoch 948/1000   error=0.018740\n",
            "epoch 949/1000   error=0.018773\n",
            "epoch 950/1000   error=0.016958\n",
            "epoch 951/1000   error=0.015033\n",
            "epoch 952/1000   error=0.017508\n",
            "epoch 953/1000   error=0.016060\n",
            "epoch 954/1000   error=0.017951\n",
            "epoch 955/1000   error=0.017037\n",
            "epoch 956/1000   error=0.019447\n",
            "epoch 957/1000   error=0.018777\n",
            "epoch 958/1000   error=0.018753\n",
            "epoch 959/1000   error=0.018444\n",
            "epoch 960/1000   error=0.018678\n",
            "epoch 961/1000   error=0.020209\n",
            "epoch 962/1000   error=0.017203\n",
            "epoch 963/1000   error=0.016836\n",
            "epoch 964/1000   error=0.014703\n",
            "epoch 965/1000   error=0.016339\n",
            "epoch 966/1000   error=0.016047\n",
            "epoch 967/1000   error=0.018382\n",
            "epoch 968/1000   error=0.013445\n",
            "epoch 969/1000   error=0.018071\n",
            "epoch 970/1000   error=0.015301\n",
            "epoch 971/1000   error=0.016447\n",
            "epoch 972/1000   error=0.022653\n",
            "epoch 973/1000   error=0.017323\n",
            "epoch 974/1000   error=0.017480\n",
            "epoch 975/1000   error=0.018826\n",
            "epoch 976/1000   error=0.018879\n",
            "epoch 977/1000   error=0.018658\n",
            "epoch 978/1000   error=0.016036\n",
            "epoch 979/1000   error=0.018872\n",
            "epoch 980/1000   error=0.016827\n",
            "epoch 981/1000   error=0.018830\n",
            "epoch 982/1000   error=0.014642\n",
            "epoch 983/1000   error=0.016388\n",
            "epoch 984/1000   error=0.015690\n",
            "epoch 985/1000   error=0.015186\n",
            "epoch 986/1000   error=0.016193\n",
            "epoch 987/1000   error=0.011528\n",
            "epoch 988/1000   error=0.019272\n",
            "epoch 989/1000   error=0.017217\n",
            "epoch 990/1000   error=0.015753\n",
            "epoch 991/1000   error=0.016659\n",
            "epoch 992/1000   error=0.016548\n",
            "epoch 993/1000   error=0.016750\n",
            "epoch 994/1000   error=0.017160\n",
            "epoch 995/1000   error=0.019914\n",
            "epoch 996/1000   error=0.018571\n",
            "epoch 997/1000   error=0.020689\n",
            "epoch 998/1000   error=0.018630\n",
            "epoch 999/1000   error=0.016106\n",
            "epoch 1000/1000   error=0.017031\n",
            "last error is  0.01703052642757324\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[array([[0.43478394]]), array([[0.41662897]]), array([[0.30603433]]), array([[0.41069769]]), array([[0.32272692]]), array([[0.40625484]]), array([[0.33894147]]), array([[0.32617565]]), array([[0.32361637]]), array([[0.36792403]]), array([[0.37593275]]), array([[0.39061718]]), array([[0.34696482]]), array([[0.32626362]]), array([[0.34877561]]), array([[0.36144434]]), array([[0.37865127]]), array([[0.45297276]]), array([[0.32353206]]), array([[0.31597378]]), array([[0.27800758]]), array([[0.34956276]]), array([[0.46764882]]), array([[0.43222544]]), array([[0.38896183]]), array([[0.34429968]]), array([[0.39373273]]), array([[0.37462462]]), array([[0.37663708]]), array([[0.41115875]]), array([[0.33268709]]), array([[0.38424814]]), array([[0.42332887]]), array([[0.3752894]]), array([[0.3756794]]), array([[0.33571387]]), array([[0.30953868]]), array([[0.32660007]]), array([[0.32742138]]), array([[0.33144734]]), array([[0.30242394]]), array([[0.41104628]]), array([[0.33905341]]), array([[0.32689774]]), array([[0.40363118]]), array([[0.27014035]]), array([[0.32748395]]), array([[0.3086006]]), array([[0.32484509]]), array([[0.30311906]]), array([[0.32058673]]), array([[0.30397968]]), array([[0.38531906]]), array([[0.35587929]]), array([[0.30093416]]), array([[0.43754976]]), array([[0.34978848]]), array([[0.31521601]]), array([[0.27254464]]), array([[0.28384974]]), array([[0.2743706]]), array([[0.3466621]]), array([[0.27724508]]), array([[0.3363823]]), array([[0.34666776]]), array([[0.28212618]]), array([[0.29751179]]), array([[0.27906412]]), array([[0.3087217]]), array([[0.42276125]]), array([[0.27402031]]), array([[0.4069356]]), array([[0.33267264]]), array([[0.31221811]]), array([[0.37070963]]), array([[0.31336829]]), array([[0.40043543]]), array([[0.41421575]]), array([[0.31509637]]), array([[0.30368666]]), array([[0.31528811]]), array([[0.4807121]]), array([[0.39086489]]), array([[0.30788786]]), array([[0.40606825]]), array([[0.33595886]]), array([[0.40913845]]), array([[0.310672]]), array([[0.33376641]]), array([[0.33651697]]), array([[0.341142]]), array([[0.33183971]]), array([[0.32326054]]), array([[0.35362185]]), array([[0.42233526]]), array([[0.30168078]]), array([[0.28501392]]), array([[0.30078869]]), array([[0.33772861]]), array([[0.34130935]]), array([[0.26182419]]), array([[0.30846188]]), array([[0.28541402]]), array([[0.28947142]]), array([[0.33108316]]), array([[0.3035564]]), array([[0.30796993]]), array([[0.45786873]]), array([[0.30285552]]), array([[0.28400312]]), array([[0.30681624]]), array([[0.32742335]]), array([[0.28726941]]), array([[0.27470731]]), array([[0.30918341]]), array([[0.27355433]]), array([[0.35816325]]), array([[0.37474414]]), array([[0.38141694]]), array([[0.30011131]]), array([[0.40189611]]), array([[0.44987391]]), array([[0.32878535]]), array([[0.31773849]]), array([[0.32668378]]), array([[0.33773696]]), array([[0.40419418]]), array([[0.33191251]]), array([[0.41179099]]), array([[0.30569819]]), array([[0.36476696]]), array([[0.36987479]]), array([[0.34793588]]), array([[0.40638647]]), array([[0.31901142]]), array([[0.30463512]]), array([[0.29717406]]), array([[0.34559733]]), array([[0.29264939]]), array([[0.28081018]]), array([[0.37144897]]), array([[0.30084792]]), array([[0.31672463]]), array([[0.29295576]]), array([[0.30135139]]), array([[0.30939711]]), array([[0.33709702]]), array([[0.33151893]]), array([[0.32620321]]), array([[0.31468468]]), array([[0.27226415]]), array([[0.28394269]]), array([[0.29342547]]), array([[0.31996738]]), array([[0.30856285]]), array([[0.38181994]]), array([[0.36163685]]), array([[0.3042668]]), array([[0.29518629]]), array([[0.30468214]]), array([[0.39957512]]), array([[0.44403074]]), array([[0.30856971]]), array([[0.46778181]]), array([[0.33590209]]), array([[0.28945814]]), array([[0.37675834]]), array([[0.40529425]]), array([[0.33578457]]), array([[0.30573414]]), array([[0.34583282]]), array([[0.35895826]]), array([[0.28940815]]), array([[0.28950963]]), array([[0.27168798]]), array([[0.28676057]]), array([[0.35714406]]), array([[0.3158697]]), array([[0.3092973]]), array([[0.50837328]]), array([[0.44341745]]), array([[0.37454758]]), array([[0.29660783]]), array([[0.35166514]]), array([[0.29049098]]), array([[0.39461032]]), array([[0.30270217]]), array([[0.30127068]]), array([[0.30634408]]), array([[0.3326485]]), array([[0.31041025]]), array([[0.27825838]]), array([[0.32743975]]), array([[0.33626014]]), array([[0.31342312]]), array([[0.33370272]]), array([[0.37977632]]), array([[0.41637606]]), array([[0.3540849]]), array([[0.31461096]]), array([[0.37943233]]), array([[0.45075864]]), array([[0.36060942]]), array([[0.31798751]]), array([[0.3513638]]), array([[0.28200859]]), array([[0.37398938]]), array([[0.31894874]]), array([[0.3468359]]), array([[0.41651947]]), array([[0.30572447]]), array([[0.4761515]]), array([[0.3637555]]), array([[0.33606764]]), array([[0.32864015]]), array([[0.30488174]]), array([[0.28883595]]), array([[0.43302777]]), array([[0.45718804]]), array([[0.32306924]]), array([[0.32135498]]), array([[0.28633157]]), array([[0.36191465]]), array([[0.32316226]]), array([[0.3377683]]), array([[0.28823905]]), array([[0.33672332]]), array([[0.31620527]]), array([[0.32278003]]), array([[0.37392475]]), array([[0.29895433]]), array([[0.30130384]]), array([[0.43291341]]), array([[0.28082866]]), array([[0.32757571]]), array([[0.4841384]]), array([[0.42328932]]), array([[0.33390939]]), array([[0.39290331]]), array([[0.32157708]]), array([[0.30693603]]), array([[0.29875366]]), array([[0.32517876]]), array([[0.39930686]]), array([[0.29033771]]), array([[0.31555583]]), array([[0.31599576]]), array([[0.29667773]]), array([[0.29944129]]), array([[0.43725649]]), array([[0.30097555]]), array([[0.4291613]]), array([[0.37711483]]), array([[0.43049032]]), array([[0.33417269]]), array([[0.43068967]]), array([[0.34616804]]), array([[0.36861837]]), array([[0.36168037]]), array([[0.43126241]]), array([[0.3776885]]), array([[0.38118276]]), array([[0.35490977]]), array([[0.39173506]]), array([[0.48538627]]), array([[0.29163475]]), array([[0.3213987]]), array([[0.31307979]]), array([[0.29109395]]), array([[0.32507665]]), array([[0.29521255]]), array([[0.45841097]]), array([[0.28295198]]), array([[0.38774859]]), array([[0.29778312]]), array([[0.29583546]]), array([[0.38478742]]), array([[0.32708036]]), array([[0.32187758]]), array([[0.41793894]]), array([[0.30237686]]), array([[0.4116989]]), array([[0.35926637]]), array([[0.31245749]]), array([[0.30923251]]), array([[0.30678378]]), array([[0.31043663]]), array([[0.29445672]]), array([[0.29730281]]), array([[0.33032188]]), array([[0.33782582]]), array([[0.311743]]), array([[0.30375637]]), array([[0.307987]]), array([[0.32009097]]), array([[0.28853845]]), array([[0.30471885]]), array([[0.33600916]]), array([[0.28665552]]), array([[0.43394327]]), array([[0.30754333]]), array([[0.41681423]]), array([[0.28776156]]), array([[0.29933035]]), array([[0.30092037]]), array([[0.31695764]]), array([[0.27519106]]), array([[0.32205525]]), array([[0.31806673]]), array([[0.30062943]]), array([[0.33779381]]), array([[0.31296987]]), array([[0.29652803]]), array([[0.27053407]]), array([[0.3071532]]), array([[0.30292856]]), array([[0.39788361]]), array([[0.27669913]]), array([[0.30391321]]), array([[0.2872956]]), array([[0.4164344]]), array([[0.31256774]]), array([[0.43377358]]), array([[0.3091914]]), array([[0.31007082]]), array([[0.32825856]]), array([[0.30415022]]), array([[0.36606564]]), array([[0.35470657]]), array([[0.3604775]]), array([[0.3163035]]), array([[0.29500404]]), array([[0.29898888]]), array([[0.30775623]]), array([[0.38531875]]), array([[0.31062783]]), array([[0.42308513]]), array([[0.28732335]]), array([[0.48434485]]), array([[0.33863941]]), array([[0.28319527]]), array([[0.29402402]]), array([[0.40871077]]), array([[0.30144575]]), array([[0.28425724]]), array([[0.30801643]]), array([[0.34067898]]), array([[0.29824818]]), array([[0.30003439]]), array([[0.30371759]]), array([[0.34780832]]), array([[0.50019653]]), array([[0.35663286]]), array([[0.29412818]]), array([[0.30776217]]), array([[0.31305497]]), array([[0.32447991]]), array([[0.27537811]]), array([[0.28954708]]), array([[0.31049863]]), array([[0.31760995]]), array([[0.31160009]]), array([[0.35834301]]), array([[0.32006926]]), array([[0.42509657]]), array([[0.41679349]]), array([[0.31310132]]), array([[0.47975819]]), array([[0.45243203]]), array([[0.37045361]]), array([[0.3377696]]), array([[0.41464264]]), array([[0.43469195]]), array([[0.32163845]]), array([[0.34538011]]), array([[0.28638843]]), array([[0.32463037]]), array([[0.32062292]]), array([[0.30146835]]), array([[0.29730898]]), array([[0.29456922]]), array([[0.30334157]]), array([[0.31141729]]), array([[0.31575442]]), array([[0.33344467]]), array([[0.30475914]]), array([[0.32764639]]), array([[0.29508765]]), array([[0.39272523]]), array([[0.28694986]]), array([[0.27704307]]), array([[0.37840594]]), array([[0.44524393]]), array([[0.30745157]]), array([[0.32470995]]), array([[0.32208868]]), array([[0.31173996]]), array([[0.29859767]]), array([[0.30656595]]), array([[0.38645191]]), array([[0.30835338]]), array([[0.31597258]]), array([[0.31187512]]), array([[0.30446145]]), array([[0.2968864]]), array([[0.3517751]]), array([[0.31690937]]), array([[0.38794199]]), array([[0.31252497]]), array([[0.30412372]]), array([[0.29739211]]), array([[0.27959895]]), array([[0.34318451]]), array([[0.34941911]]), array([[0.30454951]]), array([[0.28396715]]), array([[0.39975178]]), array([[0.30871056]]), array([[0.29708762]]), array([[0.30303915]]), array([[0.33419668]]), array([[0.29835993]]), array([[0.32531023]]), array([[0.28462739]]), array([[0.28731209]]), array([[0.29208078]]), array([[0.29917914]]), array([[0.29284428]]), array([[0.31127977]]), array([[0.34063145]]), array([[0.30502121]]), array([[0.40620124]]), array([[0.40699063]]), array([[0.33365614]]), array([[0.33950965]]), array([[0.31545062]]), array([[0.32850139]]), array([[0.32890131]]), array([[0.3237815]]), array([[0.29798282]]), array([[0.38228532]]), array([[0.32364013]]), array([[0.28683084]]), array([[0.38389365]]), array([[0.30494624]]), array([[0.39589855]]), array([[0.33824285]]), array([[0.33847279]]), array([[0.44141887]]), array([[0.30338612]]), array([[0.40100554]]), array([[0.30727213]]), array([[0.32955223]]), array([[0.31510442]]), array([[0.3267972]]), array([[0.30618239]]), array([[0.31923226]]), array([[0.3176098]]), array([[0.28612524]]), array([[0.40307137]]), array([[0.50246796]]), array([[0.33166273]]), array([[0.30092757]]), array([[0.32154546]]), array([[0.326032]]), array([[0.32190162]]), array([[0.2854209]]), array([[0.39170836]]), array([[0.30314747]]), array([[0.2860498]]), array([[0.30903457]]), array([[0.3436258]]), array([[0.31009172]]), array([[0.29292205]]), array([[0.31329471]]), array([[0.3363429]]), array([[0.32651001]]), array([[0.29776819]]), array([[0.35261175]]), array([[0.3075466]]), array([[0.33534343]]), array([[0.31900502]]), array([[0.32274794]]), array([[0.34204264]]), array([[0.30974783]]), array([[0.33824398]]), array([[0.41864545]]), array([[0.30455581]]), array([[0.3654224]]), array([[0.31476674]]), array([[0.37677424]]), array([[0.39196476]]), array([[0.30479989]]), array([[0.31900754]]), array([[0.33589172]]), array([[0.31495676]]), array([[0.31300607]]), array([[0.40424165]]), array([[0.42561691]]), array([[0.34010353]]), array([[0.33301029]]), array([[0.30765511]]), array([[0.47714605]]), array([[0.27595169]]), array([[0.27965106]]), array([[0.304681]]), array([[0.29155771]]), array([[0.35084757]]), array([[0.34890714]]), array([[0.29869593]]), array([[0.33214408]]), array([[0.33512439]]), array([[0.33823455]]), array([[0.34974997]]), array([[0.29824824]]), array([[0.39888632]]), array([[0.41491585]]), array([[0.31825504]]), array([[0.31387142]]), array([[0.27858714]]), array([[0.48016892]]), array([[0.29556056]]), array([[0.32420407]]), array([[0.28543237]]), array([[0.27318298]]), array([[0.32515445]]), array([[0.30804678]]), array([[0.31861782]]), array([[0.30505112]]), array([[0.30580163]]), array([[0.30633457]]), array([[0.32921527]]), array([[0.4163019]]), array([[0.2921848]]), array([[0.42656368]]), array([[0.33073977]]), array([[0.30159175]]), array([[0.27096659]]), array([[0.26910397]]), array([[0.29623621]]), array([[0.33787206]]), array([[0.33931511]]), array([[0.3196726]]), array([[0.3239613]]), array([[0.32690536]]), array([[0.28739614]]), array([[0.28537253]]), array([[0.28441893]]), array([[0.30006458]]), array([[0.2917772]]), array([[0.29536228]]), array([[0.31537508]]), array([[0.27790806]]), array([[0.31579673]]), array([[0.28773491]]), array([[0.2839381]]), array([[0.28237345]]), array([[0.33141979]]), array([[0.30099069]]), array([[0.32746977]]), array([[0.29741731]]), array([[0.35088648]]), array([[0.42857195]]), array([[0.44090969]]), array([[0.42170812]]), array([[0.36886157]]), array([[0.42888276]]), array([[0.27258993]])]\n",
            "The classification accuracy is  63.38028169014085 %\n",
            "Total time =  6.42742657661438\n",
            "Train time =  6.204315662384033\n",
            "Predict time =  0.0194242000579834\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Accuracy calculation \n",
        "\n",
        "TP = 0\n",
        "FP = 0\n",
        "FN = 0\n",
        "TN = 0\n",
        "\n",
        "for i in range(len(pred)):\n",
        "  if pred[i] == 1:\n",
        "    if expt[i] == 1:\n",
        "      TP += 1\n",
        "    else:\n",
        "      FP += 1\n",
        "  elif pred[i] == 0:\n",
        "    if expt[i] == 0:\n",
        "      TN += 1\n",
        "    else:\n",
        "      FN += 1\n",
        "\n",
        "accuracy = (TP + TN)/(TP+TN+FP+FN)\n",
        "\n",
        "print(\"The classification accuracy is \", accuracy*100, \"%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x47TpEz38Q2f",
        "outputId": "4b454051-3549-4c98-fd82-b9d91693ee58"
      },
      "execution_count": 170,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The classification accuracy is  87.32394366197182 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#out = net.predict(x_train)\n",
        "#print(\"out \", out)\n",
        "#print(out[0])\n",
        "\n",
        "#def answerFn(out):\n",
        "#  p = np.zeros([len(x_train)])\n",
        "#  for i in range(len(x_train)):\n",
        "#    if out[i] < 0.5:\n",
        "#      p[i] = 0\n",
        "#    else:\n",
        "#      p[i] = 1\n",
        "#  return p\n",
        "\n",
        "#pred = answerFn(out)\n",
        "#expt = y_train.reshape(len(y_train)) * 1.0\n",
        "#print(\"pred \", pred)\n",
        "#print(\"expect \", expt)"
      ],
      "metadata": {
        "id": "KfzU89IVVxL7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# DO NOT RUN\n",
        "\n",
        "\n",
        "# training data\n",
        "#x_train = np.array([[[0,0]], [[0,1]], [[1,0]], [[1,1]], [[1,1]]])\n",
        "#print(x_train)\n",
        "\n",
        "#y_train = np.array([[[0]], [[1]], [[1]], [[0]], [[0]]])\n",
        "#y_train = np.array([[[1, 0]], [[0, 1]], [[0, 1]], [[1, 0]], [[1, 0]]])\n",
        "#print(len(x_train[0][0]))\n",
        "x_train = X.reshape(len(X), len(X[0]), 1)\n",
        "y_train = Y.reshape(len(Y), 1)\n",
        "\n",
        "#print(x_train)\n",
        "#print(\"length of input 0\", len(x_train))\n",
        "#print(\"length of input 1\", len(x_train[0]))\n",
        "#print(\"length of input 2\", len(x_train[0][0]))\n",
        "\n",
        "\n",
        "#if len(Y) == len(X): \n",
        "#  print(\"lol\")\n",
        "\n",
        "#startTime = time.time()\n",
        "\n",
        "# network\n",
        "net = Network()\n",
        "\n",
        "\n",
        "#net.add(Dense(2, 5))\n",
        "#net.add(Activation(tanh, tanhDerv))\n",
        "#net.add(Dense(5, 15))\n",
        "#net.add(Activation(tanh, tanhDerv))\n",
        "#net.add(Dense(15, 5))\n",
        "#net.add(Activation(tanh, tanhDerv))\n",
        "#net.add(Dense(5, 2))\n",
        "#net.add(Activation(tanh, tanhDerv))\n",
        "\n",
        "net.add(Dense(len(x_train[0][0]), 20))\n",
        "net.add(Activation(tanh, tanhDerv))\n",
        "net.add(Dense(20, 1))\n",
        "net.add(Activation(tanh, tanhDerv))\n",
        "#net.add(Dense(10, 1))\n",
        "#net.add(Activation(tanh, tanhDerv))\n",
        "\n",
        "# train\n",
        "net.use(mse, mseDerv)\n",
        "errorStore = net.fit(x_train, y_train, epochs=2000, learning_rate=0.00001#)\n",
        "\n",
        "#endTimeHere = time.time()\n",
        "\n",
        "print(\"last error is \", errorStore[-1])\n",
        "xPlot = np.array(range(0, 2000))\n",
        "yPlot = errorStore\n",
        "plt.title(\"Plotting 1-D array\")\n",
        "plt.xlabel(\"X axis\")\n",
        "plt.ylabel(\"Y axis\")\n",
        "plt.plot(xPlot, yPlot, color = \"red\", label = \"Array elements\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "# test\n",
        "#out = net.predict(x_train)\n",
        "\n",
        "#print(\"Time taken is \", endTimeHere - startTime)\n",
        "#print(out)\n",
        "#print(y_train.T)\n",
        "#out, out2 = net.predict(x_train)\n",
        "#print(out[0])\n",
        "#res = pd.DataFrame()\n",
        "#expd = y_train.reshape(len(y_train)).T\n",
        "#pred = out2\n",
        "##pred = np.concatenate( out, axis=0 )\n",
        "##pred = np.array(pred)\n",
        "##pred = np.array(out).T\n",
        "#print(\"expected \", expd)\n",
        "#print(\"predicted \", pred)\n",
        "#res = pd.DataFrame()\n",
        "#res['Predictions'] = pred\n",
        "#res['Expectation'] = expd\n",
        "#print(res)\n",
        "#print(\"Accuracy: \", res.loc[res['Predictions']==res['Expectation']].shape[0] / res.shape[0] * 100)\n"
      ],
      "metadata": {
        "id": "ukkkEvZzkoFX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "useful links\n",
        "\n",
        "https://www.delftstack.com/howto/python/plot-array-python/\n",
        "https://builtin.com/data-science/evaluating-classification-models\n",
        "https://developers.google.com/machine-learning/crash-course/classification/accuracy\n",
        "https://towardsdatascience.com/10-tips-for-a-better-google-colab-experience-33f8fe721b82\n",
        "https://pythonbaba.com/python-code-to-find-2d-array-size/\n",
        "https://towardsdatascience.com/how-to-build-your-own-neural-network-from-scratch-in-python-68998a08e4f6"
      ],
      "metadata": {
        "id": "zKb8U6urEKvK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# DO NOT RUN\n",
        "\n",
        "#main \n",
        "\n",
        "X = np.reshape([[0,0], [0,1], [1,0], [1,1]], (4,2,1))\n",
        "Y = np.reshape([[0], [1], [1], [0]], (4,1,1))\n",
        "\n",
        "NN = [\n",
        "    Dense(2, 3), \n",
        "    Tanh(), \n",
        "    Dense(3,1), \n",
        "    Tanh()\n",
        "]\n",
        "\n",
        "epoch = 1000\n",
        "learnRate = 0.1\n",
        "\n",
        "#train\n",
        "for e in range(epoch):\n",
        "  error = 0\n",
        "  for x,y in zip(X,Y):\n",
        "    \n",
        "    #forward\n",
        "    output = x\n",
        "    for layer in NN:\n",
        "      output = layer.forward(output)\n",
        "\n",
        "    #error\n",
        "    error += mse(y, output)\n",
        "\n",
        "    #backward\n",
        "    grad = mseDerv(y, output)\n",
        "    for layer in reversed(NN):\n",
        "      grad = layer.backward(grad, learnRate)\n",
        "\n",
        "  error /= len(x)\n",
        "  print('%d/%d, error=%f' %(e+1, epoch, error))\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "4BnO3ESqZkva"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}