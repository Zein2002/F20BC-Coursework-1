{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction \n",
        "\n",
        "This code was influenced by the blog below. Any code taken from there has been highlighted using comments.\n",
        "\n",
        "https://towardsdatascience.com/math-neural-network-from-scratch-in-python-d6da9f29ce65"
      ],
      "metadata": {
        "id": "veZLAogllQZX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Neural Network Classes "
      ],
      "metadata": {
        "id": "PO9hwrkslCUj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import libaries"
      ],
      "metadata": {
        "id": "AJNSlcJUTn6z"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 266,
      "metadata": {
        "id": "DB3Zk3HAT12x"
      },
      "outputs": [],
      "source": [
        "#plan\n",
        "\n",
        "#oop nn, ie has classes \n",
        "\n",
        "# get the predicted values stored somewhere\n",
        "\n",
        "#import libraries \n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "import time"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Implement the abstract layer class"
      ],
      "metadata": {
        "id": "6AAb2cfLTv1b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#### CODE FROM SOURCE - COMMENTS ARE MINE ####\n",
        "\n",
        "# this is the abstract layer class - all layers in the network inherit from this class\n",
        "\n",
        "class Layer:\n",
        "  \n",
        "  def __init__(layer):                          # constructor\n",
        "    layer.input = None                          # property to store the number of input neurons to this layer\n",
        "    layer.output = None                         # property to store the number of neurons in this layer\n",
        "\n",
        "  def forward(layer, input):                    # function to perform forward propagation \n",
        "    raise NotImplementedError\n",
        "\n",
        "  def backward(layer, outputGrad, learnRate):   # function to perform backward propagation \n",
        "    raise NotImplementedError"
      ],
      "metadata": {
        "id": "Fs1W8fesUs-D"
      },
      "execution_count": 267,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Implement Hidden layer\n"
      ],
      "metadata": {
        "id": "np1k3EpDT-6B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#### BASED ON SOURCE CODE - COMMENTS AND CHANGES ARE MINE ####\n",
        "\n",
        "# the hidden layer class \n",
        "# this class randomises the weights and biases for the network, computes z (sum of weights and inputs plus the bias) \n",
        "# and will update the weights and biases for back propagation \n",
        "\n",
        "class Hidden(Layer):\n",
        "\n",
        "  def __init__(layer, inputSize, outputSize):          # constructor with number of input neurons and number of output neurons \n",
        "    \n",
        "    ## NEW\n",
        "    layer.inputSize = inputSize\n",
        "    layer.outputSize = outputSize\n",
        "\n",
        "    ## MY CHANGES ##\n",
        "    layer.weights = np.random.rand(layer.outputSize, layer.inputSize) - 0.5        # create a 2D array with random values between -0.5 and 0.5 for the initial weights for the layer \n",
        "    layer.bias = np.random.rand(layer.outputSize, 1) - 0.5                         # create a vector with random values between -0.5 and 0.5 for the initial biases for the layer\n",
        "\n",
        "\n",
        "  def forward(layer, inputData):                                      # function forward propagation \n",
        "    \n",
        "    layer.input = inputData                                           # store input data\n",
        "    \n",
        "    ## MY CHANGES ##\n",
        "    layer.output = np.dot(layer.weights, layer.input) + layer.bias      # calculate z using the dot product of the matrix of weights and the vector of inputs \n",
        "    \n",
        "    return layer.output                                                 # returns z as the ouput from the hidden layer \n",
        "\n",
        "\n",
        "  def backward(layer, outputGrad, learnRate):                           # function for backward propagation \n",
        "    \n",
        "    ## MY CHANGES ##\n",
        "    inputErrorDerv = np.dot(layer.weights.T, outputGrad)                # calculate the derivative of the error wrt the inputs \n",
        "    weightsErrorDerv = np.dot(outputGrad, layer.input.T)                # calculate the derivative of the error wrt the weights\n",
        "    biasErrorDerv = outputGrad                                          # the derivative of the error wrt the bias is the same as the derivative of the error wrt to the output \n",
        "\n",
        "    ## FROM SOURCE ##\n",
        "    layer.weights = layer.weights - learnRate * weightsErrorDerv        # update the weights \n",
        "    \n",
        "    ## MY CHANGES ##\n",
        "    layer.bias = layer.bias - learnRate * biasErrorDerv                 # update the bias\n",
        "    \n",
        "    return inputErrorDerv                                               # returns the derivative of the error wrt the inputs for the derivative of the activation function \n",
        "  "
      ],
      "metadata": {
        "id": "K2miSzFKWEN2"
      },
      "execution_count": 268,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Implement activation layer"
      ],
      "metadata": {
        "id": "cZrcJEoHUMGF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#### BASED ON SOURCE CODE - COMMENTS AND CHANGES ARE MINE ####\n",
        "\n",
        "# this is the activation layer \n",
        "# this layer performs the activation function for foward propagation and the derivative of the activation function for back propagation \n",
        "\n",
        "class Activation(Layer):\n",
        "\n",
        "  def __init__(layer, activationFn, activationFnDerv):            # constructor with activation function and it's derivative function \n",
        "    \n",
        "    ## FROM SOURCE ##\n",
        "    layer.activationFn = activationFn                             # the activation function provided for forward propagation \n",
        "    layer.activationFnDerv = activationFnDerv                     # the derivative of the activation function provided for backpropagation \n",
        "\n",
        "\n",
        "  def forward(layer, input):                                      # function for forward propagation \n",
        "    \n",
        "    ## FROM SOURCE ##\n",
        "    layer.input = input                                           # gets z from the previous hidden layer \n",
        "    layer.output = layer.activationFn(layer.input)                # passes z to the activation function\n",
        "    return layer.output                                           # returns the output of the activation function for forward propagation \n",
        "  \n",
        "\n",
        "  def backward(layer, outputGrad, learnRate):                     # function for backward propagation \n",
        "    \n",
        "    ## MY CHANGES ##\n",
        "    backDervFn = layer.activationFnDerv(layer.input) * outputGrad # calulates the derivative of the activation function\n",
        "\n",
        "    return backDervFn                                             # returns the result of the derivative of the activation function \n"
      ],
      "metadata": {
        "id": "qBZlye0kWWg2"
      },
      "execution_count": 269,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Activation Functions"
      ],
      "metadata": {
        "id": "slJz2g0HF3Gf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## MY CODE AND COMMENTS ##\n",
        "\n",
        "# these are the activation functions and their respective derivative functions \n",
        "\n",
        "# the hyperbolic tangent activation function\n",
        "def tanh(z):\n",
        "  z = z.astype(float) \n",
        "  r = np.tanh(z)\n",
        "  return r\n",
        "\n",
        "# the derivative of the hyperbolic tangent activation function \n",
        "def tanhDerv(x):\n",
        "  x = x.astype(float)\n",
        "  r = 1 - np.tanh(x)**2\n",
        "  return r\n",
        "\n",
        "# the REctified Linear Unit activation function\n",
        "def relu(z):\n",
        "  for i in range(len(z[0])):\n",
        "    z[0][i] = np.maximum(0,z[0][i])\n",
        "  return z\n",
        "\n",
        "# the derivative of the REctified Linear Unit activation function\n",
        "def reluDerv(z):\n",
        "  r = np.zeros([1,len(z[0])])\n",
        "  for i in range(len(z[0])):\n",
        "    if(z[0][i]<0):\n",
        "      r[0][i] = 0\n",
        "    else: \n",
        "      r[0][i] = 1\n",
        "  return r\n",
        "\n",
        "# the logistic/sigmoid activation function\n",
        "def logistic(x):\n",
        "  x=x.astype(float)\n",
        "  r = 1/(1+np.exp(-x))\n",
        "  return r\n",
        "\n",
        "# the derivative of the logistic/sigmoid activation function\n",
        "def logisticDerv(x):\n",
        "  x=x.astype(float)\n",
        "  r = logistic(x)*(1-logistic(x)) #(-np.exp(-x)/((1+np.exp(-x))**2))\n",
        "  return r"
      ],
      "metadata": {
        "id": "abIt1wd7YYdT"
      },
      "execution_count": 270,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loss Functions"
      ],
      "metadata": {
        "id": "S7oz9jFPFtBq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Loss functions and their respective derivatives \n",
        "\n",
        "# mean squared value function  \n",
        "def mse(yExpect, yPred):\n",
        "  loss = np.mean(np.power(yExpect - yPred, 2))\n",
        "  return loss\n",
        "\n",
        "# derivatice of mean squared value function \n",
        "def mseDerv(yExpect, yPred):\n",
        "  lossDerv = 2*(yPred - yExpect)/yExpect.size\n",
        "  return lossDerv\n",
        "\n",
        "# cross entropy function \n",
        "def crossEn(yExpect, yPred):\n",
        "  if yPred < 0:\n",
        "    yPred = 0\n",
        "  errCE = -(yExpect*np.log(yPred)+(1-yExpect)*np.log(1-yPred))\n",
        "  return errCE\n",
        "\n",
        "# derivative of cross entropy function \n",
        "def crossEnDerv(yExpect, yPred):\n",
        "  return -(yExpect/yPred)+((1-yExpect)/(1-yPred))\n",
        "\n",
        "# absolute loss function \n",
        "def absLoss(yExpect, yPred):\n",
        "  return abs(yExpect-yPred)\n",
        "\n",
        "# derivative of absolute loss function \n",
        "def absLossDerv(yExpect, yPred):\n",
        "  loss = np.zeros([len(yPred), 1])\n",
        "  for i in range(len(yExpect)):\n",
        "    if yPred[i] > yExpect[i]:\n",
        "      loss[i] = 1\n",
        "    elif yPred[i] < yExpect[i]:\n",
        "      loss[i] = -1\n",
        "    else :\n",
        "      loss[i] = 0\n",
        "  return loss\n"
      ],
      "metadata": {
        "id": "x4NwpK64ZeQX"
      },
      "execution_count": 271,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Nextwork class"
      ],
      "metadata": {
        "id": "RIebDOSWUPqi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#### FROM SOURCE - ALL COMMENTS AND CHANGES ARE MINE ####\n",
        "\n",
        "# the network class is used to build, train and get predictions from the neural network\n",
        "\n",
        "class Network:\n",
        "\n",
        "    def __init__(self):                 # constructor takes no parameters \n",
        "        \n",
        "        self.layers = []                # creates a list for the layers in the network\n",
        "        self.loss = None                # the loss function for the network\n",
        "        self.loss_prime = None          # the derivative of the loss function for the network \n",
        "\n",
        "\n",
        "    def add(self, layer):               # function used to add layers to the neural network\n",
        "\n",
        "        self.layers.append(layer)       # add a layer to the network by adding it to the layer list\n",
        "\n",
        "\n",
        "    def use(self, loss, loss_prime):    # assign a loss function and it's derivative to the network \n",
        "        self.loss = loss                # assigns the loss function \n",
        "        self.loss_prime = loss_prime    # assigns the derivative of the loss function \n",
        "\n",
        "\n",
        "    def predict(self, input_data):                  # predict the outputs from the trained neural network\n",
        "       \n",
        "        samples = len(input_data)                   # get the number of outputs to be predicted from the number of input rows in the dataset\n",
        "        result = []                                 # initialise a list to store the predicted results from the network\n",
        "        for i in range(samples):                    # for each row in the data (every output to be)\n",
        "            output = input_data[i]                  # get the next row in the input data          \n",
        "            for layer in self.layers:               # for each layer in the network\n",
        "              output = layer.forward(output)        # perform a forward pass through the layers in the network\n",
        "            result.append(output)                   # store the predicted result from the forward pass in the list\n",
        "        return result                               # return the list of results \n",
        "\n",
        "\n",
        "    def fitSGD(self, x_train, y_train, epochs, learning_rate):    # train the neural network using stocastic gradient descent \n",
        "        \n",
        "        samples = len(x_train)                                        # get the number of rows in the input data\n",
        "        errorStore = np.zeros(epochs)                                 # initialise an array to store the error for each epoch \n",
        "        for i in range(epochs):                                       # for each epoch\n",
        "            err = 0                                                   # set error sum to 0\n",
        "            for j in range(samples):                                  # for each row in the input data\n",
        "                \n",
        "                #Performs a forward pass and backpropagation for every row in the input data\n",
        "                \n",
        "                output = x_train[j]                                   # store the inputs for the row\n",
        "                for layer in self.layers:                             # for each layer in the network\n",
        "                  output = layer.forward(output)                      # perform forward propagation \n",
        "                err += self.loss(y_train[j], output)                  # add the error for this forward pass calculated with the loss function to the error sum\n",
        "                error = self.loss_prime(y_train[j], output)           # calclate the derivative of the loss function \n",
        "                for layer in reversed(self.layers):                   # for each layer in the network perform backpropagation \n",
        "                    error = layer.backward(error, learning_rate)      # updates the weights and biases\n",
        "            err /= samples                                            # calculate the error for this epoch\n",
        "            errorStore[i] = err                                       # store the error in the array\n",
        "            ## MY CHANGES ##\n",
        "            print('The error at epoch %d/%d is error=%f' % (i+1, epochs, err))      # print the error for the epoch\n",
        "        return errorStore\n",
        "    \n",
        "    \n",
        "    # see comments above for SGD\n",
        "    def fitBatch(self, x_train, y_train, epochs, learning_rate):\n",
        "        samples = len(x_train)\n",
        "        errorStore = np.zeros(epochs)\n",
        "        for i in range(epochs):\n",
        "            err = 0\n",
        "            for j in range(samples):\n",
        "            \n",
        "            # performs a forward pass for every row in the input data before a backpropagation \n",
        "\n",
        "                output = x_train[j]\n",
        "                for layer in self.layers:                       \n",
        "                  output = layer.forward(output)\n",
        "                err += self.loss(y_train[j], output)\n",
        "                error = self.loss_prime(y_train[j], output)\n",
        "            for layer in reversed(self.layers):\n",
        "                error = layer.backward(error, learning_rate)\n",
        "            err /= samples\n",
        "            errorStore[i] = err\n",
        "            ## MY CHANGES ##\n",
        "            print('The error at epoch %d/%d is error=%f' % (i+1, epochs, err))      # print the error for the epoch\n",
        "        return errorStore\n",
        "\n",
        "    # see comments above for SGD\n",
        "    def fitMinBatch(self, x_train, y_train, epochs, learning_rate, batchSize):\n",
        "        samples = len(x_train)\n",
        "        errorStore = np.zeros(epochs)\n",
        "        for i in range(epochs):\n",
        "            err = 0\n",
        "\n",
        "            # performs forward pass on random batch of input rows (size specified in the constructor) before performing a backpropagtion \n",
        "\n",
        "            ## MY CHANGES ##\n",
        "            for j in range(batchSize):                                                 \n",
        "                index = random.randint(0, len(x_train))-1\n",
        "                output = x_train[index]\n",
        "                for layer in self.layers:\n",
        "                  output = layer.forward(output)\n",
        "                err += self.loss(y_train[index], output)\n",
        "                error = self.loss_prime(y_train[index], output)\n",
        "\n",
        "            ## SOURCE ##\n",
        "            for layer in reversed(self.layers):\n",
        "                error = layer.backward(error, learning_rate)\n",
        "            err /= samples\n",
        "            errorStore[i] = err\n",
        "            ## MY CHANGES ##\n",
        "            print('The error at epoch %d/%d is error=%f' % (i+1, epochs, err))      # print the error for the epoch\n",
        "        return errorStore\n",
        "        "
      ],
      "metadata": {
        "id": "hZymovfzmJIR"
      },
      "execution_count": 272,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data manipulation "
      ],
      "metadata": {
        "id": "_wBkHnc-cMV7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import libaries \n",
        "\n",
        "import sys\n",
        "assert sys.version_info >= (3, 5)\n",
        "import io\n",
        "\n",
        "# upload the file - google colab\n",
        "#from google.colab import files\n",
        "#uploaded = files.upload()     # upload the wdbc.data file provided in the coursework sheet\n",
        "#data = pd.read_csv(io.BytesIO(uploaded['wdbc.data']))             # store the data in the file in a dataframe\n",
        "\n",
        "# upload the file - jupter notebook\n",
        "data = pd.read_csv(\"./wdbc.data\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "id": "5w3vkmj9b7O9",
        "outputId": "879fceb9-e457-4405-d136-1d79ca109ba7"
      },
      "execution_count": 273,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-a8eeacce-9acd-46fc-b876-46896247a9c5\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-a8eeacce-9acd-46fc-b876-46896247a9c5\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving wdbc.data to wdbc (8).data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "print(data)                                                       # sanity check \n",
        "data.head()                                                       # sanity check"
      ],
      "metadata": {
        "id": "Ipm-BbOzb8sd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 993
        },
        "outputId": "30364e46-3f35-43a0-9b64-b7e17eff4d9e"
      },
      "execution_count": 274,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "       842302  M  17.99  10.38   122.8    1001   0.1184   0.2776   0.3001  \\\n",
            "0      842517  M  20.57  17.77  132.90  1326.0  0.08474  0.07864  0.08690   \n",
            "1    84300903  M  19.69  21.25  130.00  1203.0  0.10960  0.15990  0.19740   \n",
            "2    84348301  M  11.42  20.38   77.58   386.1  0.14250  0.28390  0.24140   \n",
            "3    84358402  M  20.29  14.34  135.10  1297.0  0.10030  0.13280  0.19800   \n",
            "4      843786  M  12.45  15.70   82.57   477.1  0.12780  0.17000  0.15780   \n",
            "..        ... ..    ...    ...     ...     ...      ...      ...      ...   \n",
            "563    926424  M  21.56  22.39  142.00  1479.0  0.11100  0.11590  0.24390   \n",
            "564    926682  M  20.13  28.25  131.20  1261.0  0.09780  0.10340  0.14400   \n",
            "565    926954  M  16.60  28.08  108.30   858.1  0.08455  0.10230  0.09251   \n",
            "566    927241  M  20.60  29.33  140.10  1265.0  0.11780  0.27700  0.35140   \n",
            "567     92751  B   7.76  24.54   47.92   181.0  0.05263  0.04362  0.00000   \n",
            "\n",
            "      0.1471  ...   25.38  17.33   184.6    2019   0.1622   0.6656  0.7119  \\\n",
            "0    0.07017  ...  24.990  23.41  158.80  1956.0  0.12380  0.18660  0.2416   \n",
            "1    0.12790  ...  23.570  25.53  152.50  1709.0  0.14440  0.42450  0.4504   \n",
            "2    0.10520  ...  14.910  26.50   98.87   567.7  0.20980  0.86630  0.6869   \n",
            "3    0.10430  ...  22.540  16.67  152.20  1575.0  0.13740  0.20500  0.4000   \n",
            "4    0.08089  ...  15.470  23.75  103.40   741.6  0.17910  0.52490  0.5355   \n",
            "..       ...  ...     ...    ...     ...     ...      ...      ...     ...   \n",
            "563  0.13890  ...  25.450  26.40  166.10  2027.0  0.14100  0.21130  0.4107   \n",
            "564  0.09791  ...  23.690  38.25  155.00  1731.0  0.11660  0.19220  0.3215   \n",
            "565  0.05302  ...  18.980  34.12  126.70  1124.0  0.11390  0.30940  0.3403   \n",
            "566  0.15200  ...  25.740  39.42  184.60  1821.0  0.16500  0.86810  0.9387   \n",
            "567  0.00000  ...   9.456  30.37   59.16   268.6  0.08996  0.06444  0.0000   \n",
            "\n",
            "     0.2654  0.4601   0.1189  \n",
            "0    0.1860  0.2750  0.08902  \n",
            "1    0.2430  0.3613  0.08758  \n",
            "2    0.2575  0.6638  0.17300  \n",
            "3    0.1625  0.2364  0.07678  \n",
            "4    0.1741  0.3985  0.12440  \n",
            "..      ...     ...      ...  \n",
            "563  0.2216  0.2060  0.07115  \n",
            "564  0.1628  0.2572  0.06637  \n",
            "565  0.1418  0.2218  0.07820  \n",
            "566  0.2650  0.4087  0.12400  \n",
            "567  0.0000  0.2871  0.07039  \n",
            "\n",
            "[568 rows x 32 columns]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "     842302  M  17.99  10.38   122.8    1001   0.1184   0.2776  0.3001  \\\n",
              "0    842517  M  20.57  17.77  132.90  1326.0  0.08474  0.07864  0.0869   \n",
              "1  84300903  M  19.69  21.25  130.00  1203.0  0.10960  0.15990  0.1974   \n",
              "2  84348301  M  11.42  20.38   77.58   386.1  0.14250  0.28390  0.2414   \n",
              "3  84358402  M  20.29  14.34  135.10  1297.0  0.10030  0.13280  0.1980   \n",
              "4    843786  M  12.45  15.70   82.57   477.1  0.12780  0.17000  0.1578   \n",
              "\n",
              "    0.1471  ...  25.38  17.33   184.6    2019  0.1622  0.6656  0.7119  0.2654  \\\n",
              "0  0.07017  ...  24.99  23.41  158.80  1956.0  0.1238  0.1866  0.2416  0.1860   \n",
              "1  0.12790  ...  23.57  25.53  152.50  1709.0  0.1444  0.4245  0.4504  0.2430   \n",
              "2  0.10520  ...  14.91  26.50   98.87   567.7  0.2098  0.8663  0.6869  0.2575   \n",
              "3  0.10430  ...  22.54  16.67  152.20  1575.0  0.1374  0.2050  0.4000  0.1625   \n",
              "4  0.08089  ...  15.47  23.75  103.40   741.6  0.1791  0.5249  0.5355  0.1741   \n",
              "\n",
              "   0.4601   0.1189  \n",
              "0  0.2750  0.08902  \n",
              "1  0.3613  0.08758  \n",
              "2  0.6638  0.17300  \n",
              "3  0.2364  0.07678  \n",
              "4  0.3985  0.12440  \n",
              "\n",
              "[5 rows x 32 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-68b98faf-f114-4915-b2ee-c7a6bdfbd827\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>842302</th>\n",
              "      <th>M</th>\n",
              "      <th>17.99</th>\n",
              "      <th>10.38</th>\n",
              "      <th>122.8</th>\n",
              "      <th>1001</th>\n",
              "      <th>0.1184</th>\n",
              "      <th>0.2776</th>\n",
              "      <th>0.3001</th>\n",
              "      <th>0.1471</th>\n",
              "      <th>...</th>\n",
              "      <th>25.38</th>\n",
              "      <th>17.33</th>\n",
              "      <th>184.6</th>\n",
              "      <th>2019</th>\n",
              "      <th>0.1622</th>\n",
              "      <th>0.6656</th>\n",
              "      <th>0.7119</th>\n",
              "      <th>0.2654</th>\n",
              "      <th>0.4601</th>\n",
              "      <th>0.1189</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>842517</td>\n",
              "      <td>M</td>\n",
              "      <td>20.57</td>\n",
              "      <td>17.77</td>\n",
              "      <td>132.90</td>\n",
              "      <td>1326.0</td>\n",
              "      <td>0.08474</td>\n",
              "      <td>0.07864</td>\n",
              "      <td>0.0869</td>\n",
              "      <td>0.07017</td>\n",
              "      <td>...</td>\n",
              "      <td>24.99</td>\n",
              "      <td>23.41</td>\n",
              "      <td>158.80</td>\n",
              "      <td>1956.0</td>\n",
              "      <td>0.1238</td>\n",
              "      <td>0.1866</td>\n",
              "      <td>0.2416</td>\n",
              "      <td>0.1860</td>\n",
              "      <td>0.2750</td>\n",
              "      <td>0.08902</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>84300903</td>\n",
              "      <td>M</td>\n",
              "      <td>19.69</td>\n",
              "      <td>21.25</td>\n",
              "      <td>130.00</td>\n",
              "      <td>1203.0</td>\n",
              "      <td>0.10960</td>\n",
              "      <td>0.15990</td>\n",
              "      <td>0.1974</td>\n",
              "      <td>0.12790</td>\n",
              "      <td>...</td>\n",
              "      <td>23.57</td>\n",
              "      <td>25.53</td>\n",
              "      <td>152.50</td>\n",
              "      <td>1709.0</td>\n",
              "      <td>0.1444</td>\n",
              "      <td>0.4245</td>\n",
              "      <td>0.4504</td>\n",
              "      <td>0.2430</td>\n",
              "      <td>0.3613</td>\n",
              "      <td>0.08758</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>84348301</td>\n",
              "      <td>M</td>\n",
              "      <td>11.42</td>\n",
              "      <td>20.38</td>\n",
              "      <td>77.58</td>\n",
              "      <td>386.1</td>\n",
              "      <td>0.14250</td>\n",
              "      <td>0.28390</td>\n",
              "      <td>0.2414</td>\n",
              "      <td>0.10520</td>\n",
              "      <td>...</td>\n",
              "      <td>14.91</td>\n",
              "      <td>26.50</td>\n",
              "      <td>98.87</td>\n",
              "      <td>567.7</td>\n",
              "      <td>0.2098</td>\n",
              "      <td>0.8663</td>\n",
              "      <td>0.6869</td>\n",
              "      <td>0.2575</td>\n",
              "      <td>0.6638</td>\n",
              "      <td>0.17300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>84358402</td>\n",
              "      <td>M</td>\n",
              "      <td>20.29</td>\n",
              "      <td>14.34</td>\n",
              "      <td>135.10</td>\n",
              "      <td>1297.0</td>\n",
              "      <td>0.10030</td>\n",
              "      <td>0.13280</td>\n",
              "      <td>0.1980</td>\n",
              "      <td>0.10430</td>\n",
              "      <td>...</td>\n",
              "      <td>22.54</td>\n",
              "      <td>16.67</td>\n",
              "      <td>152.20</td>\n",
              "      <td>1575.0</td>\n",
              "      <td>0.1374</td>\n",
              "      <td>0.2050</td>\n",
              "      <td>0.4000</td>\n",
              "      <td>0.1625</td>\n",
              "      <td>0.2364</td>\n",
              "      <td>0.07678</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>843786</td>\n",
              "      <td>M</td>\n",
              "      <td>12.45</td>\n",
              "      <td>15.70</td>\n",
              "      <td>82.57</td>\n",
              "      <td>477.1</td>\n",
              "      <td>0.12780</td>\n",
              "      <td>0.17000</td>\n",
              "      <td>0.1578</td>\n",
              "      <td>0.08089</td>\n",
              "      <td>...</td>\n",
              "      <td>15.47</td>\n",
              "      <td>23.75</td>\n",
              "      <td>103.40</td>\n",
              "      <td>741.6</td>\n",
              "      <td>0.1791</td>\n",
              "      <td>0.5249</td>\n",
              "      <td>0.5355</td>\n",
              "      <td>0.1741</td>\n",
              "      <td>0.3985</td>\n",
              "      <td>0.12440</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows Ã— 32 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-68b98faf-f114-4915-b2ee-c7a6bdfbd827')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-68b98faf-f114-4915-b2ee-c7a6bdfbd827 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-68b98faf-f114-4915-b2ee-c7a6bdfbd827');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 274
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# add titles to the columns based on website provided in cw sheet\n",
        "data.columns = [\"ID_Number\",\"Diagnosis\",\"Radius1\",\"Texture1\",\"Perimeter1\",\"Area1\",\"Smoothness1\",\"Compactness1\",\"Concavity1\",\"Concave Points1\",\"Symmetry1\",\"Fractal Dimension1\",\"Radius2\",\"Texture2\",\"Perimeter2\",\"Area2\",\"Smoothness2\",\"Compactness2\",\"Concavity2\",\"Concave Points2\",\"Symmetry2\",\"Fractal Dimension2\",\"Radius3\",\"Texture3\",\"Perimeter3\",\"Area3\",\"Smoothness3\",\"Compactness3\",\"Concavity3\",\"Concave Points3\",\"Symmetry3\",\"Fractal Dimension3\"]\n",
        "data.head()   # check they have been added correctly"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 317
        },
        "id": "TUIjCXjvpRs8",
        "outputId": "612c0e39-91c6-49a7-c219-7f5095dbbb5b"
      },
      "execution_count": 275,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   ID_Number Diagnosis  Radius1  Texture1  Perimeter1   Area1  Smoothness1  \\\n",
              "0     842517         M    20.57     17.77      132.90  1326.0      0.08474   \n",
              "1   84300903         M    19.69     21.25      130.00  1203.0      0.10960   \n",
              "2   84348301         M    11.42     20.38       77.58   386.1      0.14250   \n",
              "3   84358402         M    20.29     14.34      135.10  1297.0      0.10030   \n",
              "4     843786         M    12.45     15.70       82.57   477.1      0.12780   \n",
              "\n",
              "   Compactness1  Concavity1  Concave Points1  ...  Radius3  Texture3  \\\n",
              "0       0.07864      0.0869          0.07017  ...    24.99     23.41   \n",
              "1       0.15990      0.1974          0.12790  ...    23.57     25.53   \n",
              "2       0.28390      0.2414          0.10520  ...    14.91     26.50   \n",
              "3       0.13280      0.1980          0.10430  ...    22.54     16.67   \n",
              "4       0.17000      0.1578          0.08089  ...    15.47     23.75   \n",
              "\n",
              "   Perimeter3   Area3  Smoothness3  Compactness3  Concavity3  Concave Points3  \\\n",
              "0      158.80  1956.0       0.1238        0.1866      0.2416           0.1860   \n",
              "1      152.50  1709.0       0.1444        0.4245      0.4504           0.2430   \n",
              "2       98.87   567.7       0.2098        0.8663      0.6869           0.2575   \n",
              "3      152.20  1575.0       0.1374        0.2050      0.4000           0.1625   \n",
              "4      103.40   741.6       0.1791        0.5249      0.5355           0.1741   \n",
              "\n",
              "   Symmetry3  Fractal Dimension3  \n",
              "0     0.2750             0.08902  \n",
              "1     0.3613             0.08758  \n",
              "2     0.6638             0.17300  \n",
              "3     0.2364             0.07678  \n",
              "4     0.3985             0.12440  \n",
              "\n",
              "[5 rows x 32 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-0a5c7d79-a971-45fd-81c8-dca06c09d231\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ID_Number</th>\n",
              "      <th>Diagnosis</th>\n",
              "      <th>Radius1</th>\n",
              "      <th>Texture1</th>\n",
              "      <th>Perimeter1</th>\n",
              "      <th>Area1</th>\n",
              "      <th>Smoothness1</th>\n",
              "      <th>Compactness1</th>\n",
              "      <th>Concavity1</th>\n",
              "      <th>Concave Points1</th>\n",
              "      <th>...</th>\n",
              "      <th>Radius3</th>\n",
              "      <th>Texture3</th>\n",
              "      <th>Perimeter3</th>\n",
              "      <th>Area3</th>\n",
              "      <th>Smoothness3</th>\n",
              "      <th>Compactness3</th>\n",
              "      <th>Concavity3</th>\n",
              "      <th>Concave Points3</th>\n",
              "      <th>Symmetry3</th>\n",
              "      <th>Fractal Dimension3</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>842517</td>\n",
              "      <td>M</td>\n",
              "      <td>20.57</td>\n",
              "      <td>17.77</td>\n",
              "      <td>132.90</td>\n",
              "      <td>1326.0</td>\n",
              "      <td>0.08474</td>\n",
              "      <td>0.07864</td>\n",
              "      <td>0.0869</td>\n",
              "      <td>0.07017</td>\n",
              "      <td>...</td>\n",
              "      <td>24.99</td>\n",
              "      <td>23.41</td>\n",
              "      <td>158.80</td>\n",
              "      <td>1956.0</td>\n",
              "      <td>0.1238</td>\n",
              "      <td>0.1866</td>\n",
              "      <td>0.2416</td>\n",
              "      <td>0.1860</td>\n",
              "      <td>0.2750</td>\n",
              "      <td>0.08902</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>84300903</td>\n",
              "      <td>M</td>\n",
              "      <td>19.69</td>\n",
              "      <td>21.25</td>\n",
              "      <td>130.00</td>\n",
              "      <td>1203.0</td>\n",
              "      <td>0.10960</td>\n",
              "      <td>0.15990</td>\n",
              "      <td>0.1974</td>\n",
              "      <td>0.12790</td>\n",
              "      <td>...</td>\n",
              "      <td>23.57</td>\n",
              "      <td>25.53</td>\n",
              "      <td>152.50</td>\n",
              "      <td>1709.0</td>\n",
              "      <td>0.1444</td>\n",
              "      <td>0.4245</td>\n",
              "      <td>0.4504</td>\n",
              "      <td>0.2430</td>\n",
              "      <td>0.3613</td>\n",
              "      <td>0.08758</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>84348301</td>\n",
              "      <td>M</td>\n",
              "      <td>11.42</td>\n",
              "      <td>20.38</td>\n",
              "      <td>77.58</td>\n",
              "      <td>386.1</td>\n",
              "      <td>0.14250</td>\n",
              "      <td>0.28390</td>\n",
              "      <td>0.2414</td>\n",
              "      <td>0.10520</td>\n",
              "      <td>...</td>\n",
              "      <td>14.91</td>\n",
              "      <td>26.50</td>\n",
              "      <td>98.87</td>\n",
              "      <td>567.7</td>\n",
              "      <td>0.2098</td>\n",
              "      <td>0.8663</td>\n",
              "      <td>0.6869</td>\n",
              "      <td>0.2575</td>\n",
              "      <td>0.6638</td>\n",
              "      <td>0.17300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>84358402</td>\n",
              "      <td>M</td>\n",
              "      <td>20.29</td>\n",
              "      <td>14.34</td>\n",
              "      <td>135.10</td>\n",
              "      <td>1297.0</td>\n",
              "      <td>0.10030</td>\n",
              "      <td>0.13280</td>\n",
              "      <td>0.1980</td>\n",
              "      <td>0.10430</td>\n",
              "      <td>...</td>\n",
              "      <td>22.54</td>\n",
              "      <td>16.67</td>\n",
              "      <td>152.20</td>\n",
              "      <td>1575.0</td>\n",
              "      <td>0.1374</td>\n",
              "      <td>0.2050</td>\n",
              "      <td>0.4000</td>\n",
              "      <td>0.1625</td>\n",
              "      <td>0.2364</td>\n",
              "      <td>0.07678</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>843786</td>\n",
              "      <td>M</td>\n",
              "      <td>12.45</td>\n",
              "      <td>15.70</td>\n",
              "      <td>82.57</td>\n",
              "      <td>477.1</td>\n",
              "      <td>0.12780</td>\n",
              "      <td>0.17000</td>\n",
              "      <td>0.1578</td>\n",
              "      <td>0.08089</td>\n",
              "      <td>...</td>\n",
              "      <td>15.47</td>\n",
              "      <td>23.75</td>\n",
              "      <td>103.40</td>\n",
              "      <td>741.6</td>\n",
              "      <td>0.1791</td>\n",
              "      <td>0.5249</td>\n",
              "      <td>0.5355</td>\n",
              "      <td>0.1741</td>\n",
              "      <td>0.3985</td>\n",
              "      <td>0.12440</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows Ã— 32 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-0a5c7d79-a971-45fd-81c8-dca06c09d231')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-0a5c7d79-a971-45fd-81c8-dca06c09d231 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-0a5c7d79-a971-45fd-81c8-dca06c09d231');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 275
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data.describe()     # sanity check"
      ],
      "metadata": {
        "id": "3P0TQ6VzcAcg",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 411
        },
        "outputId": "9ad0387c-2f61-45bc-ad21-e6aad7416362"
      },
      "execution_count": 276,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "          ID_Number     Radius1    Texture1  Perimeter1        Area1  \\\n",
              "count  5.680000e+02  568.000000  568.000000  568.000000   568.000000   \n",
              "mean   3.042382e+07   14.120491   19.305335   91.914754   654.279754   \n",
              "std    1.251246e+08    3.523416    4.288506   24.285848   351.923751   \n",
              "min    8.670000e+03    6.981000    9.710000   43.790000   143.500000   \n",
              "25%    8.692225e+05   11.697500   16.177500   75.135000   420.175000   \n",
              "50%    9.061570e+05   13.355000   18.855000   86.210000   548.750000   \n",
              "75%    8.825022e+06   15.780000   21.802500  103.875000   782.625000   \n",
              "max    9.113205e+08   28.110000   39.280000  188.500000  2501.000000   \n",
              "\n",
              "       Smoothness1  Compactness1  Concavity1  Concave Points1   Symmetry1  \\\n",
              "count   568.000000    568.000000  568.000000       568.000000  568.000000   \n",
              "mean      0.096321      0.104036    0.088427         0.048746    0.181055   \n",
              "std       0.014046      0.052355    0.079294         0.038617    0.027319   \n",
              "min       0.052630      0.019380    0.000000         0.000000    0.106000   \n",
              "25%       0.086290      0.064815    0.029540         0.020310    0.161900   \n",
              "50%       0.095865      0.092525    0.061400         0.033455    0.179200   \n",
              "75%       0.105300      0.130400    0.129650         0.073730    0.195625   \n",
              "max       0.163400      0.345400    0.426800         0.201200    0.304000   \n",
              "\n",
              "       ...    Radius3    Texture3  Perimeter3        Area3  Smoothness3  \\\n",
              "count  ...  568.00000  568.000000  568.000000   568.000000   568.000000   \n",
              "mean   ...   16.25315   25.691919  107.125053   878.578873     0.132316   \n",
              "std    ...    4.82232    6.141662   33.474687   567.846267     0.022818   \n",
              "min    ...    7.93000   12.020000   50.410000   185.200000     0.071170   \n",
              "25%    ...   13.01000   21.095000   84.102500   514.975000     0.116600   \n",
              "50%    ...   14.96500   25.425000   97.655000   685.550000     0.131300   \n",
              "75%    ...   18.76750   29.757500  125.175000  1073.500000     0.146000   \n",
              "max    ...   36.04000   49.540000  251.200000  4254.000000     0.222600   \n",
              "\n",
              "       Compactness3  Concavity3  Concave Points3   Symmetry3  \\\n",
              "count    568.000000  568.000000       568.000000  568.000000   \n",
              "mean       0.253541    0.271414         0.114341    0.289776   \n",
              "std        0.156523    0.207989         0.065484    0.061508   \n",
              "min        0.027290    0.000000         0.000000    0.156500   \n",
              "25%        0.146900    0.114475         0.064730    0.250350   \n",
              "50%        0.211850    0.226550         0.099840    0.282050   \n",
              "75%        0.337600    0.381400         0.161325    0.317675   \n",
              "max        1.058000    1.252000         0.291000    0.663800   \n",
              "\n",
              "       Fractal Dimension3  \n",
              "count          568.000000  \n",
              "mean             0.083884  \n",
              "std              0.018017  \n",
              "min              0.055040  \n",
              "25%              0.071412  \n",
              "50%              0.080015  \n",
              "75%              0.092065  \n",
              "max              0.207500  \n",
              "\n",
              "[8 rows x 31 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-b2bc4e44-b099-4617-b8a6-c3b8869f88e8\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ID_Number</th>\n",
              "      <th>Radius1</th>\n",
              "      <th>Texture1</th>\n",
              "      <th>Perimeter1</th>\n",
              "      <th>Area1</th>\n",
              "      <th>Smoothness1</th>\n",
              "      <th>Compactness1</th>\n",
              "      <th>Concavity1</th>\n",
              "      <th>Concave Points1</th>\n",
              "      <th>Symmetry1</th>\n",
              "      <th>...</th>\n",
              "      <th>Radius3</th>\n",
              "      <th>Texture3</th>\n",
              "      <th>Perimeter3</th>\n",
              "      <th>Area3</th>\n",
              "      <th>Smoothness3</th>\n",
              "      <th>Compactness3</th>\n",
              "      <th>Concavity3</th>\n",
              "      <th>Concave Points3</th>\n",
              "      <th>Symmetry3</th>\n",
              "      <th>Fractal Dimension3</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>5.680000e+02</td>\n",
              "      <td>568.000000</td>\n",
              "      <td>568.000000</td>\n",
              "      <td>568.000000</td>\n",
              "      <td>568.000000</td>\n",
              "      <td>568.000000</td>\n",
              "      <td>568.000000</td>\n",
              "      <td>568.000000</td>\n",
              "      <td>568.000000</td>\n",
              "      <td>568.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>568.00000</td>\n",
              "      <td>568.000000</td>\n",
              "      <td>568.000000</td>\n",
              "      <td>568.000000</td>\n",
              "      <td>568.000000</td>\n",
              "      <td>568.000000</td>\n",
              "      <td>568.000000</td>\n",
              "      <td>568.000000</td>\n",
              "      <td>568.000000</td>\n",
              "      <td>568.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>3.042382e+07</td>\n",
              "      <td>14.120491</td>\n",
              "      <td>19.305335</td>\n",
              "      <td>91.914754</td>\n",
              "      <td>654.279754</td>\n",
              "      <td>0.096321</td>\n",
              "      <td>0.104036</td>\n",
              "      <td>0.088427</td>\n",
              "      <td>0.048746</td>\n",
              "      <td>0.181055</td>\n",
              "      <td>...</td>\n",
              "      <td>16.25315</td>\n",
              "      <td>25.691919</td>\n",
              "      <td>107.125053</td>\n",
              "      <td>878.578873</td>\n",
              "      <td>0.132316</td>\n",
              "      <td>0.253541</td>\n",
              "      <td>0.271414</td>\n",
              "      <td>0.114341</td>\n",
              "      <td>0.289776</td>\n",
              "      <td>0.083884</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>1.251246e+08</td>\n",
              "      <td>3.523416</td>\n",
              "      <td>4.288506</td>\n",
              "      <td>24.285848</td>\n",
              "      <td>351.923751</td>\n",
              "      <td>0.014046</td>\n",
              "      <td>0.052355</td>\n",
              "      <td>0.079294</td>\n",
              "      <td>0.038617</td>\n",
              "      <td>0.027319</td>\n",
              "      <td>...</td>\n",
              "      <td>4.82232</td>\n",
              "      <td>6.141662</td>\n",
              "      <td>33.474687</td>\n",
              "      <td>567.846267</td>\n",
              "      <td>0.022818</td>\n",
              "      <td>0.156523</td>\n",
              "      <td>0.207989</td>\n",
              "      <td>0.065484</td>\n",
              "      <td>0.061508</td>\n",
              "      <td>0.018017</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>8.670000e+03</td>\n",
              "      <td>6.981000</td>\n",
              "      <td>9.710000</td>\n",
              "      <td>43.790000</td>\n",
              "      <td>143.500000</td>\n",
              "      <td>0.052630</td>\n",
              "      <td>0.019380</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.106000</td>\n",
              "      <td>...</td>\n",
              "      <td>7.93000</td>\n",
              "      <td>12.020000</td>\n",
              "      <td>50.410000</td>\n",
              "      <td>185.200000</td>\n",
              "      <td>0.071170</td>\n",
              "      <td>0.027290</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.156500</td>\n",
              "      <td>0.055040</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>8.692225e+05</td>\n",
              "      <td>11.697500</td>\n",
              "      <td>16.177500</td>\n",
              "      <td>75.135000</td>\n",
              "      <td>420.175000</td>\n",
              "      <td>0.086290</td>\n",
              "      <td>0.064815</td>\n",
              "      <td>0.029540</td>\n",
              "      <td>0.020310</td>\n",
              "      <td>0.161900</td>\n",
              "      <td>...</td>\n",
              "      <td>13.01000</td>\n",
              "      <td>21.095000</td>\n",
              "      <td>84.102500</td>\n",
              "      <td>514.975000</td>\n",
              "      <td>0.116600</td>\n",
              "      <td>0.146900</td>\n",
              "      <td>0.114475</td>\n",
              "      <td>0.064730</td>\n",
              "      <td>0.250350</td>\n",
              "      <td>0.071412</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>9.061570e+05</td>\n",
              "      <td>13.355000</td>\n",
              "      <td>18.855000</td>\n",
              "      <td>86.210000</td>\n",
              "      <td>548.750000</td>\n",
              "      <td>0.095865</td>\n",
              "      <td>0.092525</td>\n",
              "      <td>0.061400</td>\n",
              "      <td>0.033455</td>\n",
              "      <td>0.179200</td>\n",
              "      <td>...</td>\n",
              "      <td>14.96500</td>\n",
              "      <td>25.425000</td>\n",
              "      <td>97.655000</td>\n",
              "      <td>685.550000</td>\n",
              "      <td>0.131300</td>\n",
              "      <td>0.211850</td>\n",
              "      <td>0.226550</td>\n",
              "      <td>0.099840</td>\n",
              "      <td>0.282050</td>\n",
              "      <td>0.080015</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>8.825022e+06</td>\n",
              "      <td>15.780000</td>\n",
              "      <td>21.802500</td>\n",
              "      <td>103.875000</td>\n",
              "      <td>782.625000</td>\n",
              "      <td>0.105300</td>\n",
              "      <td>0.130400</td>\n",
              "      <td>0.129650</td>\n",
              "      <td>0.073730</td>\n",
              "      <td>0.195625</td>\n",
              "      <td>...</td>\n",
              "      <td>18.76750</td>\n",
              "      <td>29.757500</td>\n",
              "      <td>125.175000</td>\n",
              "      <td>1073.500000</td>\n",
              "      <td>0.146000</td>\n",
              "      <td>0.337600</td>\n",
              "      <td>0.381400</td>\n",
              "      <td>0.161325</td>\n",
              "      <td>0.317675</td>\n",
              "      <td>0.092065</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>9.113205e+08</td>\n",
              "      <td>28.110000</td>\n",
              "      <td>39.280000</td>\n",
              "      <td>188.500000</td>\n",
              "      <td>2501.000000</td>\n",
              "      <td>0.163400</td>\n",
              "      <td>0.345400</td>\n",
              "      <td>0.426800</td>\n",
              "      <td>0.201200</td>\n",
              "      <td>0.304000</td>\n",
              "      <td>...</td>\n",
              "      <td>36.04000</td>\n",
              "      <td>49.540000</td>\n",
              "      <td>251.200000</td>\n",
              "      <td>4254.000000</td>\n",
              "      <td>0.222600</td>\n",
              "      <td>1.058000</td>\n",
              "      <td>1.252000</td>\n",
              "      <td>0.291000</td>\n",
              "      <td>0.663800</td>\n",
              "      <td>0.207500</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>8 rows Ã— 31 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-b2bc4e44-b099-4617-b8a6-c3b8869f88e8')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-b2bc4e44-b099-4617-b8a6-c3b8869f88e8 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-b2bc4e44-b099-4617-b8a6-c3b8869f88e8');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 276
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data.info()     # check data types "
      ],
      "metadata": {
        "id": "0cSiiGTtcCGM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0e19882d-6d9c-41a7-ae81-532624e14269"
      },
      "execution_count": 277,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 568 entries, 0 to 567\n",
            "Data columns (total 32 columns):\n",
            " #   Column              Non-Null Count  Dtype  \n",
            "---  ------              --------------  -----  \n",
            " 0   ID_Number           568 non-null    int64  \n",
            " 1   Diagnosis           568 non-null    object \n",
            " 2   Radius1             568 non-null    float64\n",
            " 3   Texture1            568 non-null    float64\n",
            " 4   Perimeter1          568 non-null    float64\n",
            " 5   Area1               568 non-null    float64\n",
            " 6   Smoothness1         568 non-null    float64\n",
            " 7   Compactness1        568 non-null    float64\n",
            " 8   Concavity1          568 non-null    float64\n",
            " 9   Concave Points1     568 non-null    float64\n",
            " 10  Symmetry1           568 non-null    float64\n",
            " 11  Fractal Dimension1  568 non-null    float64\n",
            " 12  Radius2             568 non-null    float64\n",
            " 13  Texture2            568 non-null    float64\n",
            " 14  Perimeter2          568 non-null    float64\n",
            " 15  Area2               568 non-null    float64\n",
            " 16  Smoothness2         568 non-null    float64\n",
            " 17  Compactness2        568 non-null    float64\n",
            " 18  Concavity2          568 non-null    float64\n",
            " 19  Concave Points2     568 non-null    float64\n",
            " 20  Symmetry2           568 non-null    float64\n",
            " 21  Fractal Dimension2  568 non-null    float64\n",
            " 22  Radius3             568 non-null    float64\n",
            " 23  Texture3            568 non-null    float64\n",
            " 24  Perimeter3          568 non-null    float64\n",
            " 25  Area3               568 non-null    float64\n",
            " 26  Smoothness3         568 non-null    float64\n",
            " 27  Compactness3        568 non-null    float64\n",
            " 28  Concavity3          568 non-null    float64\n",
            " 29  Concave Points3     568 non-null    float64\n",
            " 30  Symmetry3           568 non-null    float64\n",
            " 31  Fractal Dimension3  568 non-null    float64\n",
            "dtypes: float64(30), int64(1), object(1)\n",
            "memory usage: 142.1+ KB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data[\"Diagnosis\"] = np.where(data[\"Diagnosis\"] == \"M\", 1, 0)            # change the Ms to 1s and Bs to 0s \n",
        "Y = np.array(data[[\"Diagnosis\"]])                                       # Y is the array of expected outcomes \n",
        "print(Y)                                                                # sanity check\n",
        "\n",
        "#data[\"DiagnosisBinary\"] = np.where(data[\"Diagnosis\"] == 1, 0, 1)       # add another row opposite to Y for binary output neurons \n",
        "#YBinary = np.array(data[[\"DiagnosisBinary\"]])\n",
        "\n",
        "# X is the array of inputs for the NN\n",
        "X = np.array(data[[\"Radius1\",\"Texture1\",\"Perimeter1\",\"Area1\",\"Smoothness1\",\"Compactness1\",\"Concavity1\",\"Concave Points1\",\"Symmetry1\",\"Fractal Dimension1\",\"Radius2\",\"Texture2\",\"Perimeter2\",\"Area2\",\"Smoothness2\",\"Compactness2\",\"Concavity2\",\"Concave Points2\",\"Symmetry2\",\"Fractal Dimension2\",\"Radius3\",\"Texture3\",\"Perimeter3\",\"Area3\",\"Smoothness3\",\"Compactness3\",\"Concavity3\",\"Concave Points3\",\"Symmetry3\",\"Fractal Dimension3\"]]) # X is the input data (does not include the outcome)\n",
        "print(X)                                                                # sanity check"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vJ0X5ZUIawNH",
        "outputId": "6f021b72-cc98-49d6-e7d3-1c45aa5323cd"
      },
      "execution_count": 278,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [0]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [0]\n",
            " [1]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [1]\n",
            " [1]\n",
            " [0]\n",
            " [1]\n",
            " [1]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [1]\n",
            " [0]\n",
            " [1]\n",
            " [1]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [1]\n",
            " [0]\n",
            " [1]\n",
            " [1]\n",
            " [0]\n",
            " [1]\n",
            " [0]\n",
            " [1]\n",
            " [1]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [1]\n",
            " [1]\n",
            " [0]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [1]\n",
            " [0]\n",
            " [0]\n",
            " [1]\n",
            " [1]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [1]\n",
            " [1]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [1]\n",
            " [0]\n",
            " [0]\n",
            " [1]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [0]\n",
            " [1]\n",
            " [1]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [1]\n",
            " [1]\n",
            " [0]\n",
            " [1]\n",
            " [0]\n",
            " [1]\n",
            " [1]\n",
            " [0]\n",
            " [1]\n",
            " [1]\n",
            " [0]\n",
            " [0]\n",
            " [1]\n",
            " [0]\n",
            " [0]\n",
            " [1]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [1]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [1]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [1]\n",
            " [1]\n",
            " [0]\n",
            " [1]\n",
            " [0]\n",
            " [0]\n",
            " [1]\n",
            " [1]\n",
            " [0]\n",
            " [0]\n",
            " [1]\n",
            " [1]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [1]\n",
            " [0]\n",
            " [0]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [0]\n",
            " [1]\n",
            " [0]\n",
            " [1]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [1]\n",
            " [0]\n",
            " [0]\n",
            " [1]\n",
            " [1]\n",
            " [0]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [0]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [0]\n",
            " [1]\n",
            " [0]\n",
            " [1]\n",
            " [0]\n",
            " [0]\n",
            " [1]\n",
            " [0]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [0]\n",
            " [0]\n",
            " [1]\n",
            " [1]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [1]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [1]\n",
            " [1]\n",
            " [0]\n",
            " [0]\n",
            " [1]\n",
            " [0]\n",
            " [0]\n",
            " [1]\n",
            " [1]\n",
            " [0]\n",
            " [1]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [1]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [1]\n",
            " [0]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [1]\n",
            " [0]\n",
            " [1]\n",
            " [0]\n",
            " [0]\n",
            " [1]\n",
            " [0]\n",
            " [0]\n",
            " [1]\n",
            " [0]\n",
            " [1]\n",
            " [1]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [1]\n",
            " [0]\n",
            " [0]\n",
            " [1]\n",
            " [0]\n",
            " [1]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [1]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [1]\n",
            " [0]\n",
            " [1]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [1]\n",
            " [0]\n",
            " [1]\n",
            " [0]\n",
            " [1]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [1]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [1]\n",
            " [1]\n",
            " [0]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [0]\n",
            " [1]\n",
            " [1]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [1]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [1]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [1]\n",
            " [0]\n",
            " [0]\n",
            " [1]\n",
            " [1]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [1]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [1]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [1]\n",
            " [0]\n",
            " [0]\n",
            " [1]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [1]\n",
            " [0]\n",
            " [1]\n",
            " [1]\n",
            " [0]\n",
            " [1]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [1]\n",
            " [0]\n",
            " [0]\n",
            " [1]\n",
            " [0]\n",
            " [1]\n",
            " [0]\n",
            " [0]\n",
            " [1]\n",
            " [0]\n",
            " [1]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [1]\n",
            " [1]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [1]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [1]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [1]\n",
            " [0]\n",
            " [1]\n",
            " [0]\n",
            " [0]\n",
            " [1]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [1]\n",
            " [1]\n",
            " [0]\n",
            " [1]\n",
            " [0]\n",
            " [1]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [1]\n",
            " [0]\n",
            " [0]\n",
            " [1]\n",
            " [0]\n",
            " [1]\n",
            " [0]\n",
            " [1]\n",
            " [1]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [1]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [1]\n",
            " [0]\n",
            " [1]\n",
            " [1]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [0]]\n",
            "[[2.057e+01 1.777e+01 1.329e+02 ... 1.860e-01 2.750e-01 8.902e-02]\n",
            " [1.969e+01 2.125e+01 1.300e+02 ... 2.430e-01 3.613e-01 8.758e-02]\n",
            " [1.142e+01 2.038e+01 7.758e+01 ... 2.575e-01 6.638e-01 1.730e-01]\n",
            " ...\n",
            " [1.660e+01 2.808e+01 1.083e+02 ... 1.418e-01 2.218e-01 7.820e-02]\n",
            " [2.060e+01 2.933e+01 1.401e+02 ... 2.650e-01 4.087e-01 1.240e-01]\n",
            " [7.760e+00 2.454e+01 4.792e+01 ... 0.000e+00 2.871e-01 7.039e-02]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalise the inputs (X)\n",
        "\n",
        "XNorm = (X - np.amin(X)) / (np.amax(X) - np.amin(X))          # perform min-max normalisation on the data to avoid skewing the NN\n",
        "print(XNorm)  # Sanity check\n",
        "X = XNorm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X-MpPEW8OV1Q",
        "outputId": "2c684e87-9b92-4648-b6df-69048d62ce61"
      },
      "execution_count": 279,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[4.83544899e-03 4.17724495e-03 3.12411848e-02 ... 4.37235543e-05\n",
            "  6.46450400e-05 2.09261871e-05]\n",
            " [4.62858486e-03 4.99529854e-03 3.05594734e-02 ... 5.71227080e-05\n",
            "  8.49318289e-05 2.05876822e-05]\n",
            " [2.68453220e-03 4.79078514e-03 1.82369535e-02 ... 6.05312647e-05\n",
            "  1.56041373e-04 4.06676070e-05]\n",
            " ...\n",
            " [3.90220969e-03 6.60084626e-03 2.54583921e-02 ... 3.33333333e-05\n",
            "  5.21391631e-05 1.83826986e-05]\n",
            " [4.84250118e-03 6.89468735e-03 3.29337094e-02 ... 6.22943112e-05\n",
            "  9.60742830e-05 2.91490362e-05]\n",
            " [1.82416549e-03 5.76868829e-03 1.12646921e-02 ... 0.00000000e+00\n",
            "  6.74894217e-05 1.65467795e-05]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def answerFn(out):                  # used to calculate the classification accuracy of the neural network\n",
        "  p = np.zeros([len(x_train)])\n",
        "  for i in range(len(x_train)):\n",
        "    if out[i] < 0.5:\n",
        "      p[i] = 0\n",
        "    else:\n",
        "      p[i] = 1\n",
        "  return p"
      ],
      "metadata": {
        "id": "LQcv9-kPgKgA"
      },
      "execution_count": 280,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Main"
      ],
      "metadata": {
        "id": "II5G6ueqcDuS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#### FROM SOURCE - COMMENTS AND CHANGES ARE MINE ####\n",
        "\n",
        "# Main\n",
        "\n",
        "## MY CHANGES ##\n",
        "x_train = X.reshape(len(X), len(X[0]), 1)               # reshape the 2D array into a 3D array where each input row in the data set is one element in a vector of the size of the number of input rows \n",
        "y_train = Y.reshape(len(Y), 1)                          # reshape the expected ouputs \n",
        "\n",
        "startTimeTotal = time.time()                            # start time for building the network\n",
        "\n",
        "## FROM SOURCE\n",
        "net = Network()                                         # build, train and run the neural network\n",
        "\n",
        "# add the layers with number of neurons and activation function (and derivative) to the network\n",
        "\n",
        "## INSTUCTIONS \n",
        "## Must add both a Hidden and Activation class together\n",
        "## The first hidden layer must have the number of input neurons equal to the number of coulmns in the input data\n",
        "## Each following hidden layer must have number of input neurons  equal to the number of output neurons of the previos hidden layer\n",
        "## The final hidden layer must have only 1 outout neuron \n",
        "## Each activation layer with activation function must have the corrisponding derivative activation function\n",
        "\n",
        "## FROM SOURCE\n",
        "net.add(Hidden(len(x_train[0]), 120))\n",
        "net.add(Activation(tanh, tanhDerv))\n",
        "net.add(Hidden(120, 5))\n",
        "net.add(Activation(tanh, tanhDerv))\n",
        "net.add(Hidden(5, 1))\n",
        "net.add(Activation(tanh, tanhDerv))\n",
        "\n",
        "## FROM SOURCE\n",
        "net.use(mse, mseDerv)               #specify the loss function \n",
        "\n",
        "## MY CHANGES\n",
        "startTimeTrain = time.time()        # time the training of the NN\n",
        "\n",
        "# train the NN and specify the no. epochs, learning rate, gradient descent method \n",
        "# note min batch has an adjustable batch size \n",
        "\n",
        "epochs = 1000                       # number of epochs \n",
        "\n",
        "# gradient descent methods \n",
        "errorStore = net.fitSGD(x_train, y_train, epochs, learning_rate = 0.00001)\n",
        "#errorStore = net.fitBatch(x_train, y_train, epochs, learning_rate = 0.00001)\n",
        "#errorStore = net.fitMinBatch(x_train, y_train, epochs, learning_rate = 0.001, batchSize = 50)\n",
        "\n",
        "\n",
        "endTimeTrain = time.time()                # time it takes to train NN\n",
        "print(\"last error is \", errorStore[-1])   # print the last error from the training\n",
        "\n",
        "# Plot the error vs the epochs \n",
        "xPlot = np.array(range(0, epochs))     \n",
        "yPlot = errorStore\n",
        "plt.title(\"Plotting Error vs Epochs\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Error\")\n",
        "plt.plot(xPlot, yPlot, color = \"red\")\n",
        "#plt.legend()\n",
        "plt.show()\n",
        "\n",
        "startTimePredict = time.time()  #time the predictions \n",
        "\n",
        "## FROM SOURCE\n",
        "out = net.predict(x_train)      # get the predictions from the NN\n",
        "\n",
        "## MY CHANGES \n",
        "endTimePredict = time.time()    # time taken to predict\n",
        "\n",
        "pred = answerFn(out)\n",
        "expt = y_train.reshape(len(y_train)) * 1.0\n",
        "\n",
        "# Accuracy calculation \n",
        "\n",
        "TP = 0  # True Positives\n",
        "FP = 0  # False Positives\n",
        "FN = 0  # False Negatives\n",
        "TN = 0  # True Negatives\n",
        "\n",
        "for i in range(len(pred)):\n",
        "  if pred[i] == 1:\n",
        "    if expt[i] == 1:\n",
        "      TP += 1\n",
        "    else:\n",
        "      FP += 1\n",
        "  elif pred[i] == 0:\n",
        "    if expt[i] == 0:\n",
        "      TN += 1\n",
        "    else:\n",
        "      FN += 1\n",
        "\n",
        "accuracy = (TP + TN)/(TP+TN+FP+FN)      # calculate the classification accuracy \n",
        "\n",
        "print(\"The classification accuracy is \", accuracy*100, \"%\") # print classification accuracy \n",
        "endTimeTotal = time.time()\n",
        "\n",
        "# print timing for NN\n",
        "print(\"Total time = \", endTimeTotal - startTimeTotal)\n",
        "print(\"Train time = \", endTimeTrain - startTimeTrain)\n",
        "print(\"Predict time = \", endTimePredict - startTimePredict)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "PG3Fd1zdaUVD",
        "outputId": "92bd9e9b-e8e3-4452-916b-676e61201083"
      },
      "execution_count": 281,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The error at epoch 1/1000 is error=1.307627\n",
            "The error at epoch 2/1000 is error=1.274995\n",
            "The error at epoch 3/1000 is error=1.241207\n",
            "The error at epoch 4/1000 is error=1.206329\n",
            "The error at epoch 5/1000 is error=1.170449\n",
            "The error at epoch 6/1000 is error=1.133676\n",
            "The error at epoch 7/1000 is error=1.096142\n",
            "The error at epoch 8/1000 is error=1.058000\n",
            "The error at epoch 9/1000 is error=1.019423\n",
            "The error at epoch 10/1000 is error=0.980598\n",
            "The error at epoch 11/1000 is error=0.941729\n",
            "The error at epoch 12/1000 is error=0.903026\n",
            "The error at epoch 13/1000 is error=0.864703\n",
            "The error at epoch 14/1000 is error=0.826970\n",
            "The error at epoch 15/1000 is error=0.790030\n",
            "The error at epoch 16/1000 is error=0.754073\n",
            "The error at epoch 17/1000 is error=0.719268\n",
            "The error at epoch 18/1000 is error=0.685763\n",
            "The error at epoch 19/1000 is error=0.653680\n",
            "The error at epoch 20/1000 is error=0.623114\n",
            "The error at epoch 21/1000 is error=0.594134\n",
            "The error at epoch 22/1000 is error=0.566782\n",
            "The error at epoch 23/1000 is error=0.541074\n",
            "The error at epoch 24/1000 is error=0.517004\n",
            "The error at epoch 25/1000 is error=0.494548\n",
            "The error at epoch 26/1000 is error=0.473664\n",
            "The error at epoch 27/1000 is error=0.454296\n",
            "The error at epoch 28/1000 is error=0.436381\n",
            "The error at epoch 29/1000 is error=0.419845\n",
            "The error at epoch 30/1000 is error=0.404612\n",
            "The error at epoch 31/1000 is error=0.390601\n",
            "The error at epoch 32/1000 is error=0.377733\n",
            "The error at epoch 33/1000 is error=0.365927\n",
            "The error at epoch 34/1000 is error=0.355105\n",
            "The error at epoch 35/1000 is error=0.345194\n",
            "The error at epoch 36/1000 is error=0.336120\n",
            "The error at epoch 37/1000 is error=0.327817\n",
            "The error at epoch 38/1000 is error=0.320222\n",
            "The error at epoch 39/1000 is error=0.313273\n",
            "The error at epoch 40/1000 is error=0.306918\n",
            "The error at epoch 41/1000 is error=0.301104\n",
            "The error at epoch 42/1000 is error=0.295784\n",
            "The error at epoch 43/1000 is error=0.290916\n",
            "The error at epoch 44/1000 is error=0.286459\n",
            "The error at epoch 45/1000 is error=0.282378\n",
            "The error at epoch 46/1000 is error=0.278639\n",
            "The error at epoch 47/1000 is error=0.275212\n",
            "The error at epoch 48/1000 is error=0.272069\n",
            "The error at epoch 49/1000 is error=0.269186\n",
            "The error at epoch 50/1000 is error=0.266539\n",
            "The error at epoch 51/1000 is error=0.264108\n",
            "The error at epoch 52/1000 is error=0.261874\n",
            "The error at epoch 53/1000 is error=0.259820\n",
            "The error at epoch 54/1000 is error=0.257930\n",
            "The error at epoch 55/1000 is error=0.256189\n",
            "The error at epoch 56/1000 is error=0.254586\n",
            "The error at epoch 57/1000 is error=0.253109\n",
            "The error at epoch 58/1000 is error=0.251745\n",
            "The error at epoch 59/1000 is error=0.250487\n",
            "The error at epoch 60/1000 is error=0.249325\n",
            "The error at epoch 61/1000 is error=0.248251\n",
            "The error at epoch 62/1000 is error=0.247257\n",
            "The error at epoch 63/1000 is error=0.246338\n",
            "The error at epoch 64/1000 is error=0.245486\n",
            "The error at epoch 65/1000 is error=0.244697\n",
            "The error at epoch 66/1000 is error=0.243965\n",
            "The error at epoch 67/1000 is error=0.243286\n",
            "The error at epoch 68/1000 is error=0.242655\n",
            "The error at epoch 69/1000 is error=0.242068\n",
            "The error at epoch 70/1000 is error=0.241523\n",
            "The error at epoch 71/1000 is error=0.241015\n",
            "The error at epoch 72/1000 is error=0.240542\n",
            "The error at epoch 73/1000 is error=0.240101\n",
            "The error at epoch 74/1000 is error=0.239690\n",
            "The error at epoch 75/1000 is error=0.239305\n",
            "The error at epoch 76/1000 is error=0.238946\n",
            "The error at epoch 77/1000 is error=0.238610\n",
            "The error at epoch 78/1000 is error=0.238295\n",
            "The error at epoch 79/1000 is error=0.238000\n",
            "The error at epoch 80/1000 is error=0.237724\n",
            "The error at epoch 81/1000 is error=0.237464\n",
            "The error at epoch 82/1000 is error=0.237220\n",
            "The error at epoch 83/1000 is error=0.236990\n",
            "The error at epoch 84/1000 is error=0.236774\n",
            "The error at epoch 85/1000 is error=0.236570\n",
            "The error at epoch 86/1000 is error=0.236378\n",
            "The error at epoch 87/1000 is error=0.236196\n",
            "The error at epoch 88/1000 is error=0.236024\n",
            "The error at epoch 89/1000 is error=0.235861\n",
            "The error at epoch 90/1000 is error=0.235707\n",
            "The error at epoch 91/1000 is error=0.235560\n",
            "The error at epoch 92/1000 is error=0.235421\n",
            "The error at epoch 93/1000 is error=0.235289\n",
            "The error at epoch 94/1000 is error=0.235162\n",
            "The error at epoch 95/1000 is error=0.235042\n",
            "The error at epoch 96/1000 is error=0.234927\n",
            "The error at epoch 97/1000 is error=0.234817\n",
            "The error at epoch 98/1000 is error=0.234712\n",
            "The error at epoch 99/1000 is error=0.234611\n",
            "The error at epoch 100/1000 is error=0.234514\n",
            "The error at epoch 101/1000 is error=0.234420\n",
            "The error at epoch 102/1000 is error=0.234331\n",
            "The error at epoch 103/1000 is error=0.234244\n",
            "The error at epoch 104/1000 is error=0.234160\n",
            "The error at epoch 105/1000 is error=0.234080\n",
            "The error at epoch 106/1000 is error=0.234002\n",
            "The error at epoch 107/1000 is error=0.233926\n",
            "The error at epoch 108/1000 is error=0.233852\n",
            "The error at epoch 109/1000 is error=0.233781\n",
            "The error at epoch 110/1000 is error=0.233711\n",
            "The error at epoch 111/1000 is error=0.233644\n",
            "The error at epoch 112/1000 is error=0.233578\n",
            "The error at epoch 113/1000 is error=0.233513\n",
            "The error at epoch 114/1000 is error=0.233450\n",
            "The error at epoch 115/1000 is error=0.233389\n",
            "The error at epoch 116/1000 is error=0.233328\n",
            "The error at epoch 117/1000 is error=0.233269\n",
            "The error at epoch 118/1000 is error=0.233211\n",
            "The error at epoch 119/1000 is error=0.233154\n",
            "The error at epoch 120/1000 is error=0.233098\n",
            "The error at epoch 121/1000 is error=0.233042\n",
            "The error at epoch 122/1000 is error=0.232988\n",
            "The error at epoch 123/1000 is error=0.232934\n",
            "The error at epoch 124/1000 is error=0.232881\n",
            "The error at epoch 125/1000 is error=0.232829\n",
            "The error at epoch 126/1000 is error=0.232777\n",
            "The error at epoch 127/1000 is error=0.232726\n",
            "The error at epoch 128/1000 is error=0.232675\n",
            "The error at epoch 129/1000 is error=0.232625\n",
            "The error at epoch 130/1000 is error=0.232575\n",
            "The error at epoch 131/1000 is error=0.232526\n",
            "The error at epoch 132/1000 is error=0.232477\n",
            "The error at epoch 133/1000 is error=0.232429\n",
            "The error at epoch 134/1000 is error=0.232380\n",
            "The error at epoch 135/1000 is error=0.232333\n",
            "The error at epoch 136/1000 is error=0.232285\n",
            "The error at epoch 137/1000 is error=0.232238\n",
            "The error at epoch 138/1000 is error=0.232190\n",
            "The error at epoch 139/1000 is error=0.232144\n",
            "The error at epoch 140/1000 is error=0.232097\n",
            "The error at epoch 141/1000 is error=0.232050\n",
            "The error at epoch 142/1000 is error=0.232004\n",
            "The error at epoch 143/1000 is error=0.231958\n",
            "The error at epoch 144/1000 is error=0.231912\n",
            "The error at epoch 145/1000 is error=0.231866\n",
            "The error at epoch 146/1000 is error=0.231820\n",
            "The error at epoch 147/1000 is error=0.231775\n",
            "The error at epoch 148/1000 is error=0.231729\n",
            "The error at epoch 149/1000 is error=0.231684\n",
            "The error at epoch 150/1000 is error=0.231638\n",
            "The error at epoch 151/1000 is error=0.231593\n",
            "The error at epoch 152/1000 is error=0.231548\n",
            "The error at epoch 153/1000 is error=0.231503\n",
            "The error at epoch 154/1000 is error=0.231457\n",
            "The error at epoch 155/1000 is error=0.231412\n",
            "The error at epoch 156/1000 is error=0.231367\n",
            "The error at epoch 157/1000 is error=0.231322\n",
            "The error at epoch 158/1000 is error=0.231277\n",
            "The error at epoch 159/1000 is error=0.231232\n",
            "The error at epoch 160/1000 is error=0.231187\n",
            "The error at epoch 161/1000 is error=0.231142\n",
            "The error at epoch 162/1000 is error=0.231097\n",
            "The error at epoch 163/1000 is error=0.231052\n",
            "The error at epoch 164/1000 is error=0.231007\n",
            "The error at epoch 165/1000 is error=0.230962\n",
            "The error at epoch 166/1000 is error=0.230917\n",
            "The error at epoch 167/1000 is error=0.230872\n",
            "The error at epoch 168/1000 is error=0.230827\n",
            "The error at epoch 169/1000 is error=0.230782\n",
            "The error at epoch 170/1000 is error=0.230737\n",
            "The error at epoch 171/1000 is error=0.230692\n",
            "The error at epoch 172/1000 is error=0.230647\n",
            "The error at epoch 173/1000 is error=0.230601\n",
            "The error at epoch 174/1000 is error=0.230556\n",
            "The error at epoch 175/1000 is error=0.230511\n",
            "The error at epoch 176/1000 is error=0.230466\n",
            "The error at epoch 177/1000 is error=0.230420\n",
            "The error at epoch 178/1000 is error=0.230375\n",
            "The error at epoch 179/1000 is error=0.230329\n",
            "The error at epoch 180/1000 is error=0.230284\n",
            "The error at epoch 181/1000 is error=0.230238\n",
            "The error at epoch 182/1000 is error=0.230193\n",
            "The error at epoch 183/1000 is error=0.230147\n",
            "The error at epoch 184/1000 is error=0.230102\n",
            "The error at epoch 185/1000 is error=0.230056\n",
            "The error at epoch 186/1000 is error=0.230010\n",
            "The error at epoch 187/1000 is error=0.229964\n",
            "The error at epoch 188/1000 is error=0.229918\n",
            "The error at epoch 189/1000 is error=0.229872\n",
            "The error at epoch 190/1000 is error=0.229826\n",
            "The error at epoch 191/1000 is error=0.229780\n",
            "The error at epoch 192/1000 is error=0.229734\n",
            "The error at epoch 193/1000 is error=0.229688\n",
            "The error at epoch 194/1000 is error=0.229641\n",
            "The error at epoch 195/1000 is error=0.229595\n",
            "The error at epoch 196/1000 is error=0.229549\n",
            "The error at epoch 197/1000 is error=0.229502\n",
            "The error at epoch 198/1000 is error=0.229456\n",
            "The error at epoch 199/1000 is error=0.229409\n",
            "The error at epoch 200/1000 is error=0.229362\n",
            "The error at epoch 201/1000 is error=0.229316\n",
            "The error at epoch 202/1000 is error=0.229269\n",
            "The error at epoch 203/1000 is error=0.229222\n",
            "The error at epoch 204/1000 is error=0.229175\n",
            "The error at epoch 205/1000 is error=0.229128\n",
            "The error at epoch 206/1000 is error=0.229081\n",
            "The error at epoch 207/1000 is error=0.229033\n",
            "The error at epoch 208/1000 is error=0.228986\n",
            "The error at epoch 209/1000 is error=0.228939\n",
            "The error at epoch 210/1000 is error=0.228891\n",
            "The error at epoch 211/1000 is error=0.228844\n",
            "The error at epoch 212/1000 is error=0.228796\n",
            "The error at epoch 213/1000 is error=0.228748\n",
            "The error at epoch 214/1000 is error=0.228701\n",
            "The error at epoch 215/1000 is error=0.228653\n",
            "The error at epoch 216/1000 is error=0.228605\n",
            "The error at epoch 217/1000 is error=0.228557\n",
            "The error at epoch 218/1000 is error=0.228509\n",
            "The error at epoch 219/1000 is error=0.228460\n",
            "The error at epoch 220/1000 is error=0.228412\n",
            "The error at epoch 221/1000 is error=0.228364\n",
            "The error at epoch 222/1000 is error=0.228315\n",
            "The error at epoch 223/1000 is error=0.228267\n",
            "The error at epoch 224/1000 is error=0.228218\n",
            "The error at epoch 225/1000 is error=0.228169\n",
            "The error at epoch 226/1000 is error=0.228121\n",
            "The error at epoch 227/1000 is error=0.228072\n",
            "The error at epoch 228/1000 is error=0.228023\n",
            "The error at epoch 229/1000 is error=0.227974\n",
            "The error at epoch 230/1000 is error=0.227924\n",
            "The error at epoch 231/1000 is error=0.227875\n",
            "The error at epoch 232/1000 is error=0.227826\n",
            "The error at epoch 233/1000 is error=0.227776\n",
            "The error at epoch 234/1000 is error=0.227727\n",
            "The error at epoch 235/1000 is error=0.227677\n",
            "The error at epoch 236/1000 is error=0.227627\n",
            "The error at epoch 237/1000 is error=0.227578\n",
            "The error at epoch 238/1000 is error=0.227528\n",
            "The error at epoch 239/1000 is error=0.227478\n",
            "The error at epoch 240/1000 is error=0.227428\n",
            "The error at epoch 241/1000 is error=0.227377\n",
            "The error at epoch 242/1000 is error=0.227327\n",
            "The error at epoch 243/1000 is error=0.227277\n",
            "The error at epoch 244/1000 is error=0.227226\n",
            "The error at epoch 245/1000 is error=0.227175\n",
            "The error at epoch 246/1000 is error=0.227125\n",
            "The error at epoch 247/1000 is error=0.227074\n",
            "The error at epoch 248/1000 is error=0.227023\n",
            "The error at epoch 249/1000 is error=0.226972\n",
            "The error at epoch 250/1000 is error=0.226921\n",
            "The error at epoch 251/1000 is error=0.226870\n",
            "The error at epoch 252/1000 is error=0.226818\n",
            "The error at epoch 253/1000 is error=0.226767\n",
            "The error at epoch 254/1000 is error=0.226715\n",
            "The error at epoch 255/1000 is error=0.226664\n",
            "The error at epoch 256/1000 is error=0.226612\n",
            "The error at epoch 257/1000 is error=0.226560\n",
            "The error at epoch 258/1000 is error=0.226508\n",
            "The error at epoch 259/1000 is error=0.226456\n",
            "The error at epoch 260/1000 is error=0.226404\n",
            "The error at epoch 261/1000 is error=0.226352\n",
            "The error at epoch 262/1000 is error=0.226299\n",
            "The error at epoch 263/1000 is error=0.226247\n",
            "The error at epoch 264/1000 is error=0.226194\n",
            "The error at epoch 265/1000 is error=0.226141\n",
            "The error at epoch 266/1000 is error=0.226089\n",
            "The error at epoch 267/1000 is error=0.226036\n",
            "The error at epoch 268/1000 is error=0.225983\n",
            "The error at epoch 269/1000 is error=0.225930\n",
            "The error at epoch 270/1000 is error=0.225876\n",
            "The error at epoch 271/1000 is error=0.225823\n",
            "The error at epoch 272/1000 is error=0.225769\n",
            "The error at epoch 273/1000 is error=0.225716\n",
            "The error at epoch 274/1000 is error=0.225662\n",
            "The error at epoch 275/1000 is error=0.225608\n",
            "The error at epoch 276/1000 is error=0.225554\n",
            "The error at epoch 277/1000 is error=0.225500\n",
            "The error at epoch 278/1000 is error=0.225446\n",
            "The error at epoch 279/1000 is error=0.225392\n",
            "The error at epoch 280/1000 is error=0.225337\n",
            "The error at epoch 281/1000 is error=0.225283\n",
            "The error at epoch 282/1000 is error=0.225228\n",
            "The error at epoch 283/1000 is error=0.225173\n",
            "The error at epoch 284/1000 is error=0.225118\n",
            "The error at epoch 285/1000 is error=0.225063\n",
            "The error at epoch 286/1000 is error=0.225008\n",
            "The error at epoch 287/1000 is error=0.224953\n",
            "The error at epoch 288/1000 is error=0.224898\n",
            "The error at epoch 289/1000 is error=0.224842\n",
            "The error at epoch 290/1000 is error=0.224787\n",
            "The error at epoch 291/1000 is error=0.224731\n",
            "The error at epoch 292/1000 is error=0.224675\n",
            "The error at epoch 293/1000 is error=0.224619\n",
            "The error at epoch 294/1000 is error=0.224563\n",
            "The error at epoch 295/1000 is error=0.224507\n",
            "The error at epoch 296/1000 is error=0.224450\n",
            "The error at epoch 297/1000 is error=0.224394\n",
            "The error at epoch 298/1000 is error=0.224337\n",
            "The error at epoch 299/1000 is error=0.224281\n",
            "The error at epoch 300/1000 is error=0.224224\n",
            "The error at epoch 301/1000 is error=0.224167\n",
            "The error at epoch 302/1000 is error=0.224110\n",
            "The error at epoch 303/1000 is error=0.224053\n",
            "The error at epoch 304/1000 is error=0.223995\n",
            "The error at epoch 305/1000 is error=0.223938\n",
            "The error at epoch 306/1000 is error=0.223880\n",
            "The error at epoch 307/1000 is error=0.223822\n",
            "The error at epoch 308/1000 is error=0.223764\n",
            "The error at epoch 309/1000 is error=0.223706\n",
            "The error at epoch 310/1000 is error=0.223648\n",
            "The error at epoch 311/1000 is error=0.223590\n",
            "The error at epoch 312/1000 is error=0.223532\n",
            "The error at epoch 313/1000 is error=0.223473\n",
            "The error at epoch 314/1000 is error=0.223415\n",
            "The error at epoch 315/1000 is error=0.223356\n",
            "The error at epoch 316/1000 is error=0.223297\n",
            "The error at epoch 317/1000 is error=0.223238\n",
            "The error at epoch 318/1000 is error=0.223179\n",
            "The error at epoch 319/1000 is error=0.223119\n",
            "The error at epoch 320/1000 is error=0.223060\n",
            "The error at epoch 321/1000 is error=0.223000\n",
            "The error at epoch 322/1000 is error=0.222941\n",
            "The error at epoch 323/1000 is error=0.222881\n",
            "The error at epoch 324/1000 is error=0.222821\n",
            "The error at epoch 325/1000 is error=0.222761\n",
            "The error at epoch 326/1000 is error=0.222700\n",
            "The error at epoch 327/1000 is error=0.222640\n",
            "The error at epoch 328/1000 is error=0.222579\n",
            "The error at epoch 329/1000 is error=0.222519\n",
            "The error at epoch 330/1000 is error=0.222458\n",
            "The error at epoch 331/1000 is error=0.222397\n",
            "The error at epoch 332/1000 is error=0.222336\n",
            "The error at epoch 333/1000 is error=0.222275\n",
            "The error at epoch 334/1000 is error=0.222213\n",
            "The error at epoch 335/1000 is error=0.222152\n",
            "The error at epoch 336/1000 is error=0.222090\n",
            "The error at epoch 337/1000 is error=0.222028\n",
            "The error at epoch 338/1000 is error=0.221966\n",
            "The error at epoch 339/1000 is error=0.221904\n",
            "The error at epoch 340/1000 is error=0.221842\n",
            "The error at epoch 341/1000 is error=0.221780\n",
            "The error at epoch 342/1000 is error=0.221717\n",
            "The error at epoch 343/1000 is error=0.221655\n",
            "The error at epoch 344/1000 is error=0.221592\n",
            "The error at epoch 345/1000 is error=0.221529\n",
            "The error at epoch 346/1000 is error=0.221466\n",
            "The error at epoch 347/1000 is error=0.221403\n",
            "The error at epoch 348/1000 is error=0.221339\n",
            "The error at epoch 349/1000 is error=0.221276\n",
            "The error at epoch 350/1000 is error=0.221212\n",
            "The error at epoch 351/1000 is error=0.221148\n",
            "The error at epoch 352/1000 is error=0.221084\n",
            "The error at epoch 353/1000 is error=0.221020\n",
            "The error at epoch 354/1000 is error=0.220956\n",
            "The error at epoch 355/1000 is error=0.220891\n",
            "The error at epoch 356/1000 is error=0.220827\n",
            "The error at epoch 357/1000 is error=0.220762\n",
            "The error at epoch 358/1000 is error=0.220697\n",
            "The error at epoch 359/1000 is error=0.220632\n",
            "The error at epoch 360/1000 is error=0.220567\n",
            "The error at epoch 361/1000 is error=0.220502\n",
            "The error at epoch 362/1000 is error=0.220437\n",
            "The error at epoch 363/1000 is error=0.220371\n",
            "The error at epoch 364/1000 is error=0.220305\n",
            "The error at epoch 365/1000 is error=0.220239\n",
            "The error at epoch 366/1000 is error=0.220173\n",
            "The error at epoch 367/1000 is error=0.220107\n",
            "The error at epoch 368/1000 is error=0.220041\n",
            "The error at epoch 369/1000 is error=0.219974\n",
            "The error at epoch 370/1000 is error=0.219907\n",
            "The error at epoch 371/1000 is error=0.219841\n",
            "The error at epoch 372/1000 is error=0.219774\n",
            "The error at epoch 373/1000 is error=0.219707\n",
            "The error at epoch 374/1000 is error=0.219639\n",
            "The error at epoch 375/1000 is error=0.219572\n",
            "The error at epoch 376/1000 is error=0.219504\n",
            "The error at epoch 377/1000 is error=0.219437\n",
            "The error at epoch 378/1000 is error=0.219369\n",
            "The error at epoch 379/1000 is error=0.219301\n",
            "The error at epoch 380/1000 is error=0.219232\n",
            "The error at epoch 381/1000 is error=0.219164\n",
            "The error at epoch 382/1000 is error=0.219095\n",
            "The error at epoch 383/1000 is error=0.219027\n",
            "The error at epoch 384/1000 is error=0.218958\n",
            "The error at epoch 385/1000 is error=0.218889\n",
            "The error at epoch 386/1000 is error=0.218820\n",
            "The error at epoch 387/1000 is error=0.218750\n",
            "The error at epoch 388/1000 is error=0.218681\n",
            "The error at epoch 389/1000 is error=0.218611\n",
            "The error at epoch 390/1000 is error=0.218542\n",
            "The error at epoch 391/1000 is error=0.218472\n",
            "The error at epoch 392/1000 is error=0.218401\n",
            "The error at epoch 393/1000 is error=0.218331\n",
            "The error at epoch 394/1000 is error=0.218261\n",
            "The error at epoch 395/1000 is error=0.218190\n",
            "The error at epoch 396/1000 is error=0.218119\n",
            "The error at epoch 397/1000 is error=0.218048\n",
            "The error at epoch 398/1000 is error=0.217977\n",
            "The error at epoch 399/1000 is error=0.217906\n",
            "The error at epoch 400/1000 is error=0.217835\n",
            "The error at epoch 401/1000 is error=0.217763\n",
            "The error at epoch 402/1000 is error=0.217691\n",
            "The error at epoch 403/1000 is error=0.217619\n",
            "The error at epoch 404/1000 is error=0.217547\n",
            "The error at epoch 405/1000 is error=0.217475\n",
            "The error at epoch 406/1000 is error=0.217403\n",
            "The error at epoch 407/1000 is error=0.217330\n",
            "The error at epoch 408/1000 is error=0.217257\n",
            "The error at epoch 409/1000 is error=0.217185\n",
            "The error at epoch 410/1000 is error=0.217111\n",
            "The error at epoch 411/1000 is error=0.217038\n",
            "The error at epoch 412/1000 is error=0.216965\n",
            "The error at epoch 413/1000 is error=0.216891\n",
            "The error at epoch 414/1000 is error=0.216818\n",
            "The error at epoch 415/1000 is error=0.216744\n",
            "The error at epoch 416/1000 is error=0.216670\n",
            "The error at epoch 417/1000 is error=0.216595\n",
            "The error at epoch 418/1000 is error=0.216521\n",
            "The error at epoch 419/1000 is error=0.216446\n",
            "The error at epoch 420/1000 is error=0.216372\n",
            "The error at epoch 421/1000 is error=0.216297\n",
            "The error at epoch 422/1000 is error=0.216222\n",
            "The error at epoch 423/1000 is error=0.216146\n",
            "The error at epoch 424/1000 is error=0.216071\n",
            "The error at epoch 425/1000 is error=0.215995\n",
            "The error at epoch 426/1000 is error=0.215920\n",
            "The error at epoch 427/1000 is error=0.215844\n",
            "The error at epoch 428/1000 is error=0.215768\n",
            "The error at epoch 429/1000 is error=0.215691\n",
            "The error at epoch 430/1000 is error=0.215615\n",
            "The error at epoch 431/1000 is error=0.215538\n",
            "The error at epoch 432/1000 is error=0.215462\n",
            "The error at epoch 433/1000 is error=0.215385\n",
            "The error at epoch 434/1000 is error=0.215307\n",
            "The error at epoch 435/1000 is error=0.215230\n",
            "The error at epoch 436/1000 is error=0.215153\n",
            "The error at epoch 437/1000 is error=0.215075\n",
            "The error at epoch 438/1000 is error=0.214997\n",
            "The error at epoch 439/1000 is error=0.214919\n",
            "The error at epoch 440/1000 is error=0.214841\n",
            "The error at epoch 441/1000 is error=0.214763\n",
            "The error at epoch 442/1000 is error=0.214684\n",
            "The error at epoch 443/1000 is error=0.214606\n",
            "The error at epoch 444/1000 is error=0.214527\n",
            "The error at epoch 445/1000 is error=0.214448\n",
            "The error at epoch 446/1000 is error=0.214368\n",
            "The error at epoch 447/1000 is error=0.214289\n",
            "The error at epoch 448/1000 is error=0.214210\n",
            "The error at epoch 449/1000 is error=0.214130\n",
            "The error at epoch 450/1000 is error=0.214050\n",
            "The error at epoch 451/1000 is error=0.213970\n",
            "The error at epoch 452/1000 is error=0.213890\n",
            "The error at epoch 453/1000 is error=0.213809\n",
            "The error at epoch 454/1000 is error=0.213729\n",
            "The error at epoch 455/1000 is error=0.213648\n",
            "The error at epoch 456/1000 is error=0.213567\n",
            "The error at epoch 457/1000 is error=0.213486\n",
            "The error at epoch 458/1000 is error=0.213404\n",
            "The error at epoch 459/1000 is error=0.213323\n",
            "The error at epoch 460/1000 is error=0.213241\n",
            "The error at epoch 461/1000 is error=0.213159\n",
            "The error at epoch 462/1000 is error=0.213077\n",
            "The error at epoch 463/1000 is error=0.212995\n",
            "The error at epoch 464/1000 is error=0.212913\n",
            "The error at epoch 465/1000 is error=0.212830\n",
            "The error at epoch 466/1000 is error=0.212747\n",
            "The error at epoch 467/1000 is error=0.212664\n",
            "The error at epoch 468/1000 is error=0.212581\n",
            "The error at epoch 469/1000 is error=0.212498\n",
            "The error at epoch 470/1000 is error=0.212415\n",
            "The error at epoch 471/1000 is error=0.212331\n",
            "The error at epoch 472/1000 is error=0.212247\n",
            "The error at epoch 473/1000 is error=0.212163\n",
            "The error at epoch 474/1000 is error=0.212079\n",
            "The error at epoch 475/1000 is error=0.211995\n",
            "The error at epoch 476/1000 is error=0.211910\n",
            "The error at epoch 477/1000 is error=0.211826\n",
            "The error at epoch 478/1000 is error=0.211741\n",
            "The error at epoch 479/1000 is error=0.211656\n",
            "The error at epoch 480/1000 is error=0.211570\n",
            "The error at epoch 481/1000 is error=0.211485\n",
            "The error at epoch 482/1000 is error=0.211399\n",
            "The error at epoch 483/1000 is error=0.211314\n",
            "The error at epoch 484/1000 is error=0.211228\n",
            "The error at epoch 485/1000 is error=0.211141\n",
            "The error at epoch 486/1000 is error=0.211055\n",
            "The error at epoch 487/1000 is error=0.210969\n",
            "The error at epoch 488/1000 is error=0.210882\n",
            "The error at epoch 489/1000 is error=0.210795\n",
            "The error at epoch 490/1000 is error=0.210708\n",
            "The error at epoch 491/1000 is error=0.210621\n",
            "The error at epoch 492/1000 is error=0.210533\n",
            "The error at epoch 493/1000 is error=0.210446\n",
            "The error at epoch 494/1000 is error=0.210358\n",
            "The error at epoch 495/1000 is error=0.210270\n",
            "The error at epoch 496/1000 is error=0.210182\n",
            "The error at epoch 497/1000 is error=0.210094\n",
            "The error at epoch 498/1000 is error=0.210005\n",
            "The error at epoch 499/1000 is error=0.209916\n",
            "The error at epoch 500/1000 is error=0.209828\n",
            "The error at epoch 501/1000 is error=0.209739\n",
            "The error at epoch 502/1000 is error=0.209649\n",
            "The error at epoch 503/1000 is error=0.209560\n",
            "The error at epoch 504/1000 is error=0.209470\n",
            "The error at epoch 505/1000 is error=0.209381\n",
            "The error at epoch 506/1000 is error=0.209291\n",
            "The error at epoch 507/1000 is error=0.209201\n",
            "The error at epoch 508/1000 is error=0.209110\n",
            "The error at epoch 509/1000 is error=0.209020\n",
            "The error at epoch 510/1000 is error=0.208929\n",
            "The error at epoch 511/1000 is error=0.208838\n",
            "The error at epoch 512/1000 is error=0.208747\n",
            "The error at epoch 513/1000 is error=0.208656\n",
            "The error at epoch 514/1000 is error=0.208565\n",
            "The error at epoch 515/1000 is error=0.208473\n",
            "The error at epoch 516/1000 is error=0.208381\n",
            "The error at epoch 517/1000 is error=0.208289\n",
            "The error at epoch 518/1000 is error=0.208197\n",
            "The error at epoch 519/1000 is error=0.208105\n",
            "The error at epoch 520/1000 is error=0.208012\n",
            "The error at epoch 521/1000 is error=0.207920\n",
            "The error at epoch 522/1000 is error=0.207827\n",
            "The error at epoch 523/1000 is error=0.207734\n",
            "The error at epoch 524/1000 is error=0.207641\n",
            "The error at epoch 525/1000 is error=0.207547\n",
            "The error at epoch 526/1000 is error=0.207454\n",
            "The error at epoch 527/1000 is error=0.207360\n",
            "The error at epoch 528/1000 is error=0.207266\n",
            "The error at epoch 529/1000 is error=0.207172\n",
            "The error at epoch 530/1000 is error=0.207077\n",
            "The error at epoch 531/1000 is error=0.206983\n",
            "The error at epoch 532/1000 is error=0.206888\n",
            "The error at epoch 533/1000 is error=0.206793\n",
            "The error at epoch 534/1000 is error=0.206698\n",
            "The error at epoch 535/1000 is error=0.206603\n",
            "The error at epoch 536/1000 is error=0.206508\n",
            "The error at epoch 537/1000 is error=0.206412\n",
            "The error at epoch 538/1000 is error=0.206316\n",
            "The error at epoch 539/1000 is error=0.206220\n",
            "The error at epoch 540/1000 is error=0.206124\n",
            "The error at epoch 541/1000 is error=0.206028\n",
            "The error at epoch 542/1000 is error=0.205932\n",
            "The error at epoch 543/1000 is error=0.205835\n",
            "The error at epoch 544/1000 is error=0.205738\n",
            "The error at epoch 545/1000 is error=0.205641\n",
            "The error at epoch 546/1000 is error=0.205544\n",
            "The error at epoch 547/1000 is error=0.205446\n",
            "The error at epoch 548/1000 is error=0.205349\n",
            "The error at epoch 549/1000 is error=0.205251\n",
            "The error at epoch 550/1000 is error=0.205153\n",
            "The error at epoch 551/1000 is error=0.205055\n",
            "The error at epoch 552/1000 is error=0.204957\n",
            "The error at epoch 553/1000 is error=0.204858\n",
            "The error at epoch 554/1000 is error=0.204760\n",
            "The error at epoch 555/1000 is error=0.204661\n",
            "The error at epoch 556/1000 is error=0.204562\n",
            "The error at epoch 557/1000 is error=0.204463\n",
            "The error at epoch 558/1000 is error=0.204363\n",
            "The error at epoch 559/1000 is error=0.204264\n",
            "The error at epoch 560/1000 is error=0.204164\n",
            "The error at epoch 561/1000 is error=0.204064\n",
            "The error at epoch 562/1000 is error=0.203964\n",
            "The error at epoch 563/1000 is error=0.203864\n",
            "The error at epoch 564/1000 is error=0.203763\n",
            "The error at epoch 565/1000 is error=0.203663\n",
            "The error at epoch 566/1000 is error=0.203562\n",
            "The error at epoch 567/1000 is error=0.203461\n",
            "The error at epoch 568/1000 is error=0.203360\n",
            "The error at epoch 569/1000 is error=0.203259\n",
            "The error at epoch 570/1000 is error=0.203157\n",
            "The error at epoch 571/1000 is error=0.203055\n",
            "The error at epoch 572/1000 is error=0.202954\n",
            "The error at epoch 573/1000 is error=0.202852\n",
            "The error at epoch 574/1000 is error=0.202749\n",
            "The error at epoch 575/1000 is error=0.202647\n",
            "The error at epoch 576/1000 is error=0.202545\n",
            "The error at epoch 577/1000 is error=0.202442\n",
            "The error at epoch 578/1000 is error=0.202339\n",
            "The error at epoch 579/1000 is error=0.202236\n",
            "The error at epoch 580/1000 is error=0.202133\n",
            "The error at epoch 581/1000 is error=0.202029\n",
            "The error at epoch 582/1000 is error=0.201926\n",
            "The error at epoch 583/1000 is error=0.201822\n",
            "The error at epoch 584/1000 is error=0.201718\n",
            "The error at epoch 585/1000 is error=0.201614\n",
            "The error at epoch 586/1000 is error=0.201509\n",
            "The error at epoch 587/1000 is error=0.201405\n",
            "The error at epoch 588/1000 is error=0.201300\n",
            "The error at epoch 589/1000 is error=0.201196\n",
            "The error at epoch 590/1000 is error=0.201091\n",
            "The error at epoch 591/1000 is error=0.200985\n",
            "The error at epoch 592/1000 is error=0.200880\n",
            "The error at epoch 593/1000 is error=0.200775\n",
            "The error at epoch 594/1000 is error=0.200669\n",
            "The error at epoch 595/1000 is error=0.200563\n",
            "The error at epoch 596/1000 is error=0.200457\n",
            "The error at epoch 597/1000 is error=0.200351\n",
            "The error at epoch 598/1000 is error=0.200244\n",
            "The error at epoch 599/1000 is error=0.200138\n",
            "The error at epoch 600/1000 is error=0.200031\n",
            "The error at epoch 601/1000 is error=0.199924\n",
            "The error at epoch 602/1000 is error=0.199817\n",
            "The error at epoch 603/1000 is error=0.199710\n",
            "The error at epoch 604/1000 is error=0.199603\n",
            "The error at epoch 605/1000 is error=0.199495\n",
            "The error at epoch 606/1000 is error=0.199388\n",
            "The error at epoch 607/1000 is error=0.199280\n",
            "The error at epoch 608/1000 is error=0.199172\n",
            "The error at epoch 609/1000 is error=0.199064\n",
            "The error at epoch 610/1000 is error=0.198955\n",
            "The error at epoch 611/1000 is error=0.198847\n",
            "The error at epoch 612/1000 is error=0.198738\n",
            "The error at epoch 613/1000 is error=0.198629\n",
            "The error at epoch 614/1000 is error=0.198520\n",
            "The error at epoch 615/1000 is error=0.198411\n",
            "The error at epoch 616/1000 is error=0.198301\n",
            "The error at epoch 617/1000 is error=0.198192\n",
            "The error at epoch 618/1000 is error=0.198082\n",
            "The error at epoch 619/1000 is error=0.197972\n",
            "The error at epoch 620/1000 is error=0.197862\n",
            "The error at epoch 621/1000 is error=0.197752\n",
            "The error at epoch 622/1000 is error=0.197642\n",
            "The error at epoch 623/1000 is error=0.197531\n",
            "The error at epoch 624/1000 is error=0.197421\n",
            "The error at epoch 625/1000 is error=0.197310\n",
            "The error at epoch 626/1000 is error=0.197199\n",
            "The error at epoch 627/1000 is error=0.197088\n",
            "The error at epoch 628/1000 is error=0.196977\n",
            "The error at epoch 629/1000 is error=0.196865\n",
            "The error at epoch 630/1000 is error=0.196753\n",
            "The error at epoch 631/1000 is error=0.196642\n",
            "The error at epoch 632/1000 is error=0.196530\n",
            "The error at epoch 633/1000 is error=0.196418\n",
            "The error at epoch 634/1000 is error=0.196305\n",
            "The error at epoch 635/1000 is error=0.196193\n",
            "The error at epoch 636/1000 is error=0.196080\n",
            "The error at epoch 637/1000 is error=0.195968\n",
            "The error at epoch 638/1000 is error=0.195855\n",
            "The error at epoch 639/1000 is error=0.195742\n",
            "The error at epoch 640/1000 is error=0.195629\n",
            "The error at epoch 641/1000 is error=0.195515\n",
            "The error at epoch 642/1000 is error=0.195402\n",
            "The error at epoch 643/1000 is error=0.195288\n",
            "The error at epoch 644/1000 is error=0.195174\n",
            "The error at epoch 645/1000 is error=0.195060\n",
            "The error at epoch 646/1000 is error=0.194946\n",
            "The error at epoch 647/1000 is error=0.194832\n",
            "The error at epoch 648/1000 is error=0.194718\n",
            "The error at epoch 649/1000 is error=0.194603\n",
            "The error at epoch 650/1000 is error=0.194489\n",
            "The error at epoch 651/1000 is error=0.194374\n",
            "The error at epoch 652/1000 is error=0.194259\n",
            "The error at epoch 653/1000 is error=0.194144\n",
            "The error at epoch 654/1000 is error=0.194028\n",
            "The error at epoch 655/1000 is error=0.193913\n",
            "The error at epoch 656/1000 is error=0.193797\n",
            "The error at epoch 657/1000 is error=0.193681\n",
            "The error at epoch 658/1000 is error=0.193566\n",
            "The error at epoch 659/1000 is error=0.193450\n",
            "The error at epoch 660/1000 is error=0.193333\n",
            "The error at epoch 661/1000 is error=0.193217\n",
            "The error at epoch 662/1000 is error=0.193101\n",
            "The error at epoch 663/1000 is error=0.192984\n",
            "The error at epoch 664/1000 is error=0.192867\n",
            "The error at epoch 665/1000 is error=0.192750\n",
            "The error at epoch 666/1000 is error=0.192633\n",
            "The error at epoch 667/1000 is error=0.192516\n",
            "The error at epoch 668/1000 is error=0.192399\n",
            "The error at epoch 669/1000 is error=0.192282\n",
            "The error at epoch 670/1000 is error=0.192164\n",
            "The error at epoch 671/1000 is error=0.192046\n",
            "The error at epoch 672/1000 is error=0.191928\n",
            "The error at epoch 673/1000 is error=0.191810\n",
            "The error at epoch 674/1000 is error=0.191692\n",
            "The error at epoch 675/1000 is error=0.191574\n",
            "The error at epoch 676/1000 is error=0.191456\n",
            "The error at epoch 677/1000 is error=0.191337\n",
            "The error at epoch 678/1000 is error=0.191218\n",
            "The error at epoch 679/1000 is error=0.191100\n",
            "The error at epoch 680/1000 is error=0.190981\n",
            "The error at epoch 681/1000 is error=0.190861\n",
            "The error at epoch 682/1000 is error=0.190742\n",
            "The error at epoch 683/1000 is error=0.190623\n",
            "The error at epoch 684/1000 is error=0.190503\n",
            "The error at epoch 685/1000 is error=0.190384\n",
            "The error at epoch 686/1000 is error=0.190264\n",
            "The error at epoch 687/1000 is error=0.190144\n",
            "The error at epoch 688/1000 is error=0.190024\n",
            "The error at epoch 689/1000 is error=0.189904\n",
            "The error at epoch 690/1000 is error=0.189784\n",
            "The error at epoch 691/1000 is error=0.189664\n",
            "The error at epoch 692/1000 is error=0.189543\n",
            "The error at epoch 693/1000 is error=0.189422\n",
            "The error at epoch 694/1000 is error=0.189302\n",
            "The error at epoch 695/1000 is error=0.189181\n",
            "The error at epoch 696/1000 is error=0.189060\n",
            "The error at epoch 697/1000 is error=0.188939\n",
            "The error at epoch 698/1000 is error=0.188817\n",
            "The error at epoch 699/1000 is error=0.188696\n",
            "The error at epoch 700/1000 is error=0.188575\n",
            "The error at epoch 701/1000 is error=0.188453\n",
            "The error at epoch 702/1000 is error=0.188331\n",
            "The error at epoch 703/1000 is error=0.188209\n",
            "The error at epoch 704/1000 is error=0.188087\n",
            "The error at epoch 705/1000 is error=0.187965\n",
            "The error at epoch 706/1000 is error=0.187843\n",
            "The error at epoch 707/1000 is error=0.187721\n",
            "The error at epoch 708/1000 is error=0.187598\n",
            "The error at epoch 709/1000 is error=0.187476\n",
            "The error at epoch 710/1000 is error=0.187353\n",
            "The error at epoch 711/1000 is error=0.187230\n",
            "The error at epoch 712/1000 is error=0.187107\n",
            "The error at epoch 713/1000 is error=0.186984\n",
            "The error at epoch 714/1000 is error=0.186861\n",
            "The error at epoch 715/1000 is error=0.186738\n",
            "The error at epoch 716/1000 is error=0.186615\n",
            "The error at epoch 717/1000 is error=0.186491\n",
            "The error at epoch 718/1000 is error=0.186368\n",
            "The error at epoch 719/1000 is error=0.186244\n",
            "The error at epoch 720/1000 is error=0.186120\n",
            "The error at epoch 721/1000 is error=0.185996\n",
            "The error at epoch 722/1000 is error=0.185872\n",
            "The error at epoch 723/1000 is error=0.185748\n",
            "The error at epoch 724/1000 is error=0.185624\n",
            "The error at epoch 725/1000 is error=0.185500\n",
            "The error at epoch 726/1000 is error=0.185375\n",
            "The error at epoch 727/1000 is error=0.185251\n",
            "The error at epoch 728/1000 is error=0.185126\n",
            "The error at epoch 729/1000 is error=0.185002\n",
            "The error at epoch 730/1000 is error=0.184877\n",
            "The error at epoch 731/1000 is error=0.184752\n",
            "The error at epoch 732/1000 is error=0.184627\n",
            "The error at epoch 733/1000 is error=0.184502\n",
            "The error at epoch 734/1000 is error=0.184376\n",
            "The error at epoch 735/1000 is error=0.184251\n",
            "The error at epoch 736/1000 is error=0.184126\n",
            "The error at epoch 737/1000 is error=0.184000\n",
            "The error at epoch 738/1000 is error=0.183875\n",
            "The error at epoch 739/1000 is error=0.183749\n",
            "The error at epoch 740/1000 is error=0.183623\n",
            "The error at epoch 741/1000 is error=0.183497\n",
            "The error at epoch 742/1000 is error=0.183371\n",
            "The error at epoch 743/1000 is error=0.183245\n",
            "The error at epoch 744/1000 is error=0.183119\n",
            "The error at epoch 745/1000 is error=0.182993\n",
            "The error at epoch 746/1000 is error=0.182867\n",
            "The error at epoch 747/1000 is error=0.182740\n",
            "The error at epoch 748/1000 is error=0.182614\n",
            "The error at epoch 749/1000 is error=0.182487\n",
            "The error at epoch 750/1000 is error=0.182360\n",
            "The error at epoch 751/1000 is error=0.182234\n",
            "The error at epoch 752/1000 is error=0.182107\n",
            "The error at epoch 753/1000 is error=0.181980\n",
            "The error at epoch 754/1000 is error=0.181853\n",
            "The error at epoch 755/1000 is error=0.181726\n",
            "The error at epoch 756/1000 is error=0.181599\n",
            "The error at epoch 757/1000 is error=0.181471\n",
            "The error at epoch 758/1000 is error=0.181344\n",
            "The error at epoch 759/1000 is error=0.181217\n",
            "The error at epoch 760/1000 is error=0.181089\n",
            "The error at epoch 761/1000 is error=0.180962\n",
            "The error at epoch 762/1000 is error=0.180834\n",
            "The error at epoch 763/1000 is error=0.180706\n",
            "The error at epoch 764/1000 is error=0.180578\n",
            "The error at epoch 765/1000 is error=0.180451\n",
            "The error at epoch 766/1000 is error=0.180323\n",
            "The error at epoch 767/1000 is error=0.180195\n",
            "The error at epoch 768/1000 is error=0.180066\n",
            "The error at epoch 769/1000 is error=0.179938\n",
            "The error at epoch 770/1000 is error=0.179810\n",
            "The error at epoch 771/1000 is error=0.179682\n",
            "The error at epoch 772/1000 is error=0.179553\n",
            "The error at epoch 773/1000 is error=0.179425\n",
            "The error at epoch 774/1000 is error=0.179296\n",
            "The error at epoch 775/1000 is error=0.179168\n",
            "The error at epoch 776/1000 is error=0.179039\n",
            "The error at epoch 777/1000 is error=0.178911\n",
            "The error at epoch 778/1000 is error=0.178782\n",
            "The error at epoch 779/1000 is error=0.178653\n",
            "The error at epoch 780/1000 is error=0.178524\n",
            "The error at epoch 781/1000 is error=0.178395\n",
            "The error at epoch 782/1000 is error=0.178266\n",
            "The error at epoch 783/1000 is error=0.178137\n",
            "The error at epoch 784/1000 is error=0.178008\n",
            "The error at epoch 785/1000 is error=0.177879\n",
            "The error at epoch 786/1000 is error=0.177749\n",
            "The error at epoch 787/1000 is error=0.177620\n",
            "The error at epoch 788/1000 is error=0.177491\n",
            "The error at epoch 789/1000 is error=0.177361\n",
            "The error at epoch 790/1000 is error=0.177232\n",
            "The error at epoch 791/1000 is error=0.177102\n",
            "The error at epoch 792/1000 is error=0.176973\n",
            "The error at epoch 793/1000 is error=0.176843\n",
            "The error at epoch 794/1000 is error=0.176714\n",
            "The error at epoch 795/1000 is error=0.176584\n",
            "The error at epoch 796/1000 is error=0.176454\n",
            "The error at epoch 797/1000 is error=0.176324\n",
            "The error at epoch 798/1000 is error=0.176194\n",
            "The error at epoch 799/1000 is error=0.176064\n",
            "The error at epoch 800/1000 is error=0.175934\n",
            "The error at epoch 801/1000 is error=0.175804\n",
            "The error at epoch 802/1000 is error=0.175674\n",
            "The error at epoch 803/1000 is error=0.175544\n",
            "The error at epoch 804/1000 is error=0.175414\n",
            "The error at epoch 805/1000 is error=0.175284\n",
            "The error at epoch 806/1000 is error=0.175154\n",
            "The error at epoch 807/1000 is error=0.175024\n",
            "The error at epoch 808/1000 is error=0.174893\n",
            "The error at epoch 809/1000 is error=0.174763\n",
            "The error at epoch 810/1000 is error=0.174633\n",
            "The error at epoch 811/1000 is error=0.174502\n",
            "The error at epoch 812/1000 is error=0.174372\n",
            "The error at epoch 813/1000 is error=0.174241\n",
            "The error at epoch 814/1000 is error=0.174111\n",
            "The error at epoch 815/1000 is error=0.173980\n",
            "The error at epoch 816/1000 is error=0.173850\n",
            "The error at epoch 817/1000 is error=0.173719\n",
            "The error at epoch 818/1000 is error=0.173588\n",
            "The error at epoch 819/1000 is error=0.173458\n",
            "The error at epoch 820/1000 is error=0.173327\n",
            "The error at epoch 821/1000 is error=0.173196\n",
            "The error at epoch 822/1000 is error=0.173065\n",
            "The error at epoch 823/1000 is error=0.172935\n",
            "The error at epoch 824/1000 is error=0.172804\n",
            "The error at epoch 825/1000 is error=0.172673\n",
            "The error at epoch 826/1000 is error=0.172542\n",
            "The error at epoch 827/1000 is error=0.172411\n",
            "The error at epoch 828/1000 is error=0.172280\n",
            "The error at epoch 829/1000 is error=0.172149\n",
            "The error at epoch 830/1000 is error=0.172018\n",
            "The error at epoch 831/1000 is error=0.171887\n",
            "The error at epoch 832/1000 is error=0.171756\n",
            "The error at epoch 833/1000 is error=0.171625\n",
            "The error at epoch 834/1000 is error=0.171494\n",
            "The error at epoch 835/1000 is error=0.171363\n",
            "The error at epoch 836/1000 is error=0.171232\n",
            "The error at epoch 837/1000 is error=0.171101\n",
            "The error at epoch 838/1000 is error=0.170970\n",
            "The error at epoch 839/1000 is error=0.170839\n",
            "The error at epoch 840/1000 is error=0.170708\n",
            "The error at epoch 841/1000 is error=0.170577\n",
            "The error at epoch 842/1000 is error=0.170446\n",
            "The error at epoch 843/1000 is error=0.170315\n",
            "The error at epoch 844/1000 is error=0.170183\n",
            "The error at epoch 845/1000 is error=0.170052\n",
            "The error at epoch 846/1000 is error=0.169921\n",
            "The error at epoch 847/1000 is error=0.169790\n",
            "The error at epoch 848/1000 is error=0.169659\n",
            "The error at epoch 849/1000 is error=0.169527\n",
            "The error at epoch 850/1000 is error=0.169396\n",
            "The error at epoch 851/1000 is error=0.169265\n",
            "The error at epoch 852/1000 is error=0.169134\n",
            "The error at epoch 853/1000 is error=0.169003\n",
            "The error at epoch 854/1000 is error=0.168871\n",
            "The error at epoch 855/1000 is error=0.168740\n",
            "The error at epoch 856/1000 is error=0.168609\n",
            "The error at epoch 857/1000 is error=0.168478\n",
            "The error at epoch 858/1000 is error=0.168347\n",
            "The error at epoch 859/1000 is error=0.168215\n",
            "The error at epoch 860/1000 is error=0.168084\n",
            "The error at epoch 861/1000 is error=0.167953\n",
            "The error at epoch 862/1000 is error=0.167822\n",
            "The error at epoch 863/1000 is error=0.167691\n",
            "The error at epoch 864/1000 is error=0.167560\n",
            "The error at epoch 865/1000 is error=0.167428\n",
            "The error at epoch 866/1000 is error=0.167297\n",
            "The error at epoch 867/1000 is error=0.167166\n",
            "The error at epoch 868/1000 is error=0.167035\n",
            "The error at epoch 869/1000 is error=0.166904\n",
            "The error at epoch 870/1000 is error=0.166773\n",
            "The error at epoch 871/1000 is error=0.166642\n",
            "The error at epoch 872/1000 is error=0.166511\n",
            "The error at epoch 873/1000 is error=0.166379\n",
            "The error at epoch 874/1000 is error=0.166248\n",
            "The error at epoch 875/1000 is error=0.166117\n",
            "The error at epoch 876/1000 is error=0.165986\n",
            "The error at epoch 877/1000 is error=0.165855\n",
            "The error at epoch 878/1000 is error=0.165724\n",
            "The error at epoch 879/1000 is error=0.165593\n",
            "The error at epoch 880/1000 is error=0.165462\n",
            "The error at epoch 881/1000 is error=0.165332\n",
            "The error at epoch 882/1000 is error=0.165201\n",
            "The error at epoch 883/1000 is error=0.165070\n",
            "The error at epoch 884/1000 is error=0.164939\n",
            "The error at epoch 885/1000 is error=0.164808\n",
            "The error at epoch 886/1000 is error=0.164677\n",
            "The error at epoch 887/1000 is error=0.164546\n",
            "The error at epoch 888/1000 is error=0.164416\n",
            "The error at epoch 889/1000 is error=0.164285\n",
            "The error at epoch 890/1000 is error=0.164154\n",
            "The error at epoch 891/1000 is error=0.164024\n",
            "The error at epoch 892/1000 is error=0.163893\n",
            "The error at epoch 893/1000 is error=0.163762\n",
            "The error at epoch 894/1000 is error=0.163632\n",
            "The error at epoch 895/1000 is error=0.163501\n",
            "The error at epoch 896/1000 is error=0.163371\n",
            "The error at epoch 897/1000 is error=0.163240\n",
            "The error at epoch 898/1000 is error=0.163110\n",
            "The error at epoch 899/1000 is error=0.162979\n",
            "The error at epoch 900/1000 is error=0.162849\n",
            "The error at epoch 901/1000 is error=0.162719\n",
            "The error at epoch 902/1000 is error=0.162588\n",
            "The error at epoch 903/1000 is error=0.162458\n",
            "The error at epoch 904/1000 is error=0.162328\n",
            "The error at epoch 905/1000 is error=0.162198\n",
            "The error at epoch 906/1000 is error=0.162068\n",
            "The error at epoch 907/1000 is error=0.161937\n",
            "The error at epoch 908/1000 is error=0.161807\n",
            "The error at epoch 909/1000 is error=0.161677\n",
            "The error at epoch 910/1000 is error=0.161547\n",
            "The error at epoch 911/1000 is error=0.161417\n",
            "The error at epoch 912/1000 is error=0.161288\n",
            "The error at epoch 913/1000 is error=0.161158\n",
            "The error at epoch 914/1000 is error=0.161028\n",
            "The error at epoch 915/1000 is error=0.160898\n",
            "The error at epoch 916/1000 is error=0.160768\n",
            "The error at epoch 917/1000 is error=0.160639\n",
            "The error at epoch 918/1000 is error=0.160509\n",
            "The error at epoch 919/1000 is error=0.160380\n",
            "The error at epoch 920/1000 is error=0.160250\n",
            "The error at epoch 921/1000 is error=0.160121\n",
            "The error at epoch 922/1000 is error=0.159991\n",
            "The error at epoch 923/1000 is error=0.159862\n",
            "The error at epoch 924/1000 is error=0.159733\n",
            "The error at epoch 925/1000 is error=0.159603\n",
            "The error at epoch 926/1000 is error=0.159474\n",
            "The error at epoch 927/1000 is error=0.159345\n",
            "The error at epoch 928/1000 is error=0.159216\n",
            "The error at epoch 929/1000 is error=0.159087\n",
            "The error at epoch 930/1000 is error=0.158958\n",
            "The error at epoch 931/1000 is error=0.158829\n",
            "The error at epoch 932/1000 is error=0.158700\n",
            "The error at epoch 933/1000 is error=0.158572\n",
            "The error at epoch 934/1000 is error=0.158443\n",
            "The error at epoch 935/1000 is error=0.158314\n",
            "The error at epoch 936/1000 is error=0.158186\n",
            "The error at epoch 937/1000 is error=0.158057\n",
            "The error at epoch 938/1000 is error=0.157929\n",
            "The error at epoch 939/1000 is error=0.157800\n",
            "The error at epoch 940/1000 is error=0.157672\n",
            "The error at epoch 941/1000 is error=0.157544\n",
            "The error at epoch 942/1000 is error=0.157415\n",
            "The error at epoch 943/1000 is error=0.157287\n",
            "The error at epoch 944/1000 is error=0.157159\n",
            "The error at epoch 945/1000 is error=0.157031\n",
            "The error at epoch 946/1000 is error=0.156903\n",
            "The error at epoch 947/1000 is error=0.156775\n",
            "The error at epoch 948/1000 is error=0.156648\n",
            "The error at epoch 949/1000 is error=0.156520\n",
            "The error at epoch 950/1000 is error=0.156392\n",
            "The error at epoch 951/1000 is error=0.156265\n",
            "The error at epoch 952/1000 is error=0.156137\n",
            "The error at epoch 953/1000 is error=0.156010\n",
            "The error at epoch 954/1000 is error=0.155882\n",
            "The error at epoch 955/1000 is error=0.155755\n",
            "The error at epoch 956/1000 is error=0.155628\n",
            "The error at epoch 957/1000 is error=0.155501\n",
            "The error at epoch 958/1000 is error=0.155374\n",
            "The error at epoch 959/1000 is error=0.155247\n",
            "The error at epoch 960/1000 is error=0.155120\n",
            "The error at epoch 961/1000 is error=0.154993\n",
            "The error at epoch 962/1000 is error=0.154866\n",
            "The error at epoch 963/1000 is error=0.154740\n",
            "The error at epoch 964/1000 is error=0.154613\n",
            "The error at epoch 965/1000 is error=0.154487\n",
            "The error at epoch 966/1000 is error=0.154360\n",
            "The error at epoch 967/1000 is error=0.154234\n",
            "The error at epoch 968/1000 is error=0.154108\n",
            "The error at epoch 969/1000 is error=0.153982\n",
            "The error at epoch 970/1000 is error=0.153856\n",
            "The error at epoch 971/1000 is error=0.153730\n",
            "The error at epoch 972/1000 is error=0.153604\n",
            "The error at epoch 973/1000 is error=0.153478\n",
            "The error at epoch 974/1000 is error=0.153352\n",
            "The error at epoch 975/1000 is error=0.153227\n",
            "The error at epoch 976/1000 is error=0.153101\n",
            "The error at epoch 977/1000 is error=0.152976\n",
            "The error at epoch 978/1000 is error=0.152850\n",
            "The error at epoch 979/1000 is error=0.152725\n",
            "The error at epoch 980/1000 is error=0.152600\n",
            "The error at epoch 981/1000 is error=0.152475\n",
            "The error at epoch 982/1000 is error=0.152350\n",
            "The error at epoch 983/1000 is error=0.152225\n",
            "The error at epoch 984/1000 is error=0.152100\n",
            "The error at epoch 985/1000 is error=0.151975\n",
            "The error at epoch 986/1000 is error=0.151851\n",
            "The error at epoch 987/1000 is error=0.151726\n",
            "The error at epoch 988/1000 is error=0.151602\n",
            "The error at epoch 989/1000 is error=0.151477\n",
            "The error at epoch 990/1000 is error=0.151353\n",
            "The error at epoch 991/1000 is error=0.151229\n",
            "The error at epoch 992/1000 is error=0.151105\n",
            "The error at epoch 993/1000 is error=0.150981\n",
            "The error at epoch 994/1000 is error=0.150857\n",
            "The error at epoch 995/1000 is error=0.150733\n",
            "The error at epoch 996/1000 is error=0.150610\n",
            "The error at epoch 997/1000 is error=0.150486\n",
            "The error at epoch 998/1000 is error=0.150363\n",
            "The error at epoch 999/1000 is error=0.150239\n",
            "The error at epoch 1000/1000 is error=0.150116\n",
            "last error is  0.150116070697758\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The classification accuracy is  79.22535211267606 %\n",
            "Total time =  65.73601365089417\n",
            "Train time =  65.5904815196991\n",
            "Predict time =  0.014384269714355469\n"
          ]
        }
      ]
    }
  ]
}